---
title: "Weibull series system - simulation study"
author: "Alex Towell"
date: "2023-03-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadlibs, echo=FALSE, warning=FALSE, message=FALSE}
library(algebraic.mle)
library(dplyr)
library(ggplot2)
library(reshape2)
library(knitr)
library(md.tools)
library(md.series.system)
library(utils)
library(tidyverse)
library(gridExtra)
```


# Introduction

In the real world, systems are quite complex:

1. They are not series systems.

2. The components are not independent.

3. The lifetimes of the systems (in the population) is not precisely modeled by
   any known probability distribution.
   
4. It may not be easy to characterize a system as either in a failed state or a
   non-failed state, and failure may only be transient.
   
5. The components may depend on many other unobserved factors.

With all of these caveats in mind, we model the data as coming from a series
system as described previously, and other factors, like ambient temperature, are
either negligible (on the distribution of component lifetimes) or are more or
less constant and so we model the component lifetimes under those conditions.
Then, the process of parametrically modeling the observed data takes the
following form:

1. Visualize the data, e.g., plot a histogram of the data.

2. Guess which parametric distribution (for the components) might fit the
   observed data for the system lifetime.
   
3. Use a statistical test for goodness-of-fit.

4. Repeat steps 2 and 3 if the measure of goodness of fit is not satisfactory.

Steps 1 and 3 are trivial to do, but step 2 may be very difficult, particularly
in our case since a histogram of the data is probably not that informative. Why?
There are two reasons:

1. The distribution of the system is a function of the distribution of the
   components. The system distribution probably does not even have a name.
   
2. The histogram is of the system lifetime data, but the distributions we guess
   are for the components.
   
   
# Optimization issues

The optimization problem is to find the MLE of the parameters of the
distribution of the components in series.

In Guo, we see that parameter rescaling is essential to converging
to the MLE. In parameter rescaling, a unit change in one of the
parameters is scaled by the derivative of the log-likelihood with
respect to that parameter. This is done to make the parameters
comparable in size.  The problem is that the derivative of the
log-likelihood with respect to the parameters is not always
available in closed form.  In that case, we can use numerical
differentiation to approximate the derivative.

In `optim`, we can also just use a fixed `parscale` option, which
is a vector of the same length as the number of parameters.  This
is a vector of scaling factors for the parameters.  The default is
1, which is not always appropriate.  We can use the `parscale`
option to scale the parameters appropriately, although we may have
to do some trial and error to get the scaling factors right. The
problem is that the scaling factors are not always obvious. For
the Weibull distribution, the scale parameter is the inverse of
the failure rate, so we can use the inverse of the failure rate
as the scaling factor for the scale parameter.  For the shape
parameter, we can use the inverse of the square of the failure
rate as the scaling factor.  This is because the shape parameter
is the reciprocal of the coefficient of variation, and the
coefficient of variation is the standard deviation divided by the
mean, and the standard deviation is the square root of the
variance, and the variance is the square of the failure rate.


`optimx`

`nlopt`

Explore: Newton-Raphson

Explore: Deriving the score function and the Fisher information matrix
for the Weibull series. At least, for a 3-component series system.

Explore: Derive the density, hazard, and surv for Weibull. Derive
the gradients of each. Use this information to derive the Weibull
series log-likelihood and score functions, which may be more accurate.
The FIM is probably too much trouble to derive, but give it a stab.


# Simulation study

The purpose of this data set is to analyze the sensivity of the
Weibull series system (7 components) with respect to a sample size.

> Let's think about how to define the component parameters so that we
> get a nice spread of values.  We want to have a range of values for
> the parameters, but we also want to have a range of values for the
> failure times. We can consider each component in isolation, and make
> some of them have a high "infant" mortality rate, and some of them
> have a low "infant" mortality rate.  Others can have a high "old age"
> mortality rate, and some can have a low "old age" mortality rate.
> The remaing components can have a high "wear out" mortality rate, and
> some can have a low "wear out" mortality rate.
>
> We should plot the pdf of the Weibull distributions and see that we get
> a nice spread of values.


It's good to focus on a few key performance measures and explore the behavior of your estimator. Including the comparison of the estimated variance-covariance matrix to the "true" variance-covariance matrix is also a valuable addition. Here's a brief outline of how to implement these measures in your simulation study:

Bias, Variance, and MSE: For each simulated dataset, obtain the MLE estimates of the parameters, and compute the bias, variance, and MSE as follows:

a. Bias: Calculate the difference between the MLE estimates and the true parameter values for each dataset. Then compute the average difference across all datasets.

b. Variance: Compute the variance of the MLE estimates across all datasets.

c. MSE: Calculate the squared difference between the MLE estimates and the true parameter values for each dataset. Then compute the average squared difference across all datasets.

Confidence Interval Coverage: For each simulated dataset, construct confidence intervals for the parameters using the estimated variance-covariance matrix (e.g., using the Fisher Information Matrix, FIM). Calculate the proportion of datasets for which the true parameter values fall within the constructed confidence intervals.

Sensitivity Analysis by Model Specification: Analyze the robustness of your estimator by changing the true parameter values in your simulations. Observe how the performance measures (bias, variance, MSE, confidence interval coverage) are affected by these changes.

Visualizations: Create plots of the performance measures against factors such as sample size, censoring percentage, and true parameter values. This will help you understand how the estimator's performance changes under different conditions.

Implementing these performance measures and analyses in your simulation study will provide a comprehensive understanding of your MLE estimator's behavior and accuracy in estimating the parameters of lifetime components in a series system with competing risks. It will also help you identify potential limitations and sources of error, which can be valuable for future research and improvement.

## Purpose

The purpose of this data set is to analyze the sensivity of the
exponential series system (7 components, all roughly the same
failure rate) with respect to a sample size.

Here is how we generated this data set (we do not evaluate this code, since
we already generated the data set and saved it to a file):

```{r run-exp-experiment-3, eval=FALSE}
#etwd("../weibull_experiments")
#getwd()
source("weibull_experiment_gen.R")

theta <- c(10, 9, 2, 15, 8, 10)
p <- .333
tau <- 1000
R <- 7

weibull_experiment_gen(
    R = R,
    bernoulli_probs = p,
    taus = tau,
    sample_sizes = c(35, 40, 45, 50, 60, 70, 80, 90, 100, 250, 500, 1000),
    theta = theta,
    seed = 157231,
    use_aneal_start = TRUE,
    append = FALSE,
    csv_filename = "weibull_experiment_1.csv")
```


We read the data set from the file with:
```{r load-exp-experiment-3}
df <- read.csv("weibull_experiment_1.csv")
head(df, n = 6)
```


## Coverage Probability
```{r visualize-coverage}
df_long <- df %>% pivot_longer(
    cols = starts_with("coverage"),
    names_to = "Coverage",
    values_to = "Value")

# Convert the "Coverage" column to a factor for better plotting
df_long$Coverage <- as.factor(df_long$Coverage)

# Plot, with a solid line for the asymptotic coverage probability
# for 0.95% confidence level
ggplot(df_long, aes(x = N, y = Value, color = Coverage)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0.95, linetype = "dashed") +
  labs(x = "Sample Size (N)", y = "Coverage Probability",
    title = "Coverage Probability vs Sample Size", color = "Coverage")
```

### Analysis

Analysis here.


## Bias
```{r visualize-bias, fig.width=8, fig.height=5}
############# BIAS #############
df_long <- df %>% pivot_longer(
    cols = starts_with("bias"),
    names_to = "Bias",
    values_to = "Value")

# Convert the "Bias" column to a factor for better plotting
df_long$Bias <- as.factor(df_long$Bias)

# Plot, with a solid blue line for the asymptotic zero bias
ggplot(df_long, aes(x = N, y = Value, color = Bias)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  labs(x = "Sample Size (N)", y = "Bias",
       title = "Bias vs Sample Size", color = "Bias")
```


### Analysis

Analysis here.

## Mean Squared Error

```{r visualize-mse, fig.width=8, fig.height=5}
cutoff <- 70
df_small <- df %>% filter(N <= cutoff)
df_large <- df %>% filter(N > cutoff)

p1 <- ggplot(df_small) +
  geom_point(aes(x = N, y = mse)) +
  geom_line(aes(x = N, y = mse,
                color = "Simulation")) +
  geom_line(aes(x = N, y = mse_asym,
                color = "Asymptotic"),
            linetype = "dashed") +
  labs(x = "Small Sample Size", y = "Mean Squared Error",
       title = "Mean Squared Error vs Small Sample Size")

p2 <- ggplot(df_large) +
  geom_point(aes(x = N, y = mse)) +
  geom_line(aes(x = N, y = mse,
                color = "Simulation")) +
  geom_line(aes(x = N, y = mse_asym,
                color = "Asymptotic"),
            linetype = "dashed") +
  labs(x = "Large Sample Size", y = "Mean Squared Error",
       title = "Mean Squared Error vs Large Sample Size")
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```



### Analysis

Analysis here.

## Standard Error

```{r visualize-se, fig.width=8, fig.height=5}
df_long <- df %>% pivot_longer(
    # regex to match column names "se<digit>"
    cols = matches("se[0-9]"),
    names_to = "SE",
    values_to = "Value")

# Convert the "SE" column to a factor for better plotting
df_long$SE <- as.factor(df_long$SE)

# Combine the two plots using facet
df_long$Group <- ifelse(df_long$N <= 150, "N < 150", "N >= 150")
ggplot(df_long, aes(x = N, y = Value, color = SE)) +
    geom_point() +
    geom_line() +
    facet_wrap(~ Group, scales = "free") +
    labs(x = "Sample Size (N)", y = "SE",
        title = "SE vs Sample Size", color = "SE")
```

### Analysis

Analysis here.

## Ratio of SE to Asymptotic SE

```{r fig.width=8, fig.height=6, fig.align="center", fig.cap="Ratio of SE to Asymptotic SE"}
cutoff <- 70

df_ratio <- df %>% mutate(
    Ratio1 = se_asym1/se1,
    Ratio2 = se_asym2/se2,
    Ratio3 = se_asym3/se3,
    Ratio4 = se_asym4/se4,
    Ratio5 = se_asym5/se5,
    Ratio6 = se_asym6/se6) %>%
    select(matches("Ratio[0-9]"), N) %>%
    mutate(mean_ratio = rowMeans(select(., matches("Ratio[0-9]"))))
df_ratio_small <- df_ratio %>% filter(N <= cutoff)
df_ratio_large <- df_ratio %>% filter(N > cutoff)

p3 <- ggplot(df_ratio_small, aes(x = N, y = mean_ratio)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  ylim(0, 7) +
  labs(x = "Sample Size (N)", y = "Mean Ratio of Asymptotic to Simulated SE",
       title = "Mean Ratio of Asymptotic to Simulated SE vs Small Sample Size")

p4 <- ggplot(df_ratio_large, aes(x = N, y = mean_ratio)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  ylim(0, 7) +
  labs(x = "Sample Size (N)", y = "Mean Ratio of Asymptotic to Simulated SE",
       title = "Mean Ratio of Asymptotic to Simulated SE vs Large Sample Size")

df_ratio_long <- df_ratio %>%
    pivot_longer(
        cols = matches("Ratio[0-9]"),
        names_to = "Ratio",
        values_to = "Value"
    )


# Split df_ratio_long into two dataframes by sample size
df_ratio_long_small <- df_ratio_long %>% filter(N < cutoff)
df_ratio_long_large <- df_ratio_long %>% filter(N >= cutoff)

# Create the two plots
p1 <- ggplot(df_ratio_long_small, aes(x = N, y = Value, color = Ratio)) +
  geom_point() +
  geom_line() +
  labs(x = "Small Sample Size",
       y = "Ratio of Asymptotic to Simulated SE",
       title = "Ratio of Asymptotic to Monte Carlo SE vs Small Samples",
       color = "Ratio")

p2 <- ggplot(df_ratio_long_large, aes(x = N, y = Value, color = Ratio)) +
  geom_point() +
  geom_line() +
  labs(x = "Large Sample Size",
       y = "Ratio of Asymptotic to Simulated SE",
       title = "Ratio of Asymptotic to Monte Carlo SE vs Large Samples",
       color = "Ratio")

# Arrange the plots side by side
grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
```


### Analysis

Analysis here.











## Hypothesis test






## Hessian computation

Once I derive the score `md_score_weibull_series_C1_C2_C3`,
I want to investigate usng
```{r}
v <- md_score_weibull_series_C1_C2_C3(data, theta0)
H1 <- v %*% t(v)
```
to compute the negative of the observed FIM. Compare to:
```{r}
H2 <- hessian(md_loglike_weibull_series_C1_C2_C3(data),
    theta0, data)
```

Since `H1` may rely upon a much more accurate score function,
which I will derive analytically, I expect `H1` to be more accurate
than `H2`. This is important because I rely on the IFIM to compute
the asymptotic variance of the MLE. In my simulation, I also want
to conduct a hypothesis test as to whether the MLE sample (MC)
is compatible with `MVN(theta, IFIM)`, according to theory. At
some point, it should be compatible.





