<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data</title>
  <meta name="description" content="Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data" />
  
  
  



<meta name="date" content="2023-09-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path=""><a href="#statmod"><i class="fa fa-check"></i><b>2</b> Series System Model</a>
<ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#comp-cause"><i class="fa fa-check"></i><b>2.1</b> Component Cause of Failure</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#reliability"><i class="fa fa-check"></i><b>2.2</b> System and Component Reliabilities</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#like-model"><i class="fa fa-check"></i><b>3</b> Likelihood Model for Masked Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#candmod"><i class="fa fa-check"></i><b>3.1</b> Masked Component Cause of Failure</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#right-censored-data"><i class="fa fa-check"></i><b>3.2</b> Right-Censored Data</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#identifiability"><i class="fa fa-check"></i><b>3.3</b> Identifiability and Convergence Issues</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#mle"><i class="fa fa-check"></i><b>4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="5" data-path=""><a href="#boot"><i class="fa fa-check"></i><b>5</b> Bias-Corrected and Accelerated Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="6" data-path=""><a href="#weibull"><i class="fa fa-check"></i><b>6</b> Series System with Weibull Components</a>
<ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#sys-weibull-like"><i class="fa fa-check"></i><b>6.1</b> Likelihood Model</a></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#reduced-weibull"><i class="fa fa-check"></i><b>6.2</b> Weibull Series System: Homogeneous Shape Parameters</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path=""><a href="#sim-study"><i class="fa fa-check"></i><b>7</b> Simulation Study: Series System with Weibull Components</a>
<ul>
<li class="chapter" data-level="7.1" data-path=""><a href="#performance-metrics"><i class="fa fa-check"></i><b>7.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="7.2" data-path=""><a href="#data-gen-proc"><i class="fa fa-check"></i><b>7.2</b> Data Generating Process</a></li>
<li class="chapter" data-level="7.3" data-path=""><a href="#overview-of-simulations"><i class="fa fa-check"></i><b>7.3</b> Overview of Simulations</a></li>
<li class="chapter" data-level="7.4" data-path=""><a href="#effect-censoring"><i class="fa fa-check"></i><b>7.4</b> Scenario: Assessing the Impact of Right-Censoring</a></li>
<li class="chapter" data-level="7.5" data-path=""><a href="#p-vs-mttf"><i class="fa fa-check"></i><b>7.5</b> Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure</a></li>
<li class="chapter" data-level="7.6" data-path=""><a href="#effect-samp-size"><i class="fa fa-check"></i><b>7.6</b> Scenario: Assessing the Impact of Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path=""><a href="#future-work"><i class="fa fa-check"></i><b>8</b> Future Work</a></li>
<li class="chapter" data-level="9" data-path=""><a href="#conclusion"><i class="fa fa-check"></i><b>9</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path=""><a href="#appendices"><i class="fa fa-check"></i>Appendices</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path=""><a href="#series-system-with-weibull-component-lifetimes"><i class="fa fa-check"></i><b>A</b> Series System with Weibull Component Lifetimes</a>
<ul>
<li class="chapter" data-level="A.1" data-path=""><a href="#app-weibull-loglik-r"><i class="fa fa-check"></i><b>A.1</b> Log-likelihood Function</a></li>
<li class="chapter" data-level="A.2" data-path=""><a href="#app-score-fn-r"><i class="fa fa-check"></i><b>A.2</b> Score Function</a></li>
<li class="chapter" data-level="A.3" data-path=""><a href="#app-series-quantile"><i class="fa fa-check"></i><b>A.3</b> Quantile Function</a></li>
<li class="chapter" data-level="A.4" data-path=""><a href="#app-mle-r"><i class="fa fa-check"></i><b>A.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="A.5" data-path=""><a href="#app-cand-model-r"><i class="fa fa-check"></i><b>A.5</b> Bernoulli Masking Model</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path=""><a href="#simulation"><i class="fa fa-check"></i><b>B</b> Simulation</a>
<ul>
<li class="chapter" data-level="B.1" data-path=""><a href="#app-sim-study-r"><i class="fa fa-check"></i><b>B.1</b> Scenario Simulation</a></li>
<li class="chapter" data-level="B.2" data-path=""><a href="#app-plot-r"><i class="fa fa-check"></i><b>B.2</b> Plot Generation Code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data</h1>
<p class="author"><em><p></p></em></p>
<p class="date"><em>2023-09-30</em></p>
<div class="abstract">
<p class="abstract">Abstract</p>
This paper investigates maximum likelihood techniques to estimate component reliability from masked failure data in series systems. A likelihood model accounts for right-censoring and candidate sets indicative of masked failure causes. Extensive simulation studies assess the accuracy and precision of maximum likelihood estimates under varying sample size, masking probability, and right-censoring time for components with Weibull lifetimes. The studies specifically examine the accuracy and precision of estimates, along with the coverage probability and width of BCa confidence intervals. Despite significant masking and censoring, the maximum likelihood estimator demonstrates good overall performance. The bootstrap yields correctly specified confidence intervals even for small sample sizes. Together, the modeling framework and simulation studies provide rigorous validation of statistical learning from masked reliability data.
</div>
</div>
<div id="introduction" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Introduction<a href="#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Quantifying the reliability of individual components in a series system
<span class="citation">[<a href="#ref-Agustin-2011" role="doc-biblioref">1</a>]</span> is challenging when only system-level failure data is
observable, especially when this data is masked by right-censoring and ambiguity
about the cause of failure. This paper develops and validates maximum likelihood
techniques to estimate component reliability from right-censored lifetimes and
candidate sets indicative of masked failure causes. Specific contributions
include:</p>
<ul>
<li><p>Deriving a likelihood model that incorporates right-censoring and candidate
sets to enable masked data to be used for parameter estimation.</p></li>
<li><p>Conducting simulation studies for a well-designed series system with component
lifetimes following a Weibull distribution. We assess the accuracy and precision
of maximum likelihood estimates (MLE) under varying conditions related to sample
size, masking probability, and right-censoring. We found that the MLE performs
well in the presence of significant masking and censoring even for relatively small
samples.</p></li>
<li><p>Evaluating the coverage probability (accuracy) and precision of the BCa
confidence intervals (CI) constructed for the MLE. We found that the CIs have
good empirical coverage probability even for small sample sizes in the presence of
significant masking and censoring, but that the CIs for the shape parameters
were less accurate, indicating that the shape parameters are more difficult to
estimate than the scale parameters.</p></li>
</ul>
<p>The simulation studies focus on three key aspects:</p>
<ol style="list-style-type: decimal">
<li>The impact of right-censoring on component parameter estimates.</li>
<li>How masking probability for the cause of failure affects the estimates.</li>
<li>The role of sample size in mitigating challenges related to censoring and masking.</li>
</ol>
<p>Together, the likelihood framework and simulation methodology enable rigorous
validation of inferring component reliability from masked system data.
This expands the capability to learn properties of the latent components and
perform robust statistical inference given significant data challenges.</p>
</div>
<div id="statmod" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Series System Model<a href="#statmod" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Consider a system composed of <span class="math inline">\(m\)</span> components arranged in a series configuration.
Each component and system has two possible states, functioning or failed.
We have <span class="math inline">\(n\)</span> systems whose lifetimes are independent and identically distributed
(i.i.d.).
The lifetime of the <span class="math inline">\(i\)</span> system is denoted by the random
variable <span class="math inline">\(T_i\)</span> and the lifetime of its <span class="math inline">\(j\)</span> component is
denoted by the random variable <span class="math inline">\(T_{i j}\)</span>. We assume the component lifetimes in a
single system are statistically independent and non-identically distributed.
Here, lifetime (or lifespan) is defined as the elapsed time from when the new,
functioning component (or system) is put into operation until it fails for the
first time. A series system fails when any component fails, thus the lifetime of
the <span class="math inline">\(i\)</span> system is given by the component with the shortest
lifetime,
<span class="math display">\[
    T_i = \min\bigl\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
\]</span></p>
<p>There are three particularly important distribution functions in reliability analysis: the
reliability function, the probability density function (pdf), and the hazard function.
The reliability function, <span class="math inline">\(R_{T_i}(t)\)</span>, is the
probability that the <span class="math inline">\(i\)</span> system has a lifetime greater than
the given duration <span class="math inline">\(t\)</span>,
<span class="math display">\[\begin{equation}
R_{T_i}(t) = \Pr\{T_i &gt; t\}.
\end{equation}\]</span>
The pdf of <span class="math inline">\(T_i\)</span> is denoted by <span class="math inline">\(f_{T_i}(t)\)</span> and may be defined as
<span class="math display">\[
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
\]</span>
Next, we introduce the hazard function.
The probability that a failure occurs between the times <span class="math inline">\(t\)</span> and <span class="math inline">\(t+\Delta t\)</span> given that no
failure occurs before time <span class="math inline">\(t\)</span> may be written as
<span class="math display">\[
\Pr\{T_i \leq t+\Delta t|T_i &gt; t\} = \frac{\Pr\{t &lt; T_i &lt; t+\Delta t\}}{\Pr\{T_i &gt; t\}}.
\]</span>
The failure rate is given by dividing this equation by the length of the time
interval, <span class="math inline">\(\Delta t\)</span>:
<span class="math display">\[
\frac{\Pr\{t &lt; T_i &lt; t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i &gt; t\}} =
    -\frac{R_{T_i}(t+\Delta t) - R_{T_i}(t)}{\Delta t} \frac{1}{R_{T_i}(t)}.
\]</span>
The hazard function <span class="math inline">\(h_{T_i}(t)\)</span> for <span class="math inline">\(T_i\)</span> is the instantaneous failure rate at time <span class="math inline">\(t\)</span>,
which is given by
<span class="math display">\[\begin{equation}
\label{eq:failure-rate}
\begin{split}
h_{T_i}(t) 
  &amp;= -\lim_{\Delta t \to 0} \frac{R_{T_i}(t+\Delta t) - R_{T_i}(t)}{\Delta t}
    \frac{1}{R_{T_i}(t)}\\
  &amp;= \left(-\frac{d}{dt} R_{T_i}(t)\right) \frac{1}{R_{T_i}(t)} = \frac{f_{T_i}(t)}{R_{T_i}(t)}
\end{split}
\end{equation}\]</span></p>
<p>The lifetime of the <span class="math inline">\(j\)</span> component is assumed to follow a parametric distribution indexed
by a parameter vector <span class="math inline">\(\boldsymbol{\theta_j}\)</span>. The parameter vector of the overall system is defined as
<span class="math display">\[
    \boldsymbol{\theta }= (\boldsymbol{\theta_1},\ldots,\boldsymbol{\theta_m}),
\]</span>
where <span class="math inline">\(\boldsymbol{\theta_j}\)</span> is the parameter vector of the <span class="math inline">\(j\)</span> component.</p>
<p>When a random variable <span class="math inline">\(X\)</span> is parameterized by a particular <span class="math inline">\(\boldsymbol{\theta}\)</span>, we denote the
reliability function by <span class="math inline">\(R_X(t;\boldsymbol{\theta})\)</span>, and the same for the other distribution functions.
As a special case, for the components in a series system, we subscript by their labels, e.g,
the pdf of the <span class="math inline">\(j\)</span> component is denoted by <span class="math inline">\(f_j(t;\boldsymbol{\theta_j})\)</span>. Two continuous
random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a joint pdf <span class="math inline">\(f_{X,Y}(x,y)\)</span>.
Given the joint pdf <span class="math inline">\(f_{X,Y}(x,y)\)</span>, the marginal pdf of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
\]</span>
where <span class="math inline">\(\mathcal{Y}\)</span> is the support of <span class="math inline">\(Y\)</span>. (If <span class="math inline">\(Y\)</span> is discrete, replace
the integration with a summation over <span class="math inline">\(\mathcal{Y}\)</span>.)</p>
<p>The conditional pdf of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>, <span class="math inline">\(f_{Y|X}(y|x)\)</span>, is defined as
<span class="math display">\[
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\]</span>
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of <span class="math inline">\(X_1,\ldots,X_m\)</span> is denoted by <span class="math inline">\(f(x_1,\ldots,x_m)\)</span>.</p>
<p>Next, we dive deeper into these concepts and provide mathematical derivations for
the reliability function, the pdf, and the hazard function of the series system.
We begin with the reliability function of the series system, as given by the following theorem.</p>
<div class="theorem">
<p><span id="thm:sys-reliability-fn" class="theorem"><strong>Theorem 2.1  </strong></span>The series system has a reliability function given by
<span class="math display">\[\begin{equation}
\label{eq:sys-reliability-fn}
R_{T_i}(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}),
\end{equation}\]</span>
where <span class="math inline">\(R_j(t;\boldsymbol{\theta_j})\)</span> is the reliability function of the <span class="math inline">\(j\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>The reliability function is defined as
<span class="math display">\[
  R_{T_i}(t;\boldsymbol{\theta}) = \Pr\{T_i &gt; t\}
\]</span>
which may be rewritten as
<span class="math display">\[
  R_{T_i}(t;\boldsymbol{\theta}) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} &gt; t\}.
\]</span>
For the minimum to be larger than <span class="math inline">\(t\)</span>, every component must be larger than <span class="math inline">\(t\)</span>,
<span class="math display">\[
  R_{T_i}(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} &gt; t,\ldots,T_{i m} &gt; t\}.
\]</span>
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
<span class="math display">\[
  R_{T_i}(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} &gt; t\} \times \cdots \times \Pr\{T_{i m} &gt; t\}.
\]</span>
By definition, <span class="math inline">\(R_j(t;\boldsymbol{\theta}) = \Pr\{T_{i j} &gt; t\}\)</span>.
Performing this substitution obtains the result
<span class="math display">\[
  R_{T_i}(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
\]</span></p>
</div>
<p>Theorem <a href="#thm:sys-reliability-fn">2.1</a> shows that the system’s overall reliability is the
product of the reliabilities of its individual components. This is an important relationship
in all series systems and will be used in the subsequent derivations. Next, we turn our
attention to the pdf of the system lifetime, described in the following theorem.</p>
<div class="theorem">
<p><span id="thm:sys-pdf" class="theorem"><strong>Theorem 2.2  </strong></span>The series system has a pdf given by
<span class="math display">\[\begin{equation}
\label{eq:sys-pdf}
f_{T_i}(t;\boldsymbol{\theta}) = \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\boldsymbol{\theta_j}),
\end{equation}\]</span>
where <span class="math inline">\(f_j(t;\boldsymbol{\theta_j})\)</span> is the pdf of the <span class="math inline">\(j\)</span> component
and <span class="math inline">\(R_k(t;\boldsymbol{\theta_k})\)</span> is the reliability function of the <span class="math inline">\(k\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>By definition, the pdf may be written as
<span class="math display">\[
    f_{T_i}(t;\boldsymbol{\theta}) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
\]</span>
By the product rule, this may be rewritten as
<span class="math display">\[\begin{align*}
  f_{T_i}(t;\boldsymbol{\theta})
    &amp;= -\frac{d}{dt} R_1(t;\boldsymbol{\theta_1})\prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j})\\
    &amp;= f_1(t;\boldsymbol{\theta}) \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}).
\end{align*}\]</span>
Recursively applying the product rule <span class="math inline">\(m-1\)</span> times results in
<span class="math display">\[
f_{T_i}(t;\boldsymbol{\theta}) = \sum_{j=1}^{m-1} f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\boldsymbol{\theta_j}) \frac{d}{dt} R_m(t;\boldsymbol{\theta_m}),
\]</span>
which simplifies to
<span class="math display">\[
f_{T_i}(t;\boldsymbol{\theta})= \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}).
\]</span></p>
</div>
<p>Theorem <a href="#thm:sys-pdf">2.2</a> shows the pdf of the system lifetime is a function of
the pdfs and reliabilities of its components. We continue with the hazard
function of the system lifetime, defined in the next theorem.</p>
<div class="theorem">
<p><span id="thm:sys-failure-rate" class="theorem"><strong>Theorem 2.3  </strong></span>The series system has a hazard function given by
<span class="math display">\[\begin{equation}
\label{eq:sys-failure-rate}
h_{T_i}(t;\boldsymbol{\theta}) = \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}), 
\end{equation}\]</span>
where <span class="math inline">\(h_j(t;\boldsymbol{\theta_j})\)</span> is the hazard function of the <span class="math inline">\(j\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>By Equation <a href="#eq:failure-rate">(<strong>??</strong>)</a>, the <span class="math inline">\(i\)</span> series system
lifetime has a hazard function defined as
<span class="math display">\[
  h_{T_i}(t;\boldsymbol{\theta}) = \frac{f_{T_i}(t;\boldsymbol{\theta})}{R_{T_i}(t;\boldsymbol{\theta})}.
\]</span>
Plugging in expressions for these functions results in
<span class="math display">\[
  h_{T_i}(t;\boldsymbol{\theta}) = \frac{\sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k})}
      {\prod_{j=1}^m R_j(t;\boldsymbol{\theta_j})},
\]</span>
which can be simplified to
<span class="math display">\[
h_{T_i}(t;\boldsymbol{\theta})
  = \sum_{j=1}^m \frac{f_j(t;\boldsymbol{\theta_j})}{R_j(t;\boldsymbol{\theta_j})}
  = \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}).
\]</span></p>
</div>
<p>Theorem <a href="#thm:sys-failure-rate">2.3</a> reveals that the system’s hazard function is
the sum of the hazard functions of its components. By definition, the hazard
function is the ratio of the pdf to the reliability function,
<span class="math display">\[
h_{T_i}(t;\boldsymbol{\theta}) = \frac{f_{T_i}(t;\boldsymbol{\theta})}{R_{T_i}(t;\boldsymbol{\theta})},
\]</span>
and we can rearrange this to get
<span class="math display">\[\begin{equation}
\label{eq:sys-pdf-2}
\begin{split}
f_{T_i}(t;\boldsymbol{\theta}) &amp;= h_{T_i}(t;\boldsymbol{\theta}) R_{T_i}(t;\boldsymbol{\theta})\\
              &amp;= \biggl\{\sum_{j=1}^m h_j(t;\boldsymbol{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}) \biggr\},
\end{split}
\end{equation}\]</span>
which we sometimes find to be a more convenient form than Equation
<a href="#eq:sys-pdf">(<strong>??</strong>)</a>.</p>
<p>In this section, we derived the mathematical forms for the system’s reliability,
probability density, and hazard functions. Next, we build upon these concepts to
derive distributions related to the component cause of failure.</p>
<div id="comp-cause" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Component Cause of Failure<a href="#comp-cause" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Whenever a series system fails, precisely one of the components is the cause.
We denote the component cause of failure of the <span class="math inline">\(i\)</span> series
system by the discrete random variable <span class="math inline">\(K_i\)</span>, whose support is given by <span class="math inline">\(\{1,\ldots,m\}\)</span>.
For example, <span class="math inline">\(K_i=j\)</span> indicates that the component indexed by <span class="math inline">\(j\)</span> failed first, i.e.,
<span class="math display">\[
    T_{i j} &lt; T_{i j&#39;}
\]</span>
for every <span class="math inline">\(j&#39;\)</span> in the support of <span class="math inline">\(K_i\)</span> except for <span class="math inline">\(j\)</span>.
Since we have series systems, <span class="math inline">\(K_i\)</span> is unique.</p>
<p>The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.</p>
<div class="theorem">
<p><span id="thm:f-k-and-t" class="theorem"><strong>Theorem 2.4  </strong></span>The joint pdf of the component cause of failure <span class="math inline">\(K_i\)</span> and the series system lifetime
<span class="math inline">\(T_i\)</span> is given by
<span class="math display">\[\begin{equation}
\label{eq:f-k-and-t}
  f_{K_i,T_i}(j,t;\boldsymbol{\theta}) = h_j(t;\boldsymbol{\theta_j}) \prod_{l=1}^m R_l(t;\boldsymbol{\theta}),
\end{equation}\]</span>
where <span class="math inline">\(h_l(t;\boldsymbol{\theta_j})\)</span> and <span class="math inline">\(R_l(t;\boldsymbol{\theta_l})\)</span> are respectively the hazard
and reliability functions of the <span class="math inline">\(l\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>Consider a series system with <span class="math inline">\(3\)</span> components.
By the assumption that component lifetimes are mutually independent,
the joint pdf of <span class="math inline">\(T_{i 1}\)</span>, <span class="math inline">\(T_{i 2}\)</span>, and <span class="math inline">\(T_{i 3}\)</span> is given by
<span class="math display">\[
    f(t_1,t_2,t_3;\boldsymbol{\theta}) = \prod_{j=1}^{3} f_j(t_j;\boldsymbol{\theta_j}),
\]</span>
where <span class="math inline">\(f_j(t_j;\boldsymbol{\theta_j})\)</span> is the pdf of the <span class="math inline">\(j\)</span> component.
The first component is the cause of failure at time <span class="math inline">\(t\)</span> if <span class="math inline">\(K_i = 1\)</span> and
<span class="math inline">\(T_i = t\)</span>, which may be rephrased as the likelihood that <span class="math inline">\(T_{i 1} = t\)</span>,
<span class="math inline">\(T_{i 2} &gt; t\)</span>, and <span class="math inline">\(T_{i 3} &gt; t\)</span>. Thus,
<span class="math display">\[\begin{align*}
f_{K_i,T_i}(j,t;\boldsymbol{\theta}) 
    &amp;= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2}) f_3(t_3;\boldsymbol{\theta_3})
        dt_3 dt_2\\
     &amp;= \int_t^{\infty} f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2})
        R_3(t;\boldsymbol{\theta_3}) dt_2\\
     &amp;= f_1(t;\boldsymbol{\theta_1}) R_2(t;\boldsymbol{\theta_2}) R_3(t_1;\boldsymbol{\theta_3}).
\end{align*}\]</span>
By definition, <span class="math inline">\(f_1(t;\boldsymbol{\theta_1}) = h_1(t;\boldsymbol{\theta_1}) R_1(t;\boldsymbol{\theta_1})\)</span>,
and when we make this substitution into the above expression for <span class="math inline">\(f_{K_i,T_i}(j,t;\boldsymbol{\theta})\)</span>,
we obtain the result
<span class="math display">\[
f_{K_i,T_i}(j,t;\boldsymbol{\theta}) = h_1(t;\boldsymbol{\theta_1}) \prod_{l=1}^m R_l(t;\boldsymbol{\theta_l}).
\]</span>
Generalizing this result completes the proof.</p>
</div>
<p>Theorem <a href="#thm:f-k-and-t">2.4</a> shows that the joint pdf of the component cause of
failure and system lifetime is a function of the hazard functions and reliability
functions of the components. This result will be used in Section <a href="#like-model">3</a>
to derive the likelihood function for the masked data.</p>
<p>The probability that the <span class="math inline">\(j\)</span> component is the cause of failure
is given by the following theorem.</p>
<div class="theorem">
<p><span id="thm:prob-k" class="theorem"><strong>Theorem 2.5  </strong></span>The probability that the <span class="math inline">\(j\)</span> component is the cause of failure
is given by
<span class="math display">\[\begin{equation}
\label{eq:prob-k}
\Pr\{K_i = j\} = E_{\boldsymbol{\theta}}
\biggl[
    \frac{h_j(T_i;\boldsymbol{\theta_j})}
         {\sum_{l=1}^m h_l(T_i ; \boldsymbol{\theta_l})}
\biggr],
\end{equation}\]</span>
where <span class="math inline">\(K_i\)</span> is the random variable denoting the component cause of failure of the
<span class="math inline">\(i\)</span> system and <span class="math inline">\(T_i\)</span> is the random variable denoting the
lifetime of the <span class="math inline">\(i\)</span> system.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>The probability the <span class="math inline">\(j\)</span> component is the cause of failure is given by
marginalizing the joint pdf of <span class="math inline">\(K_i\)</span> and <span class="math inline">\(T_i\)</span> over <span class="math inline">\(T_i\)</span>,
<span class="math display">\[
\Pr\{K_i = j\} = \int_0^{\infty} f_{K_i,T_i}(j,t;\boldsymbol{\theta}) dt.
\]</span>
By Theorem <a href="#thm:f-k-and-t">2.4</a>, this is equivalent to
<span class="math display">\[\begin{align*}
\Pr\{K_i = j\}
    &amp;= \int_0^{\infty} h_j(t;\boldsymbol{\theta_j}) R_{T_i}(t;\boldsymbol{\theta}) dt\\
    &amp;= \int_0^{\infty} \biggl(\frac{h_j(t;\boldsymbol{\theta_j})}{h_{T_i}(t ; \boldsymbol{\theta})}\biggr) f_{T_i}(t ; \boldsymbol{\theta}) dt\\
    &amp;= E_{\boldsymbol{\theta}}\biggl[\frac{h_j(T_i;\boldsymbol{\theta_j})}{\sum_{l=1}^m h_l(T_i ; \boldsymbol{\theta_l})}\biggr].
\end{align*}\]</span></p>
</div>
<p>If we know the system failure time, then we can simplify the above expression
for the probability that the <span class="math inline">\(j\)</span> component is the cause of
failure. This is given by the following theorem.</p>
<div class="theorem">
<p><span id="thm:prob-k-given-t" class="theorem"><strong>Theorem 2.6  </strong></span>The probability that the <span class="math inline">\(j\)</span>
component is the cause of system failure given that we know the system failure
occurred at time <span class="math inline">\(t_i\)</span> is given by
<span class="math display">\[
\Pr\{K_i = j|T_i=t_i\} = \frac{h_j(t_i;\boldsymbol{\theta_j})}{\sum_{l=1}^m h_l(t_i;\boldsymbol{\theta_l})}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>By the definition of conditional probability,
<span class="math display">\[\begin{align*}
    \Pr\{K_i = j|T_i = t_i\} 
        &amp;= \frac{f_{K_i,T_i}(j, t_i;\boldsymbol{\theta})}{f_{T_i}(t_i;\boldsymbol{\theta})}\\
        &amp;= \frac{h_j(t_i;\boldsymbol{\theta_j}) R_{T_i}(t_i;\boldsymbol{\theta})}{f_{T_i}(t_i;\boldsymbol{\theta})}.
\end{align*}\]</span>
Since <span class="math inline">\(f_{T_i}(t_i;\boldsymbol{\theta}) = h_{T_i}(t_i;\boldsymbol{\theta}) R_{T_i}(t_i;\boldsymbol{\theta})\)</span>,
we make this substitution and simplify to obtain
<span class="math display">\[
\Pr\{K_i = j|T_i = t_i\} = \frac{h_j(t_i;\boldsymbol{\theta_j})}{\sum_{l=1}^m h_l(t_i;\boldsymbol{\theta_l})}.
\]</span></p>
</div>
<p>Theorems <a href="#thm:prob-k">2.5</a> and <a href="#thm:prob-k-given-t">2.6</a> are closely related
and have similar forms. Theorem <a href="#thm:prob-k-given-t">2.6</a> can be seen as a
special case of Theorem <a href="#thm:prob-k">2.5</a>.</p>
</div>
<div id="reliability" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> System and Component Reliabilities<a href="#reliability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The reliability of a system is described by its reliability function, which
denotes the probability that the system is functioning at a given time,
e.g., <span class="math inline">\(R_{T_i}(t&#39;;\boldsymbol{\theta})\)</span> denotes the probability that the <span class="math inline">\(i\)</span>
system is functioning at time <span class="math inline">\(t&#39;\)</span>. If we want a summary measure of the system’s
reliability, a common measure is the mean time to failure (MTTF), which is the
expectation of the system lifetime,
<span class="math display">\[\begin{equation}
\label{mttf-sys}
\text{MTTF} = E_{\boldsymbol{\theta}}\!\bigl[T_i\bigr],
\end{equation}\]</span>
which if certain assumptions are satisfied<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is equivalent to the integration
of the reliability function over its support. While the MTTF provides a summary
measure of reliability, it is not a complete description. Depending on the
failure characteristics, the MTTF can be misleading. For example, a system that has
a high likelihood of failing early in its life may still have a large MTTF if it
is fat-tailed.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The reliability of the components in the series system determines the reliability
of the system. We denote the MTTF of the <span class="math inline">\(j\)</span> component by
<span class="math inline">\(\text{MTTF}_j\)</span> and, according to Theorem <a href="#thm:prob-k">2.5</a>, the probability
that the <span class="math inline">\(j\)</span> component is the cause of failure is given by
<span class="math inline">\(\Pr\{K_i = j\}\)</span>. In a <strong>well-designed</strong> series system, there are no components
that are much “weaker” than any of the others, e.g., they have similar
MTTFs and probabilities of being the component cause of failure. In this paper,
we perform a sensitivity analysis of the MLE for a well-designed series system.</p>
</div>
</div>
<div id="like-model" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Likelihood Model for Masked Data<a href="#like-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We aim to estimate an unknown parameter, <span class="math inline">\(\boldsymbol{\theta}\)</span>, using <em>masked data</em>.
We consider two types of masking: censoring of system failures and masking
component causes of failure.</p>
<div id="censoring" class="section level5 unnumbered hasAnchor" number="">
<h5>Censoring<a href="#censoring" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We generally encounter two types of censoring: the system failure is observed
to occur within some time interval, or the system failure is not observed
but we know that it was functioning at least until some point in time. The
latter is known as <em>right-censoring</em>, which is the type of censoring we consider
in this paper.</p>
</div>
<div id="component-cause-of-failure-masking" class="section level5 unnumbered hasAnchor" number="">
<h5>Component Cause of Failure Masking<a href="#component-cause-of-failure-masking" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In the case of masking the component cause of failure, we may not know the
precise component cause of failure, but we may have some indication. A common
example is when a diagnostician is able to isolate the cause of failure to a
subset of the components. We call this subset the <em>candidate set</em>.</p>
</div>
<div id="masked-data" class="section level5 unnumbered hasAnchor" number="">
<h5>Masked Data<a href="#masked-data" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In this paper, each system is put into operation and observed until either it
fails or its failure is right-censored after some duration <span class="math inline">\(\tau\)</span>, so we do not
directly observe the system lifetime but rather we observe the right-censored
lifetime, <span class="math inline">\(S_i\)</span>, which is given by
<span class="math display">\[\begin{equation}
    S_i = \min\{\tau, T_i\}.
\end{equation}\]</span>
We also observe an event indicator, <span class="math inline">\(\delta_i\)</span>, which is given by
<span class="math display">\[\begin{equation}
    \delta_i = 1_{T_i &lt; \tau},
\end{equation}\]</span>
where <span class="math inline">\(1_{\text{condition}}\)</span> is an indicator function that denotes <span class="math inline">\(1\)</span> if
the condition is true and <span class="math inline">\(0\)</span> otherwise.
Here, <span class="math inline">\(\delta_i = 1\)</span> indicates the <span class="math inline">\(i\)</span> system’s failure was
observed and <span class="math inline">\(\delta_i = 0\)</span> indicates it was right-censored.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
If a system failure event is observed (<span class="math inline">\(\delta_i = 1\)</span>), then we also observe a
candidate set that contains the component cause of failure. We denote the
candidate set for the <span class="math inline">\(i\)</span> system by <span class="math inline">\(\mathcal{C}_i\)</span>, which
is a subset of <span class="math inline">\(\{1,\ldots,m\}\)</span>.</p>
In summary, the observed data is assumed to be i.i.d. and is given by
<span class="math inline">\(D = \{D_1, \ldots, D_n\}\)</span>, where each <span class="math inline">\(D_i\)</span> contains the following elements:
<p>The masked data generation process is illustrated in Figure .</p>
<p>An example of masked data <span class="math inline">\(D\)</span> with a right-censoring time <span class="math inline">\(\tau = 5\)</span> can be seen in Table 1
for a series system with <span class="math inline">\(3\)</span> components.</p>
<table>
<caption>Right-censored lifetime data with masked component cause of failure.</caption>
<colgroup>
<col width="7%" />
<col width="36%" />
<col width="31%" />
<col width="24%" />
</colgroup>
<thead>
<tr class="header">
<th>System</th>
<th>Right-censored lifetime (<span class="math inline">\(S_i\)</span>)</th>
<th>Event indicator (<span class="math inline">\(\delta_i\)</span>)</th>
<th>Candidate set (<span class="math inline">\(\mathcal{C}_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(1.1\)</span></td>
<td>1</td>
<td><span class="math inline">\(\{1,2\}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(1.3\)</span></td>
<td>1</td>
<td><span class="math inline">\(\{2\}\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td><span class="math inline">\(2.6\)</span></td>
<td>1</td>
<td><span class="math inline">\(\{2,3\}\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td><span class="math inline">\(3.7\)</span></td>
<td>1</td>
<td><span class="math inline">\(\{1,2,3\}\)</span></td>
</tr>
<tr class="odd">
<td>6</td>
<td><span class="math inline">\(5\)</span></td>
<td>0</td>
<td><span class="math inline">\(\emptyset\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(5\)</span></td>
<td>0</td>
<td><span class="math inline">\(\emptyset\)</span></td>
</tr>
</tbody>
</table>
<p>In our model, we assume the data is governed by a pdf, which is determined by
a specific parameter, represented as <span class="math inline">\(\boldsymbol{\theta}\)</span> within the parameter space <span class="math inline">\(\boldsymbol{\Omega}\)</span>.
The joint pdf of the data <span class="math inline">\(D\)</span> can be represented as follows:
<span class="math display">\[
f(D ; \boldsymbol{\theta}) = \prod_{i=1}^n f(D_i;\boldsymbol{\theta}) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\boldsymbol{\theta}),
\]</span>
where <span class="math inline">\(s_i\)</span> is the observed system lifetime, <span class="math inline">\(\delta_i\)</span> is the observed event
indicator, and <span class="math inline">\(c_i\)</span> is the observed candidate set of the <span class="math inline">\(i\)</span> system.</p>
<p>This joint pdf tells us how likely we are to observe the particular data, <span class="math inline">\(D\)</span>, given
the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>. When we keep the data constant and allow the parameter
<span class="math inline">\(\boldsymbol{\theta}\)</span> to vary, we obtain what is called the likelihood function <span class="math inline">\(L\)</span>, defined as
<span class="math display">\[
L(\boldsymbol{\theta}) = \prod_{i=1}^n L_i(\boldsymbol{\theta})
\]</span>
where
<span class="math display">\[
L_i(\boldsymbol{\theta}) = f(D_i;\boldsymbol{\theta})
\]</span>
is the likelihood contribution of the <span class="math inline">\(i\)</span> system.</p>
<p>For each type of data, right-censored data and masked component cause of
failure data, we will derive the <em>likelihood contribution</em> <span class="math inline">\(L_i\)</span>, which refers
to the part of the likelihood function that this particular piece of data
contributes to. We present the following theorem for the likelihood contribution
model.</p>
<div class="theorem">
<p><span id="thm:likelihood-contribution" class="theorem"><strong>Theorem 3.1  </strong></span>The likelihood contribution of the <span class="math inline">\(i\)</span> system is given by
<span class="math display">\[\begin{equation}
\label{eq:like}
L_i(\boldsymbol{\theta}) \propto \prod_{j=1}^m R_j(s_i;\boldsymbol{\theta_j}) \biggl(\sum_{j \in c_i} h_j(s_i;\boldsymbol{\theta_j}) \biggr)^{\delta_i}
\end{equation}\]</span>
where <span class="math inline">\(\delta_i = 0\)</span> indicates the <span class="math inline">\(i\)</span> system is
right-censored at time <span class="math inline">\(s_i\)</span> and <span class="math inline">\(\delta_i = 1\)</span> indicates the <span class="math inline">\(i\)</span> system
is observed to have failed at time <span class="math inline">\(s_i\)</span> with a component cause of failure potentially masked
by the candidate set <span class="math inline">\(c_i\)</span>.</p>
</div>
<p>In the following subsections, we prove this result for each type of masked data,
right-censored system lifetime data <span class="math inline">\((\delta_i = 0)\)</span> and masking of the
component cause of failure <span class="math inline">\((\delta_i = 1)\)</span>.</p>
</div>
<div id="candmod" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Masked Component Cause of Failure<a href="#candmod" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When a system failure occurs, two types of data are observed in our data model:
the system’s lifetime and a candidate set that is indicative of
the component cause of failure without necessarily precisely identifying the
failed component. For instance, suppose we have a failed electronic device and
a diagnostician is only able to identity that a circuit board in the device has
failed but is unable to identify the precise component that failed on the board.
In this case, any of the components on the board may be the cause of failure.
The cause of failure is said to be <em>masked</em> by the other components on the board.</p>
<p>The key goal of our analysis is to estimate the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>, which
maximizes the likelihood of the observed data, and to estimate the precision and
accuracy of this estimate using the Bootstrap method. To achieve this, we first
need to assess the joint distribution of the system’s continuous lifetime,
<span class="math inline">\(T_i\)</span>, and the discrete candidate set, <span class="math inline">\(\mathcal{C}_i\)</span>, which can be written as
<span class="math display">\[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = f_{T_i}(t_i;\boldsymbol{\theta})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\},
\]</span>
where <span class="math inline">\(f_{T_i}(t_i;\boldsymbol{\theta})\)</span> is the pdf of <span class="math inline">\(T_i\)</span> and
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\)</span> is the conditional
pmf of <span class="math inline">\(\mathcal{C}_i\)</span> given <span class="math inline">\(T_i = t_i\)</span>.</p>
<p>We assume the pdf <span class="math inline">\(f_{T_i}(t_i;\boldsymbol{\theta})\)</span> is known, but we do not have knowledge
of <span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\)</span>, i.e., the data generating
process for candidate sets is unknown. However, it is critical that the masked
data, <span class="math inline">\(\mathcal{C}_i\)</span>, is correlated with the <span class="math inline">\(i\)</span> system.
This way, the conditional distribution of <span class="math inline">\(\mathcal{C}_i\)</span> given <span class="math inline">\(T_i = t_i\)</span> may
provide information about <span class="math inline">\(\boldsymbol{\theta}\)</span>, despite our statistical interest being
primarily in the series system rather than the candidate sets.</p>
To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which <span class="math inline">\(\mathcal{C}_i\)</span> is correlated with the
<span class="math inline">\(i\)</span> system is given by assuming the following condition.
<p>Assuming Condition , <span class="math inline">\(\mathcal{C}_i\)</span> must contain the
index of the failed component, but we can say little else about what other
component indices may appear in <span class="math inline">\(\mathcal{C}_i\)</span>.
In order to derive the joint distribution of <span class="math inline">\(\mathcal{C}_i\)</span> and <span class="math inline">\(T_i\)</span> assuming
Condition , we take the following approach.
We notice that <span class="math inline">\(\mathcal{C}_i\)</span> and <span class="math inline">\(K_i\)</span> are statistically dependent.
We denote the conditional pmf of <span class="math inline">\(\mathcal{C}_i\)</span> given <span class="math inline">\(T_i = t_i\)</span> and
<span class="math inline">\(K_i = j\)</span> as
<span class="math display">\[
\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
\]</span></p>
<p>Even though <span class="math inline">\(K_i\)</span> is not observable in our masked data model, we can still
consider the joint distribution of <span class="math inline">\(T_i\)</span>, <span class="math inline">\(K_i\)</span>, and <span class="math inline">\(\mathcal{C}_i\)</span>.
By Theorem <a href="#thm:f-k-and-t">2.4</a>, the joint pdf of <span class="math inline">\(T_i\)</span> and <span class="math inline">\(K_i\)</span> is given by
<span class="math display">\[
f_{T_i,K_i}(t_i,j;\boldsymbol{\theta}) = h_j(t_i;\boldsymbol{\theta_j}) \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l}),
\]</span>
where <span class="math inline">\(h_j(t_i;\boldsymbol{\theta_j})\)</span> and <span class="math inline">\(R_j(s_i;\boldsymbol{\theta_j})\)</span> are respectively the hazard
and reliability functions of the <span class="math inline">\(j\)</span> component.
Thus, the joint pdf of <span class="math inline">\(T_i\)</span>, <span class="math inline">\(K_i\)</span>, and <span class="math inline">\(\mathcal{C}_i\)</span> may be written as
<span class="math display">\[\begin{equation}
\label{eq:joint-pdf-t-k-c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\boldsymbol{\theta})
    &amp;= f_{T_i,K_i}(t_i,k;\boldsymbol{\theta}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &amp;= h_j(t_i;\boldsymbol{\theta_j}) \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation}\]</span>
We are going to need the joint pdf of <span class="math inline">\(T_i\)</span> and <span class="math inline">\(\mathcal{C}_i\)</span>, which
may be obtained by summing over the support <span class="math inline">\(\{1,\ldots,m\}\)</span> of <span class="math inline">\(K_i\)</span> in
Equation <a href="#eq:joint-pdf-t-k-c">(<strong>??</strong>)</a>,
<span class="math display">\[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\]</span>
By Condition ,
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0\)</span> when <span class="math inline">\(K_i = j\)</span> and
<span class="math inline">\(j \notin c_i\)</span>, and so we may rewrite the joint pdf of <span class="math inline">\(T_i\)</span> and
<span class="math inline">\(\mathcal{C}_i\)</span> as
<span class="math display">\[\begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}\]</span></p>
<p>When we try to find an MLE of <span class="math inline">\(\boldsymbol{\theta}\)</span> (see Section <a href="#mle">4</a>), we
solve the simultaneous equations of the MLE and choose a solution
<span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> that is a maximum for the likelihood function.
When we do this, we find that <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> depends on the unknown
conditional pmf <span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\)</span>.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\)</span>.</p>
<p>According to <span class="citation">[<a href="#ref-Fran-1991" role="doc-biblioref">3</a>]</span>, in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In this
setting, Condition  generally holds.
Assuming Conditions  and
,
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\)</span> may be factored out of the
summation in Equation <a href="#eq:part1">(<strong>??</strong>)</a>, and thus the joint pdf of <span class="math inline">\(T_i\)</span> and
<span class="math inline">\(\mathcal{C}_i\)</span> may be rewritten as
<span class="math display">\[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\} \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j})
\]</span>
where <span class="math inline">\(j&#39; \in c_i\)</span>.</p>
If <span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\}\)</span> is a function of
<span class="math inline">\(\boldsymbol{\theta}\)</span>, the MLEs are still dependent on the unknown
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\}\)</span>.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
<span class="math inline">\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\}\)</span> to find an MLE of
<span class="math inline">\(\boldsymbol{\theta}\)</span>. The last condition we assume achieves this result.
<p>When Conditions , ,
and  are satisfied, the joint pdf of <span class="math inline">\(T_i\)</span> and
<span class="math inline">\(\mathcal{C}_i\)</span> is given by
<span class="math display">\[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \beta_i \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}).
\]</span>
When we fix the sample and allow <span class="math inline">\(\boldsymbol{\theta}\)</span> to vary, we obtain the
contribution to the likelihood <span class="math inline">\(L\)</span> from the <span class="math inline">\(i\)</span> observation
when the system lifetime is exactly known (<span class="math inline">\(\delta_i = 1\)</span>) but the
component cause of failure is masked by a candidate set <span class="math inline">\(c_i\)</span>:
<span class="math display">\[\begin{equation}
\label{eq:likelihood-contribution-masked}
L_i(\boldsymbol{\theta}) \propto \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}),
\end{equation}\]</span>
where we dropped the factor <span class="math inline">\(\beta_i\)</span> since it is not a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>To summarize this result, assuming Conditions ,
, and ,
if we observe an exact system failure time for the <span class="math inline">\(i\)</span>
system (<span class="math inline">\(\delta_i = 1\)</span>), but the component that failed is masked by a
candidate set <span class="math inline">\(c_i\)</span>, then its likelihood contribution is given by Equation
<a href="#eq:likelihood-contribution-masked">(<strong>??</strong>)</a>.</p>
</div>
<div id="right-censored-data" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Right-Censored Data<a href="#right-censored-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As described in Section <a href="#like-model">3</a>, we observe realizations of
<span class="math inline">\((S_i,\delta_i,\mathcal{C}_i)\)</span> where <span class="math inline">\(S_i = \min\{T_i,\tau\}\)</span> is the
right-censored system lifetime, <span class="math inline">\(\delta_i = 1_{T_i &lt; \tau}\)</span> is
the event indicator, and <span class="math inline">\(\mathcal{C}_i\)</span> is the candidate set.</p>
<p>In the previous section, we discussed the likelihood contribution from an
observation of a masked component cause of failure, i.e., <span class="math inline">\(\delta_i = 1\)</span>.
We now derive the likelihood contribution of a <em>right-censored</em> observation,
<span class="math inline">\(\delta_i = 0\)</span>, in our masked data model.</p>
<div class="theorem">
<p><span id="thm:joint_s_d_c" class="theorem"><strong>(#thm:joint_s_d_c) </strong></span>The likelihood contribution of a right-censored observation <span class="math inline">\((\delta_i = 0)\)</span>
is given by
<span class="math display">\[\begin{equation}
L_i(\boldsymbol{\theta}) = \prod_{l=1}^m R_l(s_i;\boldsymbol{\theta_l}).
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>When right-censoring occurs, then <span class="math inline">\(S_i = \tau\)</span>, and we only know that
<span class="math inline">\(T_i &gt; \tau\)</span>, and so we integrate over all possible values that it may have
obtained,
<span class="math display">\[
L_i(\boldsymbol{\theta}) = \Pr\!{}_{\boldsymbol{\theta}}\{T_i &gt; s_i\}.
\]</span>
By definition, this is just the survival or reliability function of the series system
evaluated at <span class="math inline">\(s_i\)</span>,
<span class="math display">\[
L_i(\boldsymbol{\theta}) = R_{T_i}(s_i;\boldsymbol{\theta}) = \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l}).
\]</span></p>
</div>
<p>When we combine the two likelihood contributions, we obtain the likelihood
contribution for the <span class="math inline">\(i\)</span> system shown in Theorem
,
<span class="math display">\[
L_i(\boldsymbol{\theta}) \propto
\begin{cases}
    \prod_{l=1}^m R_l(s_i;\boldsymbol{\theta_l})         &amp;\text{ if } \delta_i = 0\\
    \prod_{l=1}^m R_l(t_i;\boldsymbol{\theta_l})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &amp;\text{ if } \delta_i = 1.
\end{cases}
\]</span>
We use this result in Section <a href="#mle">4</a> to derive the maximum likelihood
estimator (MLE) of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<div id="identifiability" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Identifiability and Convergence Issues<a href="#identifiability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our likelihood model, masking and right-censoring can lead to issues related
to identifiability and flat likelihood regions.
Identifiability refers to the unique mapping of the model parameters to the
likelihood function, and lack of identifiability can lead
to multiple sets of parameters that explain the data equally well, making inference
about the true parameters challenging <span class="citation">[<a href="#ref-lehmann1998theory" role="doc-biblioref">4</a>]</span>, while
flat likelihood regions can complicate convergence <span class="citation">[<a href="#ref-wu1983convergence" role="doc-biblioref">5</a>]</span>.</p>
<p>In our simulation study, we address these challenges in a pragmatic way. Specifically,
failure to converge to a solution within a maximum of 125 iterations is interpreted as
evidence of the aforementioned issues, leading to the discarding of the sample.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
However, in Section <a href="#boot">5</a>, where we discuss the bias-corrected and
accelerated (BCa) bootstrap method for constructing confidence intervals, we
do not discard any resamples. This strategy helps ensure the robustness of the
results, while acknowledging the inherent complexities of likelihood-based
estimation in models characterized by masking and right-censoring. In our
simulation study, we report the convergence rates, and find that for most
scenarios, the convergence rate is greater than <span class="math inline">\(95\%\)</span> (<span class="math inline">\(100\%\)</span> once
the sample is sufficiently large).</p>
</div>
</div>
<div id="mle" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Maximum Likelihood Estimation<a href="#mle" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In our analysis, we use maximum likelihood estimation (MLE) to estimate the series
system parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> from the masked data <span class="citation">[<a href="#ref-bain1992" role="doc-biblioref">6</a>], [<a href="#ref-casella2002statistical" role="doc-biblioref">7</a>]</span>.
The MLE finds parameter values that maximize the likelihood of the observed data
under the assumed model. A maximum likelihood estimate, <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, is a
solution of
<span class="math display">\[\begin{equation}
\label{eq:mle}
L(\hat{\boldsymbol{\theta}}) = \max_{\boldsymbol{\theta }\in \boldsymbol{\Omega}} L(\boldsymbol{\theta}),
\end{equation}\]</span>
where <span class="math inline">\(L(\boldsymbol{\theta})\)</span> is the likelihood function of the observed data. For computational
efficiency and analytical simplicity, we work with the log-likelihood function,
denoted as <span class="math inline">\(\ell(\boldsymbol{\theta})\)</span>, instead of the likelihood function <span class="citation">[<a href="#ref-casella2002statistical" role="doc-biblioref">7</a>]</span>.</p>
<div class="theorem">
<p><span id="thm:loglike-total" class="theorem"><strong>Theorem 4.1  </strong></span>The log-likelihood function, <span class="math inline">\(\ell(\boldsymbol{\theta})\)</span>, for our masked data model is the sum of the log-likelihoods for each observation,
<span class="math display">\[\begin{equation}
\label{eq:loglike}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\theta}),
\end{equation}\]</span>
where <span class="math inline">\(\ell_i(\boldsymbol{\theta})\)</span> is the log-likelihood contribution for the <span class="math inline">\(i\)</span> observation:
<span class="math display">\[\begin{equation}
\ell_i(\boldsymbol{\theta}) = \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) +
    \delta_i \log \biggl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \biggr).
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>The log-likelihood function is the logarithm of the likelihood function,
<span class="math display">\[
\ell(\boldsymbol{\theta}) = \log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n L_i(\boldsymbol{\theta}) =
    \sum_{i=1}^n \log L_i(\boldsymbol{\theta}).
\]</span>
Substituting <span class="math inline">\(L_i(\boldsymbol{\theta})\)</span> from Equation <a href="#eq:like">(<strong>??</strong>)</a>, we consider these two
cases of <span class="math inline">\(\delta_i\)</span> separately to obtain the result in Theorem
<a href="#thm:loglike-total">4.1</a>.
: If the <span class="math inline">\(i\)</span> system is right-censored (<span class="math inline">\(\delta_i = 0\)</span>),
<span class="math display">\[
\ell_i(\boldsymbol{\theta}) = \log \prod_{l=1}^m R_l(s_i;\boldsymbol{\theta_l}) =
    \sum_{l=1}^m \log R_l(s_i;\boldsymbol{\theta_l}).
\]</span>
: If the <span class="math inline">\(i\)</span> system’s component cause of failure is masked but
the failure time is known (<span class="math inline">\(\delta_i = 1\)</span>),
<span class="math display">\[
\ell_i(\boldsymbol{\theta}) = \sum_{l=1}^m \log R_l(t_i;\boldsymbol{\theta_l}) + \log \beta_i +
    \log \biggl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})\biggr).
\]</span>
By Condition ,
we may discard the <span class="math inline">\(\log \beta_i\)</span> term since it does not depend on <span class="math inline">\(\boldsymbol{\theta}\)</span>,
giving us the result
<span class="math display">\[
\ell_i(\boldsymbol{\theta}) = \sum_{l=1}^m \log R_l(s_i;\boldsymbol{\theta_l}) +
    \log \biggl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \biggr).
\]</span>
Combining these two cases gives us the result in Theorem <a href="#thm:loglike-total">4.1</a>.</p>
</div>
<p>The MLE, <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, is often found by solving a system of equations
derived from setting the derivative of the log-likelihood function to zero
<span class="citation">[<a href="#ref-bain1992" role="doc-biblioref">6</a>]</span>, i.e.,
<span class="math display">\[\begin{equation}
\label{eq:mle-eq}
\frac{\partial}{\partial \theta_j} \ell(\boldsymbol{\theta}) = 0,
\end{equation}\]</span>
for each component <span class="math inline">\(\theta_j\)</span> of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>. When there’s no
closed-form solution, we resort to numerical methods like the Newton-Raphson
method.</p>
<p>Assuming some regularity conditions, such as the likelihood function being
identifiable, the MLE has many desirable asymptotic properties that underpin
statistical inference, namely that it is an asymptotically unbiased estimator
of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> and it is normally distributed with a variance given
by the inverse of the Fisher Information Matrix (FIM) <span class="citation">[<a href="#ref-casella2002statistical" role="doc-biblioref">7</a>]</span>.
However, for smaller samples, these asymptotic properties may not yield accurate
approximations. We propose to use the bootstrap method to offer an empirical
approach for estimating the sampling distribution of the MLE, in particular for
computing confidence intervals.</p>
</div>
<div id="boot" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Bias-Corrected and Accelerated Bootstrap Confidence Intervals<a href="#boot" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We utilize the non-parametric bootstrap to estimate the sampling distribution
of the MLE. In the non-parametric bootstrap, we resample from the observed data
with replacement to generate a bootstrap sample. The MLE is then computed for
the bootstrap sample. This process is repeated to generate numerous bootstrap
replicates of the MLE. The sampling distribution of the MLE is then estimated
by the empirical distribution of the bootstrap replicates of the MLE.</p>
<p>Our main focus is on generating confidence intervals for the MLE. The method we
use to generate confidence intervals is known as Bias-Corrected and Accelerated
Bootstrap Confidence Intervals (BCa) <span class="citation">[<a href="#ref-efron1987better" role="doc-biblioref">8</a>]</span>, which applies two
corrections to the standard bootstrap method:</p>
<ul>
<li><p><strong>Bias Correction</strong>: This adjusts for bias in the bootstrap distribution itself.
This bias is measured as the difference between the mean of the bootstrap
distribution and the observed statistic. It works by transforming the
percentiles of the bootstrap distribution to correct for these issues.<br />
This may be a useful adjustment in our case since we are dealing with
small samples with two potential sources of bias: right-censoring and
masking component cause of failure.</p></li>
<li><p><strong>Acceleration</strong>: This adjusts for the rate of change of the statistic as a
function of the true, unknown parameter. This correction is important when the
shape of the statistic’s distribution changes with the true parameter.
Since we have a number of different shape parameters, <span class="math inline">\(k_1,\ldots,k_m\)</span>, we may
expect the shape of the distribution of the MLE to change as a function of the
true parameter, making this correction potentially useful.</p></li>
</ul>
<div id="correctly-specified-confidence-intervals" class="section level4 unnumbered hasAnchor" number="">
<h4>Correctly Specified Confidence Intervals<a href="#correctly-specified-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since we are primarily interested in generating confidence intervals for small
samples for a biased MLE, the BCa method is a reasonable choice for our
simulation study. In our simulation study, we evaluate the performance of the
BCa confidence intervals by calculating their coverage probability. A <em>correctly
specified</em> <span class="math inline">\(95\%\)</span> confidence interval contains the true parameter value
approximately <span class="math inline">\(95\%\)</span> of the time in repeated sampling.</p>
<p>In this study, we consider a coverage probability above <span class="math inline">\(90\%\)</span> to be
satisfactory, as it offers a reasonable trade-off between precision and
reliability. A coverage probability below this would signify undue confidence in
the precision of the MLE. Conversely, a coverage probability near <span class="math inline">\(100\%\)</span> may
indicate excessively wide intervals, thereby diminishing the precision of the
MLE. Our objective is to construct confidence intervals that are as narrow as
possible while achieving good empirical coverage close to the nominal level of
<span class="math inline">\(95\%\)</span>.</p>
<div id="challenges" class="section level5 unnumbered hasAnchor" number="">
<h5>Challenges<a href="#challenges" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>While the bootstrap method provides a robust and flexible tool for statistical
estimation, its effectiveness can be influenced by many factors. A few of these
factors are particularly relevant to our study:</p>
<ul>
<li><strong>Convergence</strong>: Instances of non-convergence in our bootstrap samples were
observed, which can occur when the estimation method, like the MLE used in our
analysis, fails to converge due to the specifics of the resampled data
<span class="citation">[<a href="#ref-casella2002statistical" role="doc-biblioref">7</a>]</span>. This issue can potentially introduce bias or
reduce the effective sample size of our bootstrap distribution.</li>
<li><strong>Small Samples</strong>: The bootstrap’s accuracy can be compromised with small
sample sizes, as the method relies on the Law of Large Numbers to approximate
the true sampling distribution. For small samples, the bootstrap resamples might
not adequately represent the true variability in the data, leading to inaccurate
results <span class="citation">[<a href="#ref-efron1994introduction" role="doc-biblioref">9</a>]</span>.</li>
<li><strong>Masking</strong>: Our data involves right censoring and a masking of the component
cause of failure when a system failure is observed. These aspects can cause
certain data points or trends to be underrepresented or not represented at all
in our data, introducing bias in the bootstrap distribution <span class="citation">[<a href="#ref-klein2005survival" role="doc-biblioref">10</a>]</span>.</li>
</ul>
<p>Despite these challenges, we found the bootstrap method useful in constructing
correctly specified confidence intervals.</p>
</div>
</div>
</div>
<div id="weibull" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Series System with Weibull Components<a href="#weibull" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The Weibull distribution, introduced by Waloddi Weibull in 1937, has been
instrumental in reliability analysis due to its ability to model a wide range
of failure behaviors. Reflecting on its utility, Weibull
modestly noted that it “[…] may sometimes render good service.” <span class="citation">[<a href="#ref-Abernethy2006" role="doc-biblioref">11</a>]</span>
In the context of our study, we model a system as originating from components
with Weibull distributed lifetimes arranged in a series configuration,
producing a specific form of the likelihood model described in Section <a href="#like-model">3</a>,
which deals with challenges such as right censoring and masked component cause of failure.</p>
<p>The <span class="math inline">\(j\)</span> component of the <span class="math inline">\(i\)</span> has a
lifetime distribution given by
<span class="math display">\[
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j) \qquad \text{for } i = 1,\ldots,n \text{ and } j = 1,\ldots,m,
\]</span>
where <span class="math inline">\(\lambda_j &gt; 0\)</span> is the scale parameter and <span class="math inline">\(k_j &gt; 0\)</span> is the shape parameter.
The <span class="math inline">\(j\)</span> component has a reliability function, pdf, and hazard function
given respectively by
<span class="math display">\[\begin{align}
    R_j(t;\lambda_j,k_j)
        &amp;= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &amp;= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull-haz}
        &amp;= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}.
\end{align}\]</span></p>
<p>The shape parameter of the Weibull distribution is of particular importance:</p>
<ul>
<li><span class="math inline">\(k_j &lt; 1\)</span> indicates infant mortality. An example of how this might arise is
a result of defective components being weeded out early, and the remaining
components surviving for a much longer time.</li>
<li><span class="math inline">\(k_j = 1\)</span> indicates random failures (independent of age). An example of how
this might arise is a result of random shocks to the system, but otherwise
the system is age-independent.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></li>
<li><span class="math inline">\(k_j &gt; 1\)</span> indicates wear-out failures. An example of how this might arise is a
result of components wearing as they age</li>
</ul>
<p>We show that the lifetime of the series system composed of <span class="math inline">\(m\)</span> Weibull
components has a reliability, hazard, and probability density functions given by
the following theorem.</p>
<div class="theorem">
<p><span id="thm:sys_weibull" class="theorem"><strong>(#thm:sys_weibull) </strong></span>The lifetime of a series system composed of <span class="math inline">\(m\)</span> Weibull components
has a reliability function, hazard function, and pdf respectively given by
<span class="math display">\[\begin{align}
\label{eq:sys-weibull-reliability-fn}
R_{T_i}(t;\boldsymbol{\theta}) &amp;= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
\label{eq:sys-weibull-failure-rate-fn}
h_{T_i}(t;\boldsymbol{\theta}) &amp;= \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},\\
\label{eq:sys-weibull-pdf}
f_{T_i}(t;\boldsymbol{\theta}) &amp;= \biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\},
\end{align}\]</span>
where <span class="math inline">\(\boldsymbol{\theta }= (k_1, \lambda_1, \ldots, k_m, \lambda_m)\)</span> is the parameter
vector of the series system and <span class="math inline">\(\boldsymbol{\theta_j} = (k_j, \lambda_j)\)</span> is the
parameter vector of the <span class="math inline">\(j\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>The proof for the reliability function follows from Theorem <a href="#thm:sys-reliability-fn">2.1</a>,
<span class="math display">\[
R_{T_i}(t;\boldsymbol{\theta}) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
\]</span>
Plugging in the Weibull component reliability functions obtains the result
<span class="math display">\[\begin{align*}
R_{T_i}(t;\boldsymbol{\theta})
    = \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}
    = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}\]</span>
The proof for the hazard function follows from Theorem <a href="#thm:sys-failure-rate">2.3</a>,
<span class="math display">\[\begin{align*}
h_{T_i}(t;\boldsymbol{\theta})
    = \sum_{j=1}^{m} h_j(t;\boldsymbol{\theta_j})
    = \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}.
\end{align*}\]</span>
The proof for the pdf follows from Theorem <a href="#thm:sys-pdf">2.2</a>. By definition,
<span class="math display">\[
f_{T_i}(t;\boldsymbol{\theta}) = h_{T_i}(t;\boldsymbol{\theta}) R_{T_i}(t;\boldsymbol{\theta}).
\]</span>
Plugging in the failure rate and reliability functions given respectively by
Equations <a href="#eq:sys-weibull-reliability-fn">(<strong>??</strong>)</a> and
<a href="#eq:sys-weibull-failure-rate-fn">(<strong>??</strong>)</a> completes the proof.</p>
</div>
<p>In Section <a href="#reliability">2.2</a>, we discussed the concept of reliability, with
the MTTF being a common measure of reliability. In the case of Weibull
components, the MTTF of the <span class="math inline">\(j\)</span> component is given by
<span class="math display">\[\begin{equation}
\label{eq:mttf-weibull}
\text{MTTF}_j = \lambda_j \Gamma\biggl(1 + \frac{1}{k_j}\biggr),
\end{equation}\]</span>
where <span class="math inline">\(\Gamma\)</span> is the gamma function. We mentioned that the MTTF can sometimes
be a poor measure of reliability, e.g., the MTTF and the probability of failing
early can <em>both</em> be large. The Weibull is a good example of this phenomenon.
If <span class="math inline">\(k_j &gt; 1\)</span>, the <span class="math inline">\(j\)</span> component is fat-tailed and it can
exhibit both a large MTTF and a high probability of failing early. The
probability of component failure given by Equation <a href="#eq:prob-k">(<strong>??</strong>)</a> is a
particularly useful measure of component reliability relative to the other
components in the system.</p>
<div id="sys-weibull-like" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Likelihood Model<a href="#sys-weibull-like" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="#like-model">3</a>, we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
<span class="math inline">\(i\)</span> system is given by the following theorem.</p>
<div class="theorem">
<p><span id="thm:weibull-likelihood-contribution" class="theorem"><strong>Theorem 6.1  </strong></span>Let <span class="math inline">\(\delta_i\)</span> be an indicator variable that is 1 if the
<span class="math inline">\(i\)</span> system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the <span class="math inline">\(i\)</span> system is given by
<span class="math display">\[\begin{equation}
\label{eq:weibull-likelihood-contribution}
L_i(\boldsymbol{\theta}) \propto
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    &amp; \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} &amp; \text{if } \delta_i = 0.
\end{cases}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>By Theorem <a href="#thm:likelihood-contribution">3.1</a>, the likelihood contribution of the
<span class="math inline">\(i\)</span> system is given by
<span class="math display">\[
L_i(\boldsymbol{\theta}) \propto
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                       &amp;\text{ if } \delta_i = 0\\
    R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &amp;\text{ if } \delta_i = 1.
\end{cases}
\]</span>
By Equation <a href="#eq:sys-weibull-reliability-fn">(<strong>??</strong>)</a>, the system reliability
function is given by
<span class="math display">\[
R_{T_i}(t_i;\boldsymbol{\theta}) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\},
\]</span>
where <span class="math inline">\(\boldsymbol{\theta }= (k_1,\lambda_1,\ldots,k_m,\lambda_m)\)</span> is the parameter vector and by
Equation <a href="#eq:weibull-haz">(<strong>??</strong>)</a>, the hazard function of the <span class="math inline">\(j\)</span>
component is given by
<span class="math display">\[
h_j(t_i;\boldsymbol{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1},
\]</span>
where <span class="math inline">\(\boldsymbol{\theta_j} = (k_j,\lambda_j)\)</span> is the parameter vector of the
<span class="math inline">\(j\)</span> component. Plugging these into the likelihood contribution
function obtains the result.</p>
</div>
<p>Taking the log of the likelihood contribution function obtains the following result.</p>
<div class="corollary">
<p><span id="cor:cor:weibull-log-likelihood-contribution" class="corollary"><strong>(#cor:cor:weibull-log-likelihood-contribution) </strong></span>The log-likelihood contribution of the <span class="math inline">\(i\)</span> system is given by
<span class="math display">\[\begin{equation}
\label{eq:weibull-log-likelihood-contribution}
\ell_i(\boldsymbol{\theta}) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}\]</span>
where we drop any terms that do not depend on <span class="math inline">\(\boldsymbol{\theta}\)</span> since they do not
affect the MLE.</p>
</div>
<p>See Appendix <a href="#app-weibull-loglik-r">A.1</a> for the R code that implements the log-likelihood function
for the series system with Weibull components.</p>
<p>We find an MLE by solving <a href="#eq:mle-eq">(<strong>??</strong>)</a>, i.e., a point
<span class="math inline">\(\boldsymbol{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)\)</span>
satisfying <span class="math inline">\(\nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\hat{\boldsymbol{\theta}}}) = \boldsymbol{0}\)</span>, where
<span class="math inline">\(\nabla_{\boldsymbol{\theta}} \ell\)</span> is the gradient of the log-likelihood function (score) with
respect to <span class="math inline">\(\boldsymbol{\theta}\)</span>. To solve this system of equations, we use Newton-like
methods, which sometimes require both the gradient and the Hessian of the
log-likelihood function. We analytically derive the score but we do not do the
same for the Hessian of the log-likelihood function. Our reasoning is based on
the following two observations:</p>
<ul>
<li><p>The score function is relatively easy to derive, and it is useful to have for
computing gradients efficiently and accurately, which will be useful for
accurately numerically approximating the Hessian of the log-likelihood function.</p></li>
<li><p>The Hessian is tedious and error prone to derive, and Newton-like methods
often do not require the Hessian to be explicitly computed.</p></li>
</ul>
<p>The following theorem derives the score function.</p>
<div class="theorem">
<p><span id="thm:weibull-score" class="theorem"><strong>Theorem 6.2  </strong></span>The gradient of the log-likelihood contribution of the <span class="math inline">\(i\)</span> system is given by
<span class="math display">\[\begin{equation}
\label{eq:weibull-score}
\nabla \ell_i(\boldsymbol{\theta}) = \biggl(
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_1},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_m},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_m} \biggr)&#39;,
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}.
\end{equation}\]</span></p>
</div>
<p>The result follows from taking the partial derivatives of the log-likelihood
contribution of the <span class="math inline">\(i\)</span> system given by Equation
<a href="#eq:weibull-likelihood-contribution">(<strong>??</strong>)</a>. It is a tedious calculation so the
proof has been omitted, but the result has been verified by using a very precise
numerical approximation of the gradient.</p>
<p>By the linearity of differentiation, the gradient of a sum of functions is
the sum of their gradients, and so the score function conditioned on the entire
sample is given by
<span class="math display">\[\begin{equation}
\label{eq:weibull-series-score}
\nabla \ell(\boldsymbol{\theta}) = \sum_{i=1}^n \nabla \ell_i(\boldsymbol{\theta}).
\end{equation}\]</span></p>
</div>
<div id="reduced-weibull" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Weibull Series System: Homogeneous Shape Parameters<a href="#reduced-weibull" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a series system, the system is only as reliable as its weakest link
(weakest component). In a well-designed series system, there is no single
component that is much weaker than the others. In the case of components with
Weibull lifetimes, this implies the shape parameters are homogenous and the
scale parameters are homogenous. The shape parameters are particularly important
since they determine the failure behavior of the components.</p>
<p>When the shape parameters are homogenous, the lifetime of the series system with
components that are Weibull distributed is also Weibull distributed, as shown in
the following theorem.</p>
<div class="theorem">
<p><span id="thm:weibull-series-system" class="theorem"><strong>Theorem 6.3  </strong></span>If the shape parameters of the components are homogenous, then the lifetime
series system follows a Weibull distribution with a shape parameter <span class="math inline">\(k\)</span> given by
the identical shape parameters of the components and a scale parameter <span class="math inline">\(\lambda\)</span>
given by
<span class="math display">\[\begin{equation}
\label{eq:sys-weibull-scale}
\lambda = \biggl(\sum_{j=1}^{m} \lambda_j^{-k}\biggr)^{-1/k},
\end{equation}\]</span>
where <span class="math inline">\(\lambda_j\)</span> is the scale parameter of the <span class="math inline">\(j\)</span> component.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Given <span class="math inline">\(m\)</span> Weibull lifetimes <span class="math inline">\(T_{i 1}, \ldots, T_{i m}\)</span> with the same shape
parameter <span class="math inline">\(k\)</span> and scale parameters <span class="math inline">\(\lambda_1, \ldots, \lambda_m\)</span>, the
reliability function of the series system is given by
<span class="math display">\[
R_{T_i}(t;\boldsymbol{\theta}) =
    \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k}\biggr\}.
\]</span>
To show that the series system lifetime is Weibull, we need to find a single
scale parameter <span class="math inline">\(\lambda\)</span> such that
<span class="math display">\[
R_{T_i}(t;\boldsymbol{\theta}) = \exp\biggl\{-\biggl(\frac{t}{\lambda}\biggr)^{k}\biggr\},
\]</span>
which has the solution
<span class="math display">\[
\lambda = \frac{1}{\left(\frac{1}{\lambda_1^k} + \ldots +
    \frac{1}{\lambda_m^k}\right)^{\frac{1}{k}}}.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-12" class="theorem"><strong>Theorem 6.4  </strong></span>If a series system has Weibull components with homogeneous shape parameters, the
component cause of failure is conditionally independent of the system failure
time:
<span class="math display">\[
\Pr\{K_i = j | T_i = t_i \} = \Pr\{K_i = j\} =
    \frac{\lambda_j^{-k}}{\sum_{l=1}^{m} \lambda_l^{-k}}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>By Theorem <a href="#thm:prob-k-given-t">2.6</a>, the conditional probability of the
<span class="math inline">\(j\)</span> component being the cause of failure given the system
failure time is given by
<span class="math display">\[\begin{align*}
\Pr\{K_i = j | T_i = t\}
    &amp;= \frac{f_{K_i, T_i}(j, t;\boldsymbol{\theta})}{f_{T_i}(t;\boldsymbol{\theta})}
    = \frac{h_j(t;k,\lambda_j) R_{T_i}(t;\boldsymbol{\theta})}
        {h_{T_i}(t;\boldsymbol{\theta_j}) R_{T_i}(t;\boldsymbol{\theta})}\\
    &amp;= \frac{h_j(t;k,\lambda_j)}{\sum_{l=1}^m h_l(t;k,\lambda_l)}
    = \frac{\frac{k}{\lambda_j}\bigl(\frac{t}{\lambda_j}\bigr)^{k-1}}
        {\sum_{l=1}^m \frac{k}{\lambda_l}\bigl(\frac{t}{\lambda_l}\bigr)^{k-1}}
    = \frac{\bigl(\frac{1}{\lambda_j}\bigr)^k}
        {\sum_{l=1}^m \bigl(\frac{1}{\lambda_l}\bigr)^k}.
\end{align*}\]</span></p>
</div>
<p>According to the bias-variance trade-off, we expect the MLE of the homogenous
model, which has <span class="math inline">\(m+1\)</span> parameters (<span class="math inline">\(m\)</span> being he number of components in the
series system), to be more biased but have less variance than the MLE of the
full model, which has <span class="math inline">\(2m\)</span> parameters.</p>
</div>
</div>
<div id="sim-study" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Simulation Study: Series System with Weibull Components<a href="#sim-study" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this simulation study, we assess the sensitivity of the MLE and BCa
confidence intervals to various simulation scenarios for the likelihood model
defined in Section <a href="#weibull">6</a>. We begin by specifying the parameters of the
series system that will be the central object of our simulation study. We
consider the data in <span class="citation">[<a href="#ref-Huairu-2013" role="doc-biblioref">12</a>]</span>, in which they study the reliability of a
series system with three components. They fit Weibull components in a series
configuration to the data, resulting in an MLE with shape and scale estimates
given by the first three components in Table . To make the
model slightly more complex, we add two more components to this series system,
with shape and scale parameters given by the last two components in Table
. We will refer to this system as the <strong>base</strong> system.</p>
<p>In Section <a href="#reliability">2.2</a>, we defined a well-designed series system as one
that consists of components with similar reliabilities, where we define
reliability in three ways: the reliability function, MTTF, and probability that
a specific component will be the cause of failure (which is a measure of
relative reliability of the components). We will use these three measures of
reliability to assess the base system. The base system
defined in Table  satisfies this definition of being a
well-designed system since there are no components that are significantly
less reliable than any of the others, component 1 being the most reliable and
component 3 being the least reliable.</p>
<p>The reliability function, unlike the other two measures of reliability, is not
a summary statistic of reliability, but is rather a function of time. Since most
of our simulations has the right-censoring time set to the <span class="math inline">\(82.5\%\)</span> quantile of
the series system, which we denote here by <span class="math inline">\(\tau_{0.825}\)</span>, we can compare the
reliability functions of the components at this time. We see that the
reliability of the components at this right-censoring time are similar, with
component 1 being the most reliable and component 3 being the least reliable.
These results are consistent with the previous analysis based on the MTTF and
probability of component cause of failure being similar.</p>
<p>The shape parameters for each component is larger than <span class="math inline">\(1\)</span>, which means each
component has a failure characteristic that is more wear-out than infant
mortality. The system as a whole is therefore more likely to fail due to
wear-out failures than infant mortality. This too is consistent with a
well-designed system.</p>
<div id="homogenous-shape-parameters" class="section level4 unnumbered hasAnchor" number="">
<h4>Homogenous Shape Parameters<a href="#homogenous-shape-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The base system is a well-designed system, and so it is likely that the
likelihood model that assumes homogeneous shape parameters described in Section
<a href="#reduced-weibull">6.2</a> would provide a good fit to any data generated from this
system. We performed a preliminary investigation into this by simulating data
from the base system, and deviations from the base system that make the system
less well-designed, and fitting the homogeneous model to the data. We found that
the MLE of the homogeneous model was very close to the true parameter values for
slight deviations from the base system, but the MLE was biased for larger
deviations from the base system. This is consistent with the bias-variance
trade-off, where the MLE of the homogeneous model is more biased but has less
variance than the MLE of the full model. We do not explore this further in this
simulation study, but it is an interesting avenue for future research.</p>
<table>
<caption>
<span id="tab:series-sys">Table 7.1: </span>Weibull Components in Series Configuration
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Shape (<span class="math inline">\(k_j\)</span>)
</th>
<th style="text-align:right;">
Scale (<span class="math inline">\(\lambda_j\)</span>)
</th>
<th style="text-align:right;">
MTTF<span class="math inline">\(_j\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\Pr\{K_i = j\}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(R_j(\tau_{0.825};k_j,\lambda_j)\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Component 1
</td>
<td style="text-align:right;">
1.2576
</td>
<td style="text-align:right;">
994.3661
</td>
<td style="text-align:right;">
924.869
</td>
<td style="text-align:right;">
0.169
</td>
<td style="text-align:right;">
0.744
</td>
</tr>
<tr>
<td style="text-align:left;">
Component 2
</td>
<td style="text-align:right;">
1.1635
</td>
<td style="text-align:right;">
908.9458
</td>
<td style="text-align:right;">
862.157
</td>
<td style="text-align:right;">
0.207
</td>
<td style="text-align:right;">
0.698
</td>
</tr>
<tr>
<td style="text-align:left;">
Component 3
</td>
<td style="text-align:right;">
1.1308
</td>
<td style="text-align:right;">
840.1141
</td>
<td style="text-align:right;">
803.564
</td>
<td style="text-align:right;">
0.234
</td>
<td style="text-align:right;">
0.667
</td>
</tr>
<tr>
<td style="text-align:left;">
Component 4
</td>
<td style="text-align:right;">
1.1802
</td>
<td style="text-align:right;">
940.1342
</td>
<td style="text-align:right;">
888.237
</td>
<td style="text-align:right;">
0.196
</td>
<td style="text-align:right;">
0.711
</td>
</tr>
<tr>
<td style="text-align:left;">
Component 5
</td>
<td style="text-align:right;">
1.2034
</td>
<td style="text-align:right;">
923.1631
</td>
<td style="text-align:right;">
867.748
</td>
<td style="text-align:right;">
0.195
</td>
<td style="text-align:right;">
0.711
</td>
</tr>
<tr>
<td style="text-align:left;">
Series System
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
222.884
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
0.175
</td>
</tr>
</tbody>
</table>
</div>
<div id="performance-metrics" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Performance Metrics<a href="#performance-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we describe the measures we use to assess the performance of
the MLE and the BCa confidence intervals in our simulation study. We assess two
important properties of the MLE for each simulation scenario:</p>
<ul>
<li><p><strong>Accuracy (Bias)</strong>: A pivotal metric is the proximity of the MLE’s expected
value to the true parameter value. Higher accuracy is demonstrated by a
closer proximity. We estimate the accuracy in our simulation study by plotting
the mean of the MLE.</p></li>
<li><p><strong>Precision</strong>: Another essential measure is the variability of the MLE across
samples. Higher precision is demonstrated by smaller variability. We estimate
this in our simulation study by plotting the quantiles of the sampling
distribution of the MLE.</p></li>
</ul>
<p>In parallel, we assess the following properties of the BCa confidence intervals
described in Section <a href="#boot">5</a>:</p>
<ul>
<li><p><strong>Accuracy (Coverage Probability)</strong>: In Section <a href="#boot">5</a>, we discussed
the importance of the coverage probability of the confidence intervals. The
coverage probability is the proportion of the computed confidence intervals
that contain the true parameter values. Ideally, we would like the CIs to
contain the true parameter values around <span class="math inline">\(95\%\)</span> of the time, which is the
nominal coverage probability. In this case, the CIs are said to be
correctly specified, but we consider anything over <span class="math inline">\(90\%\)</span> satisfactory.</p></li>
<li><p><strong>Precision</strong>: A useful measure of the precision of the confidence intervals
are their widths. Narrower, consistent intervals across samples indicate
higher precision. We measure this in our simulation study by plotting the
<em>median-aggregated</em> CIs, where we aggregate the CIs by taking the medians of
the upper and lower bounds. This gives us a sense of the central tendency
of the CIs. There is a trade-off between accuracy and precision, where we
want the CIs to be as narrow as possible with nominal coverage probability.</p></li>
</ul>
<p>If the confidence intervals are not accurate, then we cannot be sure that the
intervals contain the true parameters. If the confidence intervals are not
precise, then there is significant uncertainty in the locations of the true
parameters. In either case, we cannot be confident in the results of the
analysis, and we should consider alternative methods for estimating the
parameters of the series system. However, we will see that the BCa confidence
intervals are accurate and precise for most of the simulation scenarios we
consider, providing a reliable method for estimating the parameters of the
series system.</p>
<p>Finally, we also consider the convergence rate of the MLE, discussed in
Section <a href="#identifiability">3.3</a>. We take a convergence rate less than <span class="math inline">\(95\%\)</span> to
be evidence that the MLE is not a robust or reliable estimator in these cases,
due to the insufficient information.</p>
</div>
<div id="data-gen-proc" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Data Generating Process<a href="#data-gen-proc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we describe the data generating process for our simulation studies.
It consists of three parts: the series system, the candidate set model, and the
right-censoring model.</p>
<div id="series-system-lifetime" class="section level4 unnumbered hasAnchor" number="">
<h4>Series System Lifetime<a href="#series-system-lifetime" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We generate data from a series system with <span class="math inline">\(m = 5\)</span> components with Weibull
lifetimes. As described in Section <a href="#weibull">6</a>, the <span class="math inline">\(j\)</span>
component of the <span class="math inline">\(i\)</span> system has a lifetime distribution
given by
<span class="math display">\[
    T_{i j} \sim \operatorname{Weibull}(k_j, \lambda_j)
\]</span>
and the lifetime of the series system composed of <span class="math inline">\(m\)</span> Weibull components
is defined as
<span class="math display">\[
    T_i = \min\{T_{i 1}, \ldots, T_{i m}\}.
\]</span>
To generate a data set, we first generate the <span class="math inline">\(m\)</span> component failure times,
by efficiently sampling from their respective distributions, and we then set
the failure time <span class="math inline">\(t_i\)</span> of the system to the minimum of the component failure times.</p>
</div>
<div id="right-censoring-model" class="section level4 unnumbered hasAnchor" number="">
<h4>Right-Censoring Model<a href="#right-censoring-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We employ a simple right-censoring model, where the right-censoring time
<span class="math inline">\(\tau\)</span> is fixed at some known value, e.g., an experiment is run for a fixed
amount of time <span class="math inline">\(\tau\)</span>, and all systems that have not failed by the end of the
experiment are right-censored. The censoring time <span class="math inline">\(S_i\)</span> of the
<span class="math inline">\(i\)</span> system is thus given by
<span class="math display">\[
    S_i = \min\{T_i, \tau\}.
\]</span>
So, after we generate the system failure time <span class="math inline">\(T_i\)</span>, we generate the censoring
time <span class="math inline">\(S_i\)</span> by taking the minimum of <span class="math inline">\(T_i\)</span> and <span class="math inline">\(\tau\)</span>.
In our simulation study, we parameterize the right-censoring time <span class="math inline">\(\tau\)</span> by the
quantile <span class="math inline">\(q = 0.825\)</span> of the series system,
<span class="math display">\[
    \tau = F_{T_i}^{-1}(q).
\]</span>
This means that <span class="math inline">\(82.5\%\)</span> of the series systems are expected to fail before time <span class="math inline">\(\tau\)</span>
and <span class="math inline">\(17.5\%\)</span> of the series are expected to be right-censored. To solve for the <span class="math inline">\(82.5\%\)</span>
quantile of the series system, we define the function <span class="math inline">\(g\)</span> as
<span class="math display">\[
g(\tau) = F_{T_i}(\tau;\boldsymbol{\theta}) - q
\]</span>
and find its root using the Newton’s method. See Appendix
<a href="#app-series-quantile">A.3</a> for the R code that implements this procedure.</p>
</div>
<div id="masking-model-for-component-cause-of-failure" class="section level4 unnumbered hasAnchor" number="">
<h4>Masking Model for Component Cause of Failure<a href="#masking-model-for-component-cause-of-failure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We must generate data that satisfies the masking conditions described in
Section <a href="#candmod">3.1</a>.
There are many ways to satisfying the masking conditions. We choose a simple
method, which we call the <em>Bernoulli masking model</em>. In this model, we
satisfy Conditions <a href="#cond:c-contains-k"><strong>??</strong></a>, <a href="#cond:equal-prob-failure-cause"><strong>??</strong></a>,
and <a href="#cond:masked-indept-theta"><strong>??</strong></a> by generating a candidate set <span class="math inline">\(c_i\)</span> for
each system <span class="math inline">\(i\)</span> as follows:</p>
<ul>
<li><p>If the <span class="math inline">\(j\)</span> component fails, it is deterministically
placed in the candidate set. This satisfies Condition <a href="#cond:c-contains-k"><strong>??</strong></a>,
<span class="math inline">\(\Pr\{K_i \in \mathcal{C}_i\} = 1\)</span>.</p></li>
<li><p>For each of the <span class="math inline">\(m-1\)</span> components that did not fail, we generate a Bernoulli
random variable <span class="math inline">\(X_j\)</span> with probability <span class="math inline">\(p\)</span> of being <span class="math inline">\(1\)</span>, where <span class="math inline">\(p\)</span> is a fixed
probability. If <span class="math inline">\(X_j = 1\)</span>, the <span class="math inline">\(j\)</span> component is placed in
the candidate set, which satisfies Condition <a href="#cond:masked-indept-theta"><strong>??</strong></a>,
since the Bernoulli random variables are not a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p></li>
<li><p>Condition <a href="#cond:equal-prob-failure-cause"><strong>??</strong></a> may be the least intuitive of
the three conditions. It states that
<span class="math display">\[
\Pr\{\mathcal{C}_i = c_i | K_i = j, T_i = t_i\} = \Pr\{\mathcal{C}_i = c_i | K_i = j&#39;, T_i = t_i\}
\]</span>
for all <span class="math inline">\(j,j&#39; \in c_i\)</span>. In words, this means that the probability of the
candidate set <span class="math inline">\(\mathcal{C}_i\)</span> being equal to some set <span class="math inline">\(c_i\)</span> is the same when
conditioned on any component in <span class="math inline">\(c_i\)</span> being the cause of failure and the
system failure time <span class="math inline">\(t_i\)</span>. This is satisfied by the Bernoulli masking model
since, first, it is independent of the system failure time <span class="math inline">\(t_i\)</span>, and second,
the probability that a non-failed component is in the candidate set is fixed
at some constant <span class="math inline">\(p\)</span> for all components. To see this, consider the following
example. Suppose we have a system with <span class="math inline">\(m = 3\)</span> components, and the first
component is the cause of failure. Then the probability of the candidate set
being equal to some set <span class="math inline">\(c_i\)</span> is given by
<span class="math display">\[\begin{align}
\Pr\{\mathcal{C}_i = c_i | K_i = 1, T_i = t_i\} =
\begin{cases}
   (1-p)^2  &amp; \text{if } c_i = \{1\}\\
   p(1-p)   &amp; \text{if } c_i = \{1,2\}\\
   (1-p)p   &amp; \text{if } c_i = \{1,3\}\\
   p^2      &amp; \text{if } c_i = \{1,2,3\},
\end{cases}
\end{align}\]</span>
where a non-failed component is in the candidate set with probability <span class="math inline">\(p\)</span>
and otherwise it is not placed in the candidate set with probability <span class="math inline">\(1-p\)</span>.
Now, let’s change the component cause of failure to the second component:
<span class="math display">\[\begin{align}
\Pr\{\mathcal{C}_i = c_i | K_i = 2, T_i = t_i\} =
\begin{cases}
   (1-p)^2  &amp; \text{if } c_i = \{2\}\\
   p(1-p)   &amp; \text{if } c_i = \{1,2\}\\
   (1-p)p   &amp; \text{if } c_i = \{2,3\}\\
   p^2      &amp; \text{if } c_i = \{1,2,3\}.
\end{cases}
\end{align}\]</span>
The same pattern holds for the third component. We see that the probability
of the candidate set being equal to some set <span class="math inline">\(c_i\)</span> is the same for all
components in that <span class="math inline">\(c_i\)</span> and for all system failure times <span class="math inline">\(t_i\)</span>, e.g., if
the probability that <span class="math inline">\(c_i = \{1,2\}\)</span> is <span class="math inline">\(p(1-p)\)</span> when we condition in either
compoent <span class="math inline">\(1\)</span> or component <span class="math inline">\(2\)</span> being the cause of failure, which satisfies
Condition <a href="#cond:equal-prob-failure-cause"><strong>??</strong></a>.</p>
<p>There are many more ways to satisfy the masking conditions, but we choose
the Bernoulli masking model because it is simple to understand and
allows us to easily vary the masking probability <span class="math inline">\(p\)</span> for our simulation
study.</p></li>
</ul>
<p>See Appendix <a href="#app-cand-model-r">A.5</a> for the R code that implements this model.</p>
</div>
</div>
<div id="overview-of-simulations" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Overview of Simulations<a href="#overview-of-simulations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We define a simulation scenario to be some combination of <span class="math inline">\(n\)</span> (sample size),
<span class="math inline">\(p\)</span> (masking probability in our Bernoulli masking model), and <span class="math inline">\(q\)</span>
(right-censoring quantile). We are interested in choosing a small number of
scenarios that are representative of real-world scenarios and that are
interesting to analyze. For how we run a simulation scenario, see Appendix
<a href="#app-sim-study-r">B.1</a>, but here is an outline of the process:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Parameter Initialization</strong>: Fix a combination of simulation parameters to
some value, and vary the remaining parameters. For example, if we want to
assess how the sampling distribution of the MLE changes with respect to
sample size, we might choose some particular values for <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> and then
vary the sample size <span class="math inline">\(n\)</span> over the desired range.</p></li>
<li><p><strong>Data Generation</strong>: Simulate <span class="math inline">\(R \geq 300\)</span> datasets from the Data Generating
Process (DGP) described in Section <a href="#data-gen-proc">7.2</a>.</p></li>
<li><p><strong>MLE Computation</strong>: Compute an MLE for each of the <span class="math inline">\(R\)</span> datasets.</p></li>
<li><p><strong>Statistical Evaluation</strong>: For each of these <span class="math inline">\(R\)</span> MLEs, compute some function
of the MLE, like the BCa confidence intervals. This will give us <span class="math inline">\(R\)</span>
statistics as a Monte-carlo estimate of the sampling distribution of the
statistic.</p></li>
<li><p><strong>Distribution Property Estimation</strong>: Use the <span class="math inline">\(R\)</span> statistics to estimate some
property of the sampling distribution of the statistic, e.g., the mean of the
MLE or the coverage probability of the BCa confidence intervals, with respect
to the parameter we are varying in the scenario, e.g., assess how the
coverage probability of the BCa confidence intervals changes with respect to
sample size.</p></li>
</ol>
<p>In this study, we are focusing on three distinct scenarios.
Section <a href="#effect-censoring">7.4</a> explores how varying the right-censoring affects
the estimator with the masking and sample size fixed. Section <a href="#p-vs-mttf">7.5</a>
explores how varying the masking affects the estimator with the right-censoring
and sample size fixed. Section <a href="#effect-samp-size">7.6</a> explores how varying
the sample size affects the estimator with the right-censoring and masking fixed.
Each scenario aims to provide insights into how these parameters influence the
behavior of MLEs, which is crucial for understanding their performance in
real-world applications.</p>
</div>
<div id="effect-censoring" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Scenario: Assessing the Impact of Right-Censoring<a href="#effect-censoring" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this scenario, we use the well-designed series system described in Table ,
and we vary the right-censoring quantile (<span class="math inline">\(q\)</span>) from <span class="math inline">\(60\%\)</span> to <span class="math inline">\(100\%\)</span>
(no right-censoring), with a component cause of failure masking
probability of <span class="math inline">\(p = 21.5\%\)</span> and sample size <span class="math inline">\(n = 100\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:q-vs-stats"></span>
<embed src="image/5_system_tau_fig.pdf" title="Right-Censoring Quantile ($q$) vs MLE ($p = 0.215, n = 90$)" width="100%" type="application/pdf" />
<p class="caption">
Figure 7.1: Right-Censoring Quantile (<span class="math inline">\(q\)</span>) vs MLE (<span class="math inline">\(p = 0.215, n = 90\)</span>)
</p>
</div>
<p>In Figure , we show the effect of right-censoring on the
MLEs for the shape and scale parameters. The top four plots only show the effect
on the MLEs for the shape and scale parameters of components <span class="math inline">\(1\)</span> and <span class="math inline">\(3\)</span>.
We chose these components because they are the most and least reliable
components, respectively, and so we expect them to be the most and least
sensitive to right-censoring. The bottom two plots show the coverage
probabilities for all parameters.</p>
<div id="background" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Background<a href="#background" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When a right-censoring event occurs, in order to increase the likelihood of the data, the MLE
is nudged in a direction that increases the probability of a right-censoring event at time <span class="math inline">\(\tau\)</span>,
which is given by <span class="math inline">\(R_{T_i}(t;\boldsymbol{\theta})\)</span>, representing a source of bias in the estimate.</p>
<p>To increase <span class="math inline">\(R_{T_i}(\tau)\)</span>, we move in the direction (gradient) of these partial derivatives.
The partial derivatives of <span class="math inline">\(R_{T_i}(\tau)\)</span>
are given by
<span class="math display">\[\begin{align*}
\frac{\partial R_{T_i}(\tau)}{\partial \lambda_j} &amp;= R_{T_i}(\tau;\boldsymbol{\theta}) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \frac{k_j}{\lambda_j},\\
\frac{\partial R_{T_i}(\tau)}{\partial k_j}       &amp;= R_{T_i}(\tau;\boldsymbol{\theta}) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \left(\log \lambda_j - \log \tau\right),
\end{align*}\]</span>
for <span class="math inline">\(j = 1, \ldots, m\)</span>. We see that these partial derivatives are related to the score of a right-censored likelihood contribution in
Theorem <a href="#thm:weibull-score">6.2</a>. Let us analyze the implications these
partial derivatives have on the MLE:</p>
<ul>
<li><p><strong>Effect of Increasing Right-Censoring Quantile</strong>: As the right-censoring
quantile <span class="math inline">\(q\)</span> increases (<span class="math inline">\(\tau\)</span> increases), <span class="math inline">\(R_{T_i}(\tau;\boldsymbol{\theta})\)</span> decreases,
reducing the impact of right-censoring on the MLE. This behavior is evident in
Figure .</p></li>
<li><p><strong>Positive Bias in Scale Parameters</strong>: The partial derivatives with respect to
the scale parameters are always positive. This means that right-censoring
introduces a positive bias in the scale parameter estimates, making
right-censoring events more likely. The extent of this bias is related
to the amount of right-censoring (<span class="math inline">\(1-q\)</span>), as seen in Figure .</p></li>
<li><p><strong>Conditional Bias in Shape Parameters</strong>: The partial derivative with respect
to the shape parameter of the <span class="math inline">\(j\)</span> component, <span class="math inline">\(k_j\)</span>, is
non-negative if <span class="math inline">\(\lambda_j \geq \tau\)</span> and otherwise negative. In our
well-designed series system, the scale parameters are large compared to most of
the right-censoring times for <span class="math inline">\(\tau(q)\)</span>, so the MLE nudges the shape parameter
estimates in a positive direction to increase the probability of a
right-censoring event <span class="math inline">\(R_{T_i}(\tau)\)</span> at time <span class="math inline">\(\tau\)</span>. We see this in Figure
, where the shape parameter estimates are positively biased
for most of the quantiles <span class="math inline">\(q\)</span>.</p></li>
</ul>
</div>
<div id="key-observations" class="section level3 hasAnchor" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Key Observations<a href="#key-observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="coverage-probability-cp" class="section level5 unnumbered hasAnchor" number="">
<h5>Coverage Probability (CP)<a href="#coverage-probability-cp" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The confidence intervals are generally correctly specified, obtaining coverages
above <span class="math inline">\(90\%\)</span> for most of the parameters across the entire range of right-censoring
quantiles, and they are converging to the nominal <span class="math inline">\(95\%\)</span> level as the
right-censoring quantile increases. This suggests that the bootstrapped CIs will
contain the true value of the parameters with the specified confidence level
with high probability. The CIs are neither too wide nor too narrow.</p>
<p>However, the scale parameters are better calibrated than the shape parameters.
The scale parameters are consistently around the nominal <span class="math inline">\(95\%\)</span> level for all
right-censoring quantiles, but the shape parameters are consistently less
correctly specified, suggesting that the shape parameters are more difficult to
estimate than the scale parameters.</p>
<p>We also see that the coverage probabilities for <span class="math inline">\(k_1\)</span> and <span class="math inline">\(\lambda_1\)</span> are
generally have worse coverage than the other parameters. This is likely due to
the fact that component 1 is the most reliable component, and so it is less
likely to fail before the right-censoring time <span class="math inline">\(\tau\)</span>. This means that the
likelihood contribution of component 1 is less informative than the other
components. Conversely, we see that <span class="math inline">\(k_3\)</span> and <span class="math inline">\(\lambda_3\)</span> generally have
better coverage than the other parameters. This is likely due to the fact that
component 3 is the least reliable component, and so it is more likely to fail
before the right-censoring time. This means that the likelihood contribution of
component 3 is more informative than the other components in the presence of
right-censoring.</p>
</div>
<div id="dispersion-of-mles" class="section level5 unnumbered hasAnchor" number="">
<h5>Dispersion of MLEs<a href="#dispersion-of-mles" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The shaded regions representing the 95% probability range of the MLEs get
narrower as the right-censoring quantile increases. This is an indicator of the
increased precision in the estimates as more data is available due to decreased
censoring.</p>
<p>We see that the dispersion of the MLEs for <span class="math inline">\(k_1\)</span> and <span class="math inline">\(\lambda_1\)</span> are much
larger than the dispersion of the MLEs for <span class="math inline">\(k_3\)</span> and <span class="math inline">\(\lambda_3\)</span>. This is
consistent with the previous analysis for the coverage probabilities.</p>
</div>
<div id="median-aggregated-cis" class="section level5 unnumbered hasAnchor" number="">
<h5>Median-Aggregated CIs<a href="#median-aggregated-cis" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The median CI (vertical blue bars) decreases in length as the right-censoring
quantile increases. This suggests that the bootstrapped CIs become more
consistent and centered around a tighter range as the right-censoring quantile
increases, while maintaining a good coverage probability. As right-censoring
events become less likely, the bootstrapped CIs gravitate closer to each other
and the true parameter values.</p>
<p>For small right-censoring quantiles, the CIs are quite large, which was
necessary to maintain good coverage. The estimator is sensitive to the data,
and so the bootstrapped CIs are wide to account for this sensitivity when the
sample contains insufficient information due to censoring. Again, we see
that the median-aggregated CIs for <span class="math inline">\(k_1\)</span> and <span class="math inline">\(\lambda_1\)</span> are much wider than
the median-aggregated CIs for <span class="math inline">\(k_3\)</span> and <span class="math inline">\(\lambda_3\)</span>.</p>
</div>
<div id="bias-of-mles" class="section level5 unnumbered hasAnchor" number="">
<h5>Bias of MLEs<a href="#bias-of-mles" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The red dashed line indicating the mean of MLEs initially is quite biased
for the shape parameters, but quickly diminishes to negligible levels as the
right-censoring quantile increases. The bias for the shape parameters never
reach zero, but this is potentially due to masking.</p>
<p>The bias for the scale parameters is quite small and remains stable across
different right-censoring quantiles, suggesting that the scale MLEs are
reasonably unbiased. It could be the case that making or other factors are
counteracting the bias due to right-censoring.</p>
<p>Again, we see that the bias for <span class="math inline">\(k_1\)</span> and <span class="math inline">\(\lambda_1\)</span> is much greater than the
bias for <span class="math inline">\(k_3\)</span> and <span class="math inline">\(\lambda_3\)</span>, which is consistent with the previous analysis.</p>
</div>
<div id="convergence-rate" class="section level5 unnumbered hasAnchor" number="">
<h5>Convergence Rate<a href="#convergence-rate" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The convergence rate increases as the right-censoring quantile <span class="math inline">\(q\)</span>
increases. This is consistent with the expectation that more censoring
reduces the information in a sample, making the likelihood function less
informative and more difficult to identify the parameters that maximize it. We
see that the convergence rate is around <span class="math inline">\(95\%\)</span> or greater for right-censoring
quantiles <span class="math inline">\(q \geq 0.7\)</span>, but drops below <span class="math inline">\(95\%\)</span> for <span class="math inline">\(q &lt; 0.7\)</span>.</p>
</div>
</div>
<div id="summary" class="section level3 hasAnchor" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Summary<a href="#summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this scenario, we find that right-censoring significantly influences the MLEs
of shape and scale parameters. As the degree of right-censoring decreases, the
precision of these estimates improves, and the bias in the shape parameters
diminishes, although it never fully disappears, possibly due to masking effects.
Additionally, Bootstrapped (BCa) confidence intervals generally show good
coverage probabilities, particularly for scale parameters, and become more
focused as right-censoring decreases. As expected, the estimates for the most
reliable component are more sensitive to right-censoring than the least
reliable component. We also find that the convergence rate of the MLE decreases
as the degree of right-censoring increases, suggesting that the MLE is less
reliable in these cases.</p>
<p>These insights set the stage for the subsequent scenario on masking probability,
where we will explore how masking adds another layer of complexity to parameter
estimation and how these effects might be mitigated.</p>
</div>
</div>
<div id="p-vs-mttf" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure<a href="#p-vs-mttf" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this scenario, we use the well-designed series system described in
Table . We fix the sample size to <span class="math inline">\(n = 90\)</span> (moderate sample
size) and we fix the right-censoring quantile to <span class="math inline">\(q = 0.825\)</span> (moderate
right-censoring), and we vary the masking probability <span class="math inline">\(p\)</span> from <span class="math inline">\(0.1\)</span>
(light masking of the component cause of failure) to <span class="math inline">\(0.7\)</span> (extreme masking of
the component cause of failure).</p>
<p>In Figure , we show the effect of the masking
probability <span class="math inline">\(p\)</span> on the MLE for the shape and scale parameters. The top four
plots only show the effect on the MLE for the the shape and scale parameters of
components <span class="math inline">\(1\)</span> and <span class="math inline">\(3\)</span>, since the rest demonstrated similar results. The bottom
left plot shows the coverage probabilities for all parameters and the bottom
right plot shows the convergence rate of the MLE.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:masking-prob-vs-stats"></span>
<embed src="image/5_system_prob_fig.pdf" title="Component Cause of Failure Masking ($p$) vs MLE ($q = 0.825, n = 90$)" type="application/pdf" />
<p class="caption">
Figure 7.2: Component Cause of Failure Masking (<span class="math inline">\(p\)</span>) vs MLE (<span class="math inline">\(q = 0.825, n = 90\)</span>)
</p>
</div>
<div id="background-1" class="section level3 hasAnchor" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Background<a href="#background-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Masking introduces a layer of complexity that is different from right-censoring.
While right-censoring deals with the uncertainty in the timing of failure,
masking adds ambiguity in identifying which component actually failed. In the
Bernoulli masking model, the failed component is guaranteed to be in the
candidate set and each non-failed component is included with a probability <span class="math inline">\(p\)</span>.
This has the following implications on the MLE:</p>
<ul>
<li><p><strong>Ambiguity</strong>: A higher <span class="math inline">\(p\)</span> makes larger candidate sets more probable, making
it less clear which parameter estimates should be adjusted to make the data
more likely in the MLE. This is particularly problematic for components that are
not the cause of failure, since the MLE will adjust their parameters to make
them more likely to be the cause of failure, which is not necessarily correct.</p></li>
<li><p><strong>Bias</strong>: In an ideal scenario, knowing which component failed would allow
the MLE to make that component’s failure more likely and a right-censoring
effect would be applied to the non-failed components. However, a larger masking
probability <span class="math inline">\(p\)</span> introduces uncertainty, causing the MLE to adjust the estimates
for the parameters of the non-failed components to be more likely to fail at the
observed failure time while simultaneously applying a right-censoring effect to
the other components, including the failed component. This introduces a bias
similar to the bias introduced by right-censoring, and the greater the masking
probability <span class="math inline">\(p\)</span>, the greater the bias.</p></li>
<li><p><strong>Precision</strong>: As the masking probability <span class="math inline">\(p\)</span> increases, the likelihood
function becomes less informative, reducing the precision of the estimates.</p></li>
</ul>
</div>
<div id="key-observations-1" class="section level3 hasAnchor" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Key Observations<a href="#key-observations-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="coverage-probability-cp-1" class="section level5 unnumbered hasAnchor" number="">
<h5>Coverage Probability (CP)<a href="#coverage-probability-cp-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>For the scale parameters, the <span class="math inline">\(95\%\)</span> CI is correctly specified for Bernoulli
masking probabilities up to <span class="math inline">\(p = 0.7\)</span>, which is quite significant,
obtaining coverages over <span class="math inline">\(90\%\)</span>. For the shape parameters, the <span class="math inline">\(95\%\)</span> CI is
correctly specified for masking probabilities only up to <span class="math inline">\(p = 0.4\)</span>, which is still
large. At <span class="math inline">\(p = 0.4\)</span>, the coverage probability is around <span class="math inline">\(90%\)</span>, but continues to
drop after that point well below <span class="math inline">\(90\%\)</span>. This suggests that the shape parameters
are more difficult to estimate than the scale parameters, which is consistent
with the previous scenario where we varied the amount of right-censoring.</p>
<p>The BCa confidence intervals are correctly specified for most realistic masking
probabilities, constructing CIs that are neither too wide nor too narrow, but
when the masking is severe and the sample size is small, one should take the CIs
with a grain of salt.</p>
</div>
<div id="dispersion-of-mles-1" class="section level5 unnumbered hasAnchor" number="">
<h5>Dispersion of MLEs<a href="#dispersion-of-mles-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The shaded regions representing the <span class="math inline">\(95\%\)</span> quantile of the MLEs become wider as
the masking probability increases. This is an indicator of the decreased
precision in the estimates when provided with more ambiguous data about the
component cause of failure.</p>
</div>
<div id="median-aggregated-cis-1" class="section level5 unnumbered hasAnchor" number="">
<h5>Median-Aggregated CIs<a href="#median-aggregated-cis-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The median-aggregated CIs (vertical dark blue bars) show that the BCa CIs are
becoming more spread out as the masking probability increases. They are also
asymmetric, with the lower bound being more spread out than the upper-bound,
which is consistent with the behavior of the dispersion of the MLEs. The width
of the CIs consistently increase as the masking probability increases, which we
intuitively expected given the increased uncertainty about the component cause
of failure.</p>
</div>
<div id="bias-of-mles-1" class="section level5 unnumbered hasAnchor" number="">
<h5>Bias of MLEs<a href="#bias-of-mles-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The mean of the MLE (red dashed lines) demonstrates a steadily increasing
positive bias as the masking probability increases. This is consistent with the
expectation that the MLE will apply an increasing right-censoring effect to the
estimates as the masking probability increases.</p>
</div>
<div id="convergence-rate-1" class="section level5 unnumbered hasAnchor" number="">
<h5>Convergence Rate<a href="#convergence-rate-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The convergence rate decreases as the Bernoulli masking probability <span class="math inline">\(p\)</span>
increases. This is consistent with the expectation that a higher masking
probability decreases the information the sample contains about the the
parameters. We see that the convergence rate remains above <span class="math inline">\(95\%\)</span> for masking
probabilities up to <span class="math inline">\(p = 0.4\)</span>, but drops below <span class="math inline">\(95\%\)</span> for <span class="math inline">\(p &gt; 0.4\)</span>. This is
consistent with behavior of the CPs, which drop below <span class="math inline">\(90\%\)</span> for <span class="math inline">\(p &gt; 0.4\)</span>.</p>
</div>
</div>
<div id="summary-1" class="section level3 hasAnchor" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> Summary<a href="#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this scenario, we examine the influence of masking probability <span class="math inline">\(p\)</span> on the
MLE, keeping the sample size and right-censoring constant. As the masking
probability increases, the precision of the MLEs decreases,
and the coverage probability of the CIs begins to drop. However, even at fairly
significant Bernoulli masking probabilities, particularly for the scale
parameters, the CIs have good coverage. These observations highlight the
challenges of parameter estimation under varying degrees of masking and set the
stage for the subsequent scenario on sample size, which shows how increasing the
sample size can mitigate the effects of both right-censoring and masking.</p>
</div>
</div>
<div id="effect-samp-size" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Scenario: Assessing the Impact of Sample Size<a href="#effect-samp-size" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this scenario, we use the well-designed series system described in
Table . We fix the masking probability to <span class="math inline">\(p = 0.215\)</span>
(moderate masking), we fix the right-censoring quantile to <span class="math inline">\(q = 0.825\)</span>
(moderate censoring), and we vary the sample size <span class="math inline">\(n\)</span> from <span class="math inline">\(50\)</span> (small sample
size) to <span class="math inline">\(500\)</span> (large sample size).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:samp-size-n-vs-stats"></span>
<embed src="image/5_system_samp_size_fig.pdf" title="Sample Size ($n$) vs MLEs ($p = 0.215, q = 0.825$)" width="100%" type="application/pdf" />
<p class="caption">
Figure 7.3: Sample Size (<span class="math inline">\(n\)</span>) vs MLEs (<span class="math inline">\(p = 0.215, q = 0.825\)</span>)
</p>
</div>
<p>In Figure , we show the effect of the sample size
<span class="math inline">\(n\)</span> on the MLEs for the shape and scale parameters. The top four plots only show
the effect on the MLEs for the shape and scale parameters of components <span class="math inline">\(1\)</span> and
<span class="math inline">\(3\)</span>, component 1 being the most reliable component in the system and component 3
being the least reliable component. The bottom left plot shows the coverage
probabilities of the confidence intervals for all parameters and the bottom
right plot shows the convergence rate of the MLE.</p>
<div id="key-observations-2" class="section level3 hasAnchor" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Key Observations<a href="#key-observations-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="coverage-probability-cp-2" class="section level5 unnumbered hasAnchor" number="">
<h5>Coverage Probability (CP)<a href="#coverage-probability-cp-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The confidence intervals are correctly specified, obtaining coverages
above <span class="math inline">\(90\%\)</span> for most of the parameters across the entire sample size range,
and they are converging to the nominal <span class="math inline">\(95\%\)</span> level as the sample size
increases. The BCa CIs contain the true value of the parameters with the
specified confidence level, and the CIs neither too wide nor too narrow.</p>
<p>However, as in the previous scenario where we varied the right-censoring
amount, the scale parameters have better coverage than the shape parameters.
The scale parameters are consistently around the nominal <span class="math inline">\(95\%\)</span> level for all
sample sizes, but the shape parameters lag behind, suggesting that the shape
parameters are more difficult to estimate than the scale parameters.</p>
</div>
<div id="dispersion-of-mles-2" class="section level5 unnumbered hasAnchor" number="">
<h5>Dispersion of MLEs<a href="#dispersion-of-mles-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The shaded regions representing the <span class="math inline">\(95\%\)</span> quantile range of the MLEs become
more narrow as the sample size increases. This is an indicator of the increased
precision in the estimates when provided with more data.</p>
<p>We also see that the dispersion of the MLEs for <span class="math inline">\(k_1\)</span> and <span class="math inline">\(\lambda_1\)</span> are much
larger than the dispersion of the MLEs for <span class="math inline">\(k_3\)</span> and <span class="math inline">\(\lambda_3\)</span>. This is
consistent with the previous analysis in the right-censoring scenario.</p>
</div>
<div id="median-aggregated-cis-2" class="section level5 unnumbered hasAnchor" number="">
<h5>Median-Aggregated CIs<a href="#median-aggregated-cis-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The median-aggregated CIs (vertical dark blue bars) show that the CIs are
becoming less spread out as the sample size increases, indicating that they are
becoming more consistent and centered around a tighter range while maintaining
good coverage probability. The end result is that we can construct more precise
and accurate CIs with larger samples and thus we can make more confident
inferences about the true parameter value.</p>
<p>The estimator is quite sensitive to the data, and so the CIs are wide to account
for this sensitivity when the sample size is small and not necessarily
representative of the true distribution.</p>
<p>We also observe that the upper bound of the CI is more spread out than the lower
bound. This is consistent with the behavior of the dispersion of the MLEs, which
have a positive bias. Thus, the CIs are accounting for this bias by being
more spread out in the direction of the bias.</p>
</div>
<div id="bias-of-mles-2" class="section level5 unnumbered hasAnchor" number="">
<h5>Bias of MLEs<a href="#bias-of-mles-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The mean of the MLE (red dashed lines) initially has a large positive
bias, but diminishes to negligible levels as the sample size increases. In the
previous right-censoring scenario, the bias never reached zero, but we see that
in this scenario, at around a sample size of <span class="math inline">\(250\)</span>, the estimator is essentially
unbiased, suggesting that there is enough information in the sample to overcome
the bias from the right-censoring (<span class="math inline">\(q = 0.825\)</span>) and masking (<span class="math inline">\(p = 0.215\)</span>)
effects.</p>
</div>
<div id="convergence-rate-2" class="section level5 unnumbered hasAnchor" number="">
<h5>Convergence Rate<a href="#convergence-rate-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The convergence rate increases as the sample size <span class="math inline">\(n\)</span> increases. This is
consistent with the expectation that more data provides more information about
the parameters, making the likelihood function more informative and easier to
identify the parameters that maximize it. We see that the convergence rate is
<span class="math inline">\(95\%\)</span> or greater for sample sizes <span class="math inline">\(n \geq 100\)</span> given moderate right-censoring
and masking. Given how quickly the convergence rate increased, we anticipate
that even for extreme censoring and masking, the convergence rate would likely
rapidly increase to over <span class="math inline">\(95\%\)</span> as the sample size increases. However, for
sample sizes <span class="math inline">\(n &lt; 100\)</span>, at least for series systems with <span class="math inline">\(m = 5\)</span> components with
Weibull lifetimes, the convergence rate is low. For small samples, any estimates
should be taken with a grain of salt.</p>
</div>
</div>
<div id="summary-2" class="section level3 hasAnchor" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> Summary<a href="#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this scenario, we delve into the mitigating effects of sample size on the
challenges identified in previous scenarios concerning right-censoring and
masking. The precision and accuracy of the MLE rapidly improves with a bias
approaching zero. This is consistent with statistical theory and suggests that
increasing the sample size can mitigate the effects of right-censoring and
masking. The BCa CIs also narrow and become more reliable (with coverage
probabilities approaching the nominal <span class="math inline">\(95\%\)</span> level) with larger samples,
reinforcing the role of sample size in achieving robust estimates.</p>
</div>
</div>
</div>
<div id="future-work" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> Future Work<a href="#future-work" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This paper developed maximum likelihood techniques and simulation studies to
estimate component reliability from masked failure data in series systems. The
key results were:</p>
<ul>
<li>The likelihood model enabled rigorous inference from masked data via
right-censoring and candidate sets.</li>
<li>Despite masking and censoring, the MLE demonstrated accurate and robust
performance in simulation studies.</li>
<li>BCa confidence intervals had good coverage probability even for small samples.</li>
<li>Estimation of shape parameters was more challenging than scale parameters.</li>
</ul>
<p>Building on these findings, next we consider promising areas for future work.</p>
<div id="relaxing-masking-conditions" class="section level4 unnumbered hasAnchor" number="">
<h4>Relaxing Masking Conditions<a href="#relaxing-masking-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To further generalize the likelihood model, we can relax conditions
<a href="#cond:c-contains-k"><strong>??</strong></a>, <a href="#cond:equal-prob-failure-cause"><strong>??</strong></a>, and
<a href="#cond:masked-indept-theta"><strong>??</strong></a> on the masking model. We could perform
a sensitivity analysis to violations of these conditions, which would provide
insights into the robustness of the likelihood model, or we could develop
alternative likelihood models that are less stringent.</p>
</div>
<div id="deviations-from-well-designed-series-systems" class="section level4 unnumbered hasAnchor" number="">
<h4>Deviations from Well-Designed Series Systems<a href="#deviations-from-well-designed-series-systems" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Assess the sensitivity of the estimator to deviations from well-designed
series systems, in which there are no components that are significantly more
likely to fail than others. We could vary the probabilities of component failure
(while keeping the system reliability constant) to assess how the estimator
behaves when the system deviates from the well-designed system criteria. We did
some preliminary investigation of this, and we found that the estimator was
quite sensitive to deviations in system design.</p>
</div>
<div id="homogenous-shape-parameter" class="section level4 unnumbered hasAnchor" number="">
<h4>Homogenous Shape Parameter<a href="#homogenous-shape-parameter" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Assess the trade-off between using the homogenous shape parameter model and the
full model. The homogenous shape parameter model assumes that the shape
parameters are equal, which is a simplification of the full model. We could
assess the sensitivity of the estimator to deviations in the homogenous shape
parameter assumption. By the bias-variance trade-off, we expect that
the homogenous shape parameter model will have less variance but more bias than
the full model.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> We did some preliminary investigation of this, and we
found that the homogenous shape parameter model worked quite well for
moderate masking and censoring when the true system was a reasonably
well-designed series system, and only had more bias than the full model when
the sample size was extremely large.</p>
</div>
<div id="semi-parametric-bootstrap" class="section level4 unnumbered hasAnchor" number="">
<h4>Semi-Parametric Bootstrap<a href="#semi-parametric-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We used the non-parametric bootstrap to construct confidence intervals, but we
could also investigate the semi-parametric bootstrap. In the semi-parametric
bootstrap, instead of resampling from the original data, we sample component
lifetimes from the parametric distribution fitted to the original data and
sample candidate sets from the conditional empirical distribution of the
candidate sets in the original data. This is a compromise between the
non-parametric bootstrap and the fully parametric bootstrap.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
</div>
<div id="regularization-methods" class="section level4 unnumbered hasAnchor" number="">
<h4>Regularization Methods<a href="#regularization-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Investigate regularization methods like data augmentation and penalized
likelihood to improve parameter estimates, in particular shape parameter
estimates.</p>
</div>
<div id="extending-the-likelihood-model-with-predictors" class="section level4 unnumbered hasAnchor" number="">
<h4>Extending the Likelihood Model with Predictors<a href="#extending-the-likelihood-model-with-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Our research is based on a general likelihood model. A straightforward extension
is to integrate predictors, allowing each observation’s hazard function to be
influenced by specific variables, such as in the Cox proportional hazards model
<span class="citation">[<a href="#ref-cox1972regression" role="doc-biblioref">13</a>]</span>, but any function that satisfies the fundamental
mathematical principles of a hazard function could be used. This would allow
the likelihood model to be more flexible and adaptable to a wider range of
applications.</p>
</div>
<div id="bootstrapped-prediction-intervals" class="section level4 unnumbered hasAnchor" number="">
<h4>Bootstrapped Prediction Intervals<a href="#bootstrapped-prediction-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this paper, we applied the bootstrap method to construct confidence
intervals for the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>, which generated correctly specified
confidence intervals in our simulation study. We could do a similar analysis for
other statistics, in particular prediction intervals, which are useful for
predicting the reliability of a system or a component at a future time.</p>
<p>The current results provide a solid foundation for extensions like these that
can further refine the methods and expand their applicability. By leveraging the
rigorous likelihood framework and simulation techniques validated in this study,
future work can continue advancing the capability for statistical learning from
masked reliability data.</p>
</div>
</div>
<div id="conclusion" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Conclusion<a href="#conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This work presented maximum likelihood techniques to estimate component
reliability from masked failure data in series systems. The methods demonstrated
accurate and robust performance despite significant challenges introduced by
masking and right-censoring.</p>
<p>Simulation studies revealed that for our well-designed series system with
Weibull component lifetimes, right-censoring and masking positively biased the
estimates and the more reliable components were more sensitive to these effects.
Additionally, the shape parameters were more difficult to estimate than the
scale parameters. However, with sufficiently large sample sizes, these
difficulties were overcome, suggesting enough information existed in the data to
mitigate censoring and masking effects.</p>
<p>Despite the challenges, the bootstrapped bias-corrected and accelerated
confidence intervals were generally correctly specified with good overall
coverage probabilities, even for relatively small sample sizes. This good
empirical coverage demonstrates the reliability of these intervals for
statistical inference, striking a balance between precision and robustness. The
modeling framework offers a statistically rigorous method for estimating latent
component properties based on limited observational data concerning system
reliability. The simulation studies validate these techniques and provide
practical insights into their efficacy under diverse real-world scenarios. This
enhances our capacity for statistical learning from obscured system failure
data.</p>
</div>
<div id="appendices" class="section level1 unnumbered hasAnchor" number="">
<h1>Appendices<a href="#appendices" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>



<div id="series-system-with-weibull-component-lifetimes" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">A</span> Series System with Weibull Component Lifetimes<a href="#series-system-with-weibull-component-lifetimes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These functions are implemented in the R library <code>wei.series.md.c1.c2.c3</code>, which
is available on GitHub at
<a href="https://github.com/queelius/wei.series.md.c1.c2.c3"><code>github.com/queelius/wei.series.md.c1.c2.c3</code></a>
<span class="citation">[<a href="#ref-towell2023weibull" role="doc-biblioref">14</a>]</span>.
They are for series systems with Weibull component lifetimes with masked data
described in Section <a href="#like-model">3</a>. For clarity and brevity, we
removed some of the functionality and safeguards in the actual code, but we
provide the full code in the R package.</p>
<div id="app-weibull-loglik-r" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">A.1</span> Log-likelihood Function<a href="#app-weibull-loglik-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The log-likelihood function is the sum of the log-likelihood contributions for
each system. For our series system with Weibull component lifetimes, we
analytically derived the log-likelihood function in Theorem
<a href="#thm:weibull-likelihood-contribution">6.1</a> and implemented it in the
<code>loglik_wei_series_md_c1_c2_c3</code> R function below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co">#&#39; Generates a log-likelihood function for a series system with Weibull</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="co">#&#39; component lifetimes for masked data.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="co">#&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="co">#&#39; @param df masked data frame</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="co">#&#39; @param theta parameter vector</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a><span class="co">#&#39; @returns log-likelihood function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>loglik_wei_series_md_c1_c2_c3 &lt;-<span class="st"> </span><span class="cf">function</span>(df, theta) {</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>  C &lt;-<span class="st"> </span><span class="kw">md_decode_matrix</span>(df, <span class="st">&quot;x&quot;</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>  m &lt;-<span class="st"> </span><span class="kw">ncol</span>(C)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>  delta &lt;-<span class="st"> </span>df[[<span class="st">&quot;delta&quot;</span>]]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>  shapes &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>  scales &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a>  t &lt;-<span class="st"> </span>df[[lifetime]]    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a>  s &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>    s &lt;-<span class="st"> </span>s <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>((t[i] <span class="op">/</span><span class="st"> </span>scales)<span class="op">^</span>shapes)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a>    <span class="cf">if</span> (delta[i]) {</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a>      s &lt;-<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">sum</span>(shapes[C[i, ]] <span class="op">/</span><span class="st"> </span>scales[C[i, ]] <span class="op">*</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a><span class="st">        </span>(t[i] <span class="op">/</span><span class="st"> </span>scales[C[i, ]])<span class="op">^</span>(shapes[C[i, ]] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a>    }</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true"></a>  }</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true"></a>  <span class="kw">return</span>(s)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="app-score-fn-r" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">A.2</span> Score Function<a href="#app-score-fn-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The score function is the gradient of the log-likelihood function. For our
series system with Weibull component lifetimes, we analytically derived the
score function in Theorem <a href="#thm:weibull-score">6.2</a> and implemented it in
the <code>score_wei_series_md_c1_c2_c3</code> R function below.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="co">#&#39; Computes the score function for a series system with Weibull component</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="co">#&#39; lifetimes for masked data.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="co">#&#39;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a><span class="co">#&#39; @param df masked data frame</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="co">#&#39; @param theta parameter vector</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a><span class="co">#&#39; @returns score function</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>score_wei_series_md_c1_c2_c3 &lt;-<span class="st"> </span><span class="cf">function</span>(df, theta) {</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>  C &lt;-<span class="st"> </span><span class="kw">md_decode_matrix</span>(df, <span class="st">&quot;x&quot;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>  m &lt;-<span class="st"> </span><span class="kw">ncol</span>(C)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>  delta &lt;-<span class="st"> </span>df[[<span class="st">&quot;delta&quot;</span>]]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>  t &lt;-<span class="st"> </span>df[[lifetime]]  </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>  shapes &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a>  scales &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a>  shapes.scr &lt;-<span class="st"> </span>scales.scr &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, m)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a>    shapes.rt &lt;-<span class="st"> </span><span class="op">-</span>(t[i] <span class="op">/</span><span class="st"> </span>scales)<span class="op">^</span>shapes <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(t[i] <span class="op">/</span><span class="st"> </span>scales)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a>    scales.rt &lt;-<span class="st"> </span>(shapes <span class="op">/</span><span class="st"> </span>scales) <span class="op">*</span><span class="st"> </span>(t[i] <span class="op">/</span><span class="st"> </span>scales)<span class="op">^</span>shapes</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a>    shapes.trm &lt;-<span class="st"> </span>scales.trm &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, m)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a>    <span class="cf">if</span> (delta[i]) {</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a>      c &lt;-<span class="st"> </span>C[i, ]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true"></a>      denom &lt;-<span class="st"> </span><span class="kw">sum</span>(shapes[c] <span class="op">/</span><span class="st"> </span>scales[c] <span class="op">*</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true"></a><span class="st">        </span>(t[i] <span class="op">/</span><span class="st"> </span>scales[c])<span class="op">^</span>(shapes[c] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true"></a>      shapes.num &lt;-<span class="st"> </span>(t[i] <span class="op">/</span><span class="st"> </span>scales[c])<span class="op">^</span>shapes[c] <span class="op">/</span><span class="st"> </span>t[i] <span class="op">*</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true"></a><span class="st">        </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>shapes[c] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(t[i] <span class="op">/</span><span class="st"> </span>scales[c]))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true"></a>      shapes.trm[c] &lt;-<span class="st"> </span>shapes.num <span class="op">/</span><span class="st"> </span>denom</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true"></a>      scales.num &lt;-<span class="st"> </span>(shapes[c] <span class="op">/</span><span class="st"> </span>scales[c])<span class="op">^</span><span class="dv">2</span> <span class="op">*</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true"></a><span class="st">        </span>(t[i] <span class="op">/</span><span class="st"> </span>scales[c])<span class="op">^</span>(shapes[c] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true"></a>      scales.trm[c] &lt;-<span class="st"> </span>scales.num <span class="op">/</span><span class="st"> </span>denom</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true"></a>    }</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true"></a>    shapes.scr &lt;-<span class="st"> </span>shapes.scr <span class="op">+</span><span class="st"> </span>shapes.rt <span class="op">+</span><span class="st"> </span>shapes.trm</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true"></a>    scales.scr &lt;-<span class="st"> </span>scales.scr <span class="op">+</span><span class="st"> </span>scales.rt <span class="op">-</span><span class="st"> </span>scales.trm</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true"></a>  }</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true"></a>  scr &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(theta))</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true"></a>  scr[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)] &lt;-<span class="st"> </span>shapes.scrs</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true"></a>  scr[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)] &lt;-<span class="st"> </span>scales.scrs</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true"></a>  <span class="kw">return</span>(scr)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="app-series-quantile" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">A.3</span> Quantile Function<a href="#app-series-quantile" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For our series system with Weibull component lifetimes, the quantile function is
the inverse of the cdf <span class="math inline">\(F_{T_i}\)</span>. By definition, the quantile <span class="math inline">\(p\)</span> for the strictly
monotonically increasing cdf <span class="math inline">\(F_{T_i}\)</span> is the value <span class="math inline">\(t\)</span> that satisfies
<span class="math inline">\(F_{T_i}(t;\boldsymbol{\theta}) - p = 0\)</span>, and so we solve for <span class="math inline">\(t\)</span> using Newton’s method,
in which the <span class="math inline">\(k\)</span> iteration is given by
<span class="math display">\[
t^{(k+1)} = t^{(k)} - \frac{F_{T_i}(t^{(k)};\boldsymbol{\theta}) - p}{f_{T_i}(t^{(k)};\boldsymbol{\theta})}.
\]</span>
We have derived a slightly more efficient method in the <code>qwei_series</code> R function
below.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co">#&#39; Quantile function for a series system with Weibull component lifetimes.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="co">#&#39;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co">#&#39; @param p quantile</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="co">#&#39; @param shapes shape parameters</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="co">#&#39; @param scales scale parameters</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="co">#&#39; @returns p-th quantile</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>qwei_series &lt;-<span class="st"> </span><span class="cf">function</span>(p, shapes, scales) {</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>  t0 &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>  <span class="cf">repeat</span> {</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>    t1 &lt;-<span class="st"> </span>t0 <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>((t0 <span class="op">/</span><span class="st"> </span>scales)<span class="op">^</span>shapes) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">/</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="st">      </span><span class="kw">sum</span>(shapes <span class="op">*</span><span class="st"> </span>t0<span class="op">^</span>(shapes <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>scales<span class="op">^</span>shapes)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="kw">abs</span>(t1 <span class="op">-</span><span class="st"> </span>t0) <span class="op">&lt;</span><span class="st"> </span>tol) {</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a>      <span class="cf">break</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a>    }</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a>    t0 &lt;-<span class="st"> </span>t1</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a>  }</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>  <span class="kw">return</span>(t1)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="app-mle-r" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">A.4</span> Maximum Likelihood Estimation<a href="#app-mle-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the Newton-Raphson method for Maximum Likelihood Estimation (MLE) in a
series system with Weibull component lifetimes. Numerical optimization is
carried out using R’s <code>optim</code> package and the <strong>L-BFGS-B</strong> method <span class="citation">[<a href="#ref-byrd1995" role="doc-biblioref">15</a>]</span>.
This quasi-Newton method approximates the Hessian using the gradient of the
log-likelihood function (see Appendices <a href="#app-weibull-loglik-r">A.1</a> and
<a href="#app-score-fn-r">A.2</a>). Bound constraints are applied to maintain positive
shape and scale parameters.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="co">#&#39; L-BFGS-B solver for the series system with Weibull component lifetimes</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a><span class="co">#&#39; given masked data.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="co">#&#39; </span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a><span class="co">#&#39; @param df masked data frame</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a><span class="co">#&#39; @param theta0 initial guess</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a><span class="co">#&#39; @return MLE solution</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>mle_lbfgsb_wei_series_md_c1_c2_c3 &lt;-<span class="st"> </span><span class="cf">function</span>(df, theta0) {</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a>  <span class="kw">optim</span>(theta0,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>    <span class="dt">fn =</span> <span class="cf">function</span>(theta) <span class="kw">loglik_wei_series_md_c1_c2_c3</span>(df,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>      theta[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)], theta[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>    <span class="dt">gr =</span> <span class="cf">function</span>(theta) <span class="kw">score_wei_series_md_c1_c2_c3</span>(df,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a>      theta[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)], theta[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a>    <span class="dt">lower =</span> <span class="kw">rep</span>(<span class="fl">1e-9</span>, <span class="kw">length</span>(theta0)),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>    <span class="dt">method =</span> <span class="st">&quot;L-BFGS-B&quot;</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">fnscale =</span> <span class="dv">-1</span>, <span class="dt">maxit =</span> 125L))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="app-cand-model-r" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">A.5</span> Bernoulli Masking Model<a href="#app-cand-model-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the Bernoulli masking model, the failed component is guaranteed to be in the
candidate set and each non-failed component is included with some fixed
probability <span class="math inline">\(p\)</span>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co">#&#39; Bernoulli masking model is a particular type of *uninformed* model.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="co">#&#39; Note that we do not generate candidate sets with this function. See</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co">#&#39; `md_cand_sampler` for that.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="co">#&#39;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a><span class="co">#&#39; @param df masked data frame.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="co">#&#39; @param p Bernoulli masking probability</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="co">#&#39; @returns masked data frame with Bernoulli candidate set probabilities</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>md_bernoulli_cand_c1_c2_c3 &lt;-<span class="st"> </span><span class="cf">function</span>(df, p) {</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(df)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>  p &lt;-<span class="st"> </span><span class="kw">rep</span>(p, <span class="dt">length.out =</span> n)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>  Tm &lt;-<span class="st"> </span><span class="kw">md_decode_matrix</span>(df, <span class="st">&quot;t&quot;</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>  m &lt;-<span class="st"> </span><span class="kw">ncol</span>(Tm)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>  Q &lt;-<span class="st"> </span><span class="kw">matrix</span>(p, <span class="dt">nrow =</span> n, <span class="dt">ncol =</span> m)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>  Q[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="kw">apply</span>(Tm, <span class="dv">1</span>, which.min))] &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>  Q[<span class="op">!</span>df[[<span class="st">&quot;delta&quot;</span>]], ] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a>  df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(<span class="kw">md_encode_matrix</span>(Q, prob))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>}</span></code></pre></div>
</div>
</div>
<div id="simulation" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">B</span> Simulation<a href="#simulation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This appendix showcases select Python and R code instrumental in generating
this paper. The entire source code can be found on GitHub:
<a href="https://github.com/queelius/reliability-estimation-in-series-systems"><code>github.com/queelius/reliability-estimation-in-series-systems</code></a>
<span class="citation">[<a href="#ref-towell2023reliability" role="doc-biblioref">16</a>]</span>. For clarity and brevity, certain functionalities and
safeguards from the original code have been omitted. However, the full, unedited
code, as well as the methods to reproduce this paper, are available in the
GitHub repository.</p>
<div id="app-sim-study-r" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">B.1</span> Scenario Simulation<a href="#app-sim-study-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below, you’ll find the R code for the Monte-Carlo simulation, tailored to run
the scenarios discussed in Section <a href="#sim-study">7</a>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="co"># A function to simulate the effects of various parameters on the sampling</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="co"># distribution of the MLE</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>simulate_scenario &lt;-<span class="st"> </span><span class="cf">function</span>(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a>    csv_file,            <span class="co"># Destination file to save simulation results</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>    N,                   <span class="co"># Vector of sample sizes</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a>    P,                   <span class="co"># Vector of masking probabilities</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a>    Q,                   <span class="co"># Vector of quantiles of the series system</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    R,                   <span class="co"># Number of simulation replicates</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    B,                   <span class="co"># Number of bootstrap samples</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>    theta                <span class="co"># True parameters for the Weibull distribution</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>) {</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>  shapes &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true"></a>  scales &lt;-<span class="st"> </span>theta[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)]</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true"></a>  m &lt;-<span class="st"> </span><span class="kw">length</span>(scales)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true"></a>  cname &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;p&quot;</span>, <span class="st">&quot;q&quot;</span>, <span class="st">&quot;tau&quot;</span>, <span class="st">&quot;B&quot;</span>,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true"></a>             <span class="kw">paste0</span>(<span class="st">&quot;shape.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m), <span class="kw">paste0</span>(<span class="st">&quot;scale.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m),</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true"></a>             <span class="kw">paste0</span>(<span class="st">&quot;shape.mle.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m), <span class="kw">paste0</span>(<span class="st">&quot;scale.mle.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m),</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true"></a>             <span class="kw">paste0</span>(<span class="st">&quot;shape.lower.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m), <span class="kw">paste0</span>(<span class="st">&quot;shape.upper.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m),</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true"></a>             <span class="kw">paste0</span>(<span class="st">&quot;scale.lower.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m), <span class="kw">paste0</span>(<span class="st">&quot;scale.upper.&quot;</span>, <span class="dv">1</span><span class="op">:</span>m),</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true"></a>             <span class="st">&quot;convergence&quot;</span>, <span class="st">&quot;loglik&quot;</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true"></a>  <span class="cf">for</span> (n <span class="cf">in</span> N) {</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true"></a>    <span class="cf">for</span> (p <span class="cf">in</span> P) {</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true"></a>      <span class="cf">for</span> (q <span class="cf">in</span> Q) {</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true"></a>        tau &lt;-<span class="st"> </span><span class="kw">qwei_series</span>(<span class="dt">p =</span> q, <span class="dt">scales =</span> scales, <span class="dt">shapes =</span> shapes)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true"></a>        <span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R) {</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true"></a>          df &lt;-<span class="st"> </span><span class="kw">generate_guo_weibull_table_2_data</span>(shapes, scales, n, p, tau)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true"></a>          sol &lt;-<span class="st"> </span><span class="kw">mle_lbfgsb_wei_series_md_c1_c2_c3</span>(<span class="dt">theta0 =</span> theta, <span class="dt">df =</span> df)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true"></a>          mle_solver &lt;-<span class="st"> </span><span class="cf">function</span>(df, i) {</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true"></a>            <span class="kw">mle_lbfgsb_wei_series_md_c1_c2_c3</span>(<span class="dt">theta0 =</span> sol<span class="op">$</span>par, <span class="dt">df =</span> df[i, ])</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true"></a>          }</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true"></a>          sol.boot &lt;-<span class="st"> </span>boot<span class="op">::</span><span class="kw">boot</span>(df, mle_solver, <span class="dt">R =</span> B)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true"></a>          ci &lt;-<span class="st"> </span><span class="kw">confint</span>(<span class="kw">mle_boot</span>(sol.boot))</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true"></a>          result &lt;-<span class="st"> </span><span class="kw">c</span>(n, p, q, tau, B, shapes, scales,</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true"></a>                      sol<span class="op">$</span>par[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)],</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true"></a>                      sol<span class="op">$</span>par[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>)],</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true"></a>                      ci[<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>), <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true"></a>                      ci[<span class="kw">seq</span>(<span class="dv">2</span>, <span class="kw">length</span>(theta), <span class="dv">2</span>), <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true"></a>                      sol<span class="op">$</span>convergence, sol<span class="op">$</span>value)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true"></a>          </span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true"></a>          result_df &lt;-<span class="st"> </span><span class="kw">setNames</span>(<span class="kw">data.frame</span>(<span class="kw">t</span>(result)), cname)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true"></a>          <span class="kw">write.table</span>(result_df, <span class="dt">file =</span> csv_file, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="dt">append =</span> <span class="ot">TRUE</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true"></a>        }</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true"></a>      }</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true"></a>    }</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true"></a>  }</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true"></a>}</span></code></pre></div>
<p>For example, to run the simulation study for the effect of masking probability,
we would run the following code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="kw">source</span>(<span class="st">&quot;sim-scenario.R&quot;</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="kw">library</span>(wei.series.md.c1.c2.c3)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="kw">simulate_scenario</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>    <span class="dt">theta =</span> alex_weibull_series<span class="op">$</span>theta, <span class="co"># the well-designed system</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>    <span class="dt">N =</span> <span class="kw">c</span>(90L), <span class="co"># sample size</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>    <span class="dt">P =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.215</span>, <span class="fl">0.4</span>, <span class="fl">0.55</span>, <span class="fl">0.7</span>), <span class="co"># vary masking probability</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>    <span class="dt">Q =</span> <span class="kw">c</span>(<span class="fl">0.825</span>), <span class="co"># right-censoring quantile</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>    <span class="dt">R =</span> 400L, <span class="dt">B =</span> 1000L,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>    <span class="dt">max_iter =</span> 125L, <span class="dt">max_boot_iter =</span> 125L,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>    <span class="dt">n_cores =</span> 2L, <span class="dt">csv_file =</span> <span class="st">&quot;masking-prob-sim.csv&quot;</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>    <span class="dt">ci_method =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">ci_level =</span> <span class="fl">.95</span>)</span></code></pre></div>
<p>Please see the GitHub repository for the full code, since the code as presented
will not run without supporting functions.</p>
</div>
<div id="app-plot-r" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">B.2</span> Plot Generation Code<a href="#app-plot-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following Python code generates the plots for the simulation study,
<code>plot_cp</code> for the coverage probabilities and <code>plot_mle</code> for the
the sampling distribution of the MLE, both with respect to some simulation
parameter, like sample size (<span class="math inline">\(n\)</span>) or masking probability (<span class="math inline">\(p\)</span>).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="kw">def</span> plot_cp(data, x_col, x_col_label):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a>  rel_cols <span class="op">=</span> [x_col] <span class="op">+</span> [<span class="ss">f&#39;shape.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a>    [<span class="ss">f&#39;scale.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a>    [<span class="ss">f&#39;shape.lower.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a>    [<span class="ss">f&#39;shape.upper.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true"></a>    [<span class="ss">f&#39;scale.lower.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true"></a>    [<span class="ss">f&#39;scale.upper.</span><span class="sc">{i}</span><span class="ss">&#39;</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true"></a>  rel_data <span class="op">=</span> data[rel_cols].copy()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true"></a>  <span class="kw">def</span> compute_coverage(row, j):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true"></a>    shape_in <span class="op">=</span> row[<span class="ss">f&#39;shape.lower.</span><span class="sc">{j}</span><span class="ss">&#39;</span>] <span class="op">&lt;=</span> row[<span class="ss">f&#39;shape.</span><span class="sc">{j}</span><span class="ss">&#39;</span>] <span class="op">&lt;=</span> row[<span class="ss">f&#39;shape.upper.</span><span class="sc">{j}</span><span class="ss">&#39;</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true"></a>    scale_in <span class="op">=</span> row[<span class="ss">f&#39;scale.lower.</span><span class="sc">{j}</span><span class="ss">&#39;</span>] <span class="op">&lt;=</span> row[<span class="ss">f&#39;scale.</span><span class="sc">{j}</span><span class="ss">&#39;</span>] <span class="op">&lt;=</span> row[<span class="ss">f&#39;scale.upper.</span><span class="sc">{j}</span><span class="ss">&#39;</span>]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true"></a>    <span class="cf">return</span> pd.Series([shape_in, scale_in], index<span class="op">=</span>[<span class="ss">f&#39;shape.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>, <span class="ss">f&#39;scale.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>])</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true"></a>    rel_data[[<span class="ss">f&#39;shape.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>, <span class="ss">f&#39;scale.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>]] <span class="op">=</span> <span class="op">\</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true"></a>      rel_data.<span class="bu">apply</span>(<span class="kw">lambda</span> row: compute_coverage(row, j), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true"></a>  cols.cov <span class="op">=</span> [<span class="ss">f&#39;shape.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true"></a>             [<span class="ss">f&#39;scale.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)]</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true"></a>  cps <span class="op">=</span> rel_data.groupby(x_col)[cols.cov].mean().reset_index()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true"></a>  <span class="co"># Mean coverage probabilities</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true"></a>  mean_shape_cp <span class="op">=</span> cps[[<span class="ss">f&#39;shape.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)]].mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true"></a>  mean_scale_cp <span class="op">=</span> cps[[<span class="ss">f&#39;scale.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>)]].mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true"></a>  cps[<span class="st">&#39;mean_shape_cp&#39;</span>] <span class="op">=</span> mean_shape_cp</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true"></a>  cps[<span class="st">&#39;mean_scale_cp&#39;</span>] <span class="op">=</span> mean_scale_cp</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true"></a>  plt.figure(figsize<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">4</span>])</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true"></a>  shape_cmap <span class="op">=</span> plt.get_cmap(<span class="st">&#39;Blues&#39;</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true"></a>  scale_cmap <span class="op">=</span> plt.get_cmap(<span class="st">&#39;Reds&#39;</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true"></a>  shape.ls <span class="op">=</span> [<span class="st">&#39;-&#39;</span>, <span class="st">&#39;--&#39;</span>, <span class="st">&#39;-.&#39;</span>, <span class="st">&#39;:&#39;</span>, <span class="st">&#39;-&#39;</span>]</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true"></a>  shape.mk <span class="op">=</span> [<span class="st">&#39;o&#39;</span>, <span class="st">&#39;s&#39;</span>, <span class="st">&#39;^&#39;</span>, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;D&#39;</span>]</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true"></a>  <span class="cf">for</span> j, color, ls, mk <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>), <span class="op">\</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true"></a>      shape_cmap(np.linspace(<span class="fl">0.4</span>, <span class="dv">1</span>, <span class="dv">5</span>)), shape.ls, shape.mk):</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true"></a>    plt.plot(cps[x_col], cps[<span class="ss">f&#39;shape.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>], label<span class="op">=</span><span class="ss">f&#39;$k_</span><span class="sc">{j}</span><span class="ss">$&#39;</span>, <span class="op">\</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true"></a>      color<span class="op">=</span>color, linestyle<span class="op">=</span>ls, marker<span class="op">=</span>mk)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true"></a>  scale.ls <span class="op">=</span> [<span class="st">&#39;-&#39;</span>, <span class="st">&#39;--&#39;</span>, <span class="st">&#39;-.&#39;</span>, <span class="st">&#39;:&#39;</span>, <span class="st">&#39;-&#39;</span>]</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true"></a>  scale.mk <span class="op">=</span> [<span class="st">&#39;o&#39;</span>, <span class="st">&#39;s&#39;</span>, <span class="st">&#39;^&#39;</span>, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;D&#39;</span>]</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true"></a>  <span class="cf">for</span> j, color, ls, mk <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>), <span class="op">\</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true"></a>      scale_cmap(np.linspace(<span class="fl">0.4</span>, <span class="dv">1</span>, <span class="dv">5</span>)), scale.ls, scale.mk):</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true"></a>    plt.plot(cps[x_col], cps[<span class="ss">f&#39;scale.cov.</span><span class="sc">{j}</span><span class="ss">&#39;</span>], label<span class="op">=</span><span class="ss">f&#39;$\lambda_</span><span class="sc">{j}</span><span class="ss">$&#39;</span>, <span class="op">\</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true"></a>      color<span class="op">=</span>color, linestyle<span class="op">=</span>ls, marker<span class="op">=</span>mk)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true"></a>  plt.plot(cps[x_col], cps[<span class="st">&#39;mean_shape_cp&#39;</span>], color<span class="op">=</span><span class="st">&#39;darkblue&#39;</span>, linewidth<span class="op">=</span><span class="dv">4</span>, <span class="op">\</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true"></a>    linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, label<span class="op">=</span><span class="st">&#39;$</span><span class="ch">\\</span><span class="st">bar</span><span class="sc">{k}</span><span class="st">$&#39;</span>)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true"></a>  plt.plot(cps[x_col], cps[<span class="st">&#39;mean_scale_cp&#39;</span>], color<span class="op">=</span><span class="st">&#39;darkred&#39;</span>, linewidth<span class="op">=</span><span class="dv">4</span>, <span class="op">\</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true"></a>    linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, label<span class="op">=</span><span class="st">&#39;$</span><span class="ch">\\</span><span class="st">bar{\lambda}$&#39;</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true"></a>  plt.axhline(y<span class="op">=</span><span class="fl">0.95</span>, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, label<span class="op">=</span><span class="st">&#39;$95</span><span class="ch">\\</span><span class="st">%$&#39;</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true"></a>  plt.axhline(y<span class="op">=</span><span class="fl">0.90</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, label<span class="op">=</span><span class="st">&#39;$90</span><span class="ch">\\</span><span class="st">%$&#39;</span>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true"></a>  plt.xlabel(<span class="ss">f&#39;</span><span class="sc">{</span>x_col_label<span class="sc">}</span><span class="ss"> ($</span><span class="sc">{</span>x_col<span class="sc">}</span><span class="ss">$)&#39;</span>)</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true"></a>  plt.ylabel(<span class="st">&#39;Coverage Probability (CP)&#39;</span>)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true"></a>  plt.gca().yaxis.set_major_formatter(</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true"></a>      FuncFormatter(<span class="kw">lambda</span> y, _: <span class="st">&#39;</span><span class="sc">{:.2f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(y)))</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true"></a>  plt.title(<span class="st">&#39;Coverage Probabilities for Parameters&#39;</span>)</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true"></a>  plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true"></a>  plt.tight_layout()</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true"></a>  plt.savefig(<span class="ss">f&#39;plot-</span><span class="sc">{</span>x_col<span class="sc">}</span><span class="ss">-vs-cp.pdf&#39;</span>)</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true"></a></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true"></a><span class="kw">def</span> plot_mle(raw_data, x_col, par, par_label, label, k<span class="op">=</span><span class="dv">100</span>, loc<span class="op">=</span><span class="st">&#39;upper left&#39;</span>):</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true"></a>  ps <span class="op">=</span> par.split(<span class="st">&#39;.&#39;</span>)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true"></a>  par_low <span class="op">=</span> <span class="ss">f&#39;</span><span class="sc">{ps</span>[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">.lower.</span><span class="sc">{ps</span>[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true"></a>  par_up <span class="op">=</span> <span class="ss">f&#39;</span><span class="sc">{ps</span>[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">.upper.</span><span class="sc">{ps</span>[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true"></a>  par_mle <span class="op">=</span> <span class="ss">f&#39;</span><span class="sc">{ps</span>[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">.mle.</span><span class="sc">{ps</span>[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true"></a>  x_vals <span class="op">=</span> <span class="bu">sorted</span>(raw_data[x_col].unique())</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true"></a></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true"></a>  median_mles <span class="op">=</span> []</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true"></a>  true_vals <span class="op">=</span> []</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true"></a>  mean_mles <span class="op">=</span> []</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true"></a>  low_q <span class="op">=</span> []</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true"></a>  up_q <span class="op">=</span> []</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true"></a>  plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true"></a></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true"></a>  <span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(x_vals):</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true"></a>    data <span class="op">=</span> raw_data[raw_data[x_col] <span class="op">==</span> x]</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true"></a>    low_med, up_med <span class="op">=</span> np.percentile(</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true"></a>      data[par_low], <span class="dv">50</span>), np.percentile(data[par_up], <span class="dv">50</span>)</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true"></a>    mean_mle <span class="op">=</span> data[par_mle].mean()</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true"></a>    true_val <span class="op">=</span> data[par].mean()</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true"></a>    median_mle <span class="op">=</span> data[par_mle].median()</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true"></a>    mean_mles.append(mean_mle)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true"></a>    median_mles.append(median_mle)</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true"></a>    low_q.append(np.percentile(data[par_mle], <span class="fl">2.5</span>))</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true"></a>    up_q.append(np.percentile(data[par_mle], <span class="fl">97.5</span>))</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true"></a>    true_vals.append(true_val)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true"></a></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true"></a>    plt.vlines(i, low_med, up_med, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>,</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true"></a>      label<span class="op">=</span><span class="st">&#39;Median 95% CI&#39;</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true"></a>    plt.plot(i, mean_mle, <span class="st">&#39;ro&#39;</span>, label<span class="op">=</span><span class="st">&#39;Mean MLE&#39;</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true"></a>    plt.plot(i, median_mle, <span class="st">&#39;bo&#39;</span>, label<span class="op">=</span><span class="st">&#39;Median MLE&#39;</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true"></a></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true"></a>  plt.plot(np.arange(<span class="bu">len</span>(x_vals)), mean_mles, <span class="st">&#39;r--&#39;</span>)</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true"></a>  plt.plot(np.arange(<span class="bu">len</span>(x_vals)), median_mles, <span class="st">&#39;b--&#39;</span>)</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true"></a>  plt.plot(np.arange(<span class="bu">len</span>(x_vals)), true_vals,</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true"></a>    <span class="st">&#39;g-&#39;</span>, label<span class="op">=</span><span class="st">&#39;True Value&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true"></a></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true"></a>  plt.fill_between(np.arange(<span class="bu">len</span>(x_vals)), low_q, up_q,</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true"></a>    color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, alpha<span class="op">=</span><span class="fl">0.15</span>, label<span class="op">=</span><span class="st">&#39;95% Quantile Range&#39;</span>)</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true"></a></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true"></a>  plt.xticks(np.arange(<span class="bu">len</span>(x_vals)), x_vals)</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true"></a>  plt.xlabel(<span class="ss">f&#39;</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss"> ($</span><span class="sc">{</span>x_col<span class="sc">}</span><span class="ss">$)&#39;</span>)</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true"></a>  plt.ylabel(<span class="ss">f&#39;Statistics for </span><span class="sc">{</span>par_label<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true"></a>  plt.title(<span class="ss">f&#39;MLE for </span><span class="sc">{</span>par_label<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true"></a>  plt.legend(loc<span class="op">=</span>loc)</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true"></a></span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true"></a>  leg <span class="op">=</span> plt.gca().get_legend()</span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true"></a>  <span class="cf">for</span> text <span class="kw">in</span> leg.get_texts():</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true"></a>    plt.setp(text, fontsize<span class="op">=</span><span class="st">&#39;small&#39;</span>)</span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true"></a></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true"></a>  plt.tight_layout(h_pad<span class="op">=</span><span class="fl">4.0</span>, w_pad<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true"></a>  plt.savefig(<span class="ss">f&#39;plot-</span><span class="sc">{</span>x_col<span class="sc">}</span><span class="ss">-vs-</span><span class="sc">{</span>par<span class="sc">}</span><span class="ss">-mle.pdf&#39;</span>)</span></code></pre></div>
<p>For example, to generate a plot of the coverage probabilities with respect
to sample size (<span class="math inline">\(n\)</span>) for the CSV file <code>data.csv</code>, we would run the following
code:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="im">from</span> plot_utils <span class="im">import</span> plot_cp</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>plot_cp(pd.read_csv(<span class="st">&quot;data.csv&quot;</span>), <span class="st">&#39;n&#39;</span>, <span class="st">&#39;Sample Size&#39;</span>)</span></code></pre></div>
<p>Please see the GitHub repository for the full code, since the code as presented
will not run without supporting functions.</p>
</div>
</div>
<div id="acknowledgements" class="section level1 unnumbered hasAnchor" number="">
<h1>Acknowledgements<a href="#acknowledgements" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>I am grateful to my advisor, Dr. Marcus Agustin, for his invaluable guidance and
unwavering support throughout my extended period of research and graduate
studies. His expertise and support were notably important as I navigated through
health challenges.</p>
<p>Special acknowledgment goes to Dr. Beidi Qiang, whose teachings and feedback
have been instrumental in my academic progress. I have benefited from her
insights both in and out of the classroom over the years.</p>
<p>I also thank Dr. Ed Sewell for his willingness to contribute to my committee.
His expertise in reviewing and providing feedback is an important component of
this research.</p>

<div id="refs" class="references">
<div id="ref-Agustin-2011">
<p>[1] M. Agustin, “Systems in series,” in <em>Wiley encyclopedia of operations research and management science</em>, John Wiley &amp; Sons, Ltd, 2011. doi: <a href="https://doi.org/https://doi.org/10.1002/9780470400531.eorms0866">https://doi.org/10.1002/9780470400531.eorms0866</a>. Available: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470400531.eorms0866">https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470400531.eorms0866</a></p>
</div>
<div id="ref-taleb2007black">
<p>[2] N. N. Taleb, <em>The black swan: The impact of the highly improbable</em>. Random House, 2007.</p>
</div>
<div id="ref-Fran-1991">
<p>[3] F. M. Guess, T. J. Hodgson, and J. S. Usher, “Estimating system and component reliabilities under partial information on cause of failure,” <em>Journal of Statistical Planning and Inference</em>, vol. 29, nos. 1-2, pp. 75–85, Sep. 1991, doi: <a href="https://doi.org/10.1016/0378-3758(92)90123-a">10.1016/0378-3758(92)90123-a</a>. Available: <a href="libgen.li/file.php?md5=ac54bdac9dbec6abfdfd63066c1cfad6">libgen.li/file.php?md5=ac54bdac9dbec6abfdfd63066c1cfad6</a></p>
</div>
<div id="ref-lehmann1998theory">
<p>[4] E. L. Lehmann and G. Casella, <em>Theory of point estimation</em>. Springer Science &amp; Business Media, 1998.</p>
</div>
<div id="ref-wu1983convergence">
<p>[5] C. F. J. Wu, “On the convergence properties of the em algorithm,” <em>The Annals of Statistics</em>, vol. 11, no. 1, pp. 95–103, 1983.</p>
</div>
<div id="ref-bain1992">
<p>[6] L. J. Bain and M. Engelhardt, <em>Introduction to probability and mathematical statistics</em>, Second. Duxbury Press, 1992, p. 644.</p>
</div>
<div id="ref-casella2002statistical">
<p>[7] G. Casella and R. L. Berger, <em>Statistical inference</em>. Duxbury Advanced Series, 2002.</p>
</div>
<div id="ref-efron1987better">
<p>[8] B. Efron, “Better bootstrap confidence intervals,” <em>Journal of the American Statistical Association</em>, vol. 82, no. 397, pp. 171–185, 1987.</p>
</div>
<div id="ref-efron1994introduction">
<p>[9] B. Efron and R. J. Tibshirani, <em>An introduction to the bootstrap</em>. CRC press, 1994.</p>
</div>
<div id="ref-klein2005survival">
<p>[10] J. P. Klein and M. L. Moeschberger, <em>Survival analysis: Techniques for censored and truncated data</em>. Springer Science &amp; Business Media, 2005.</p>
</div>
<div id="ref-Abernethy2006">
<p>[11] R. B. Abernethy, <em>New weibull handbook</em>, 5th ed. Abernethy, 2006.</p>
</div>
<div id="ref-Huairu-2013">
<p>[12] H. Guo, P. Niu, and F. Szidarovszky, “Estimating component reliabilities from incomplete system failure data,” <em>Annual Reliability and Maintainability Symposium (RAMS)</em>, pp. 1–6, Jan. 2013, doi: <a href="https://doi.org/10.1109/rams.2013.6517765">10.1109/rams.2013.6517765</a></p>
</div>
<div id="ref-cox1972regression">
<p>[13] D. R. Cox, “Regression models and life-tables,” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, vol. 34, no. 2, pp. 187–202, 1972, doi: <a href="https://doi.org/https://doi.org/10.1111/j.2517-6161.1972.tb00899.x">https://doi.org/10.1111/j.2517-6161.1972.tb00899.x</a>. Available: <a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1972.tb00899.x">https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1972.tb00899.x</a></p>
</div>
<div id="ref-towell2023weibull">
<p>[14] A. Towell, <em>Wei.series.md.c1.c2.c3: Estimating reliability of weibull components in series from masked data</em>. 2023. Available: <a href="https://queelius.github.io/wei.series.md.c1.c2.c3/">https://queelius.github.io/wei.series.md.c1.c2.c3/</a></p>
</div>
<div id="ref-byrd1995">
<p>[15] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, “A limited memory algorithm for bound constrained optimization,” <em>SIAM Journal on Scientific Computing</em>, vol. 16, no. 5, pp. 1190–1208, 1995.</p>
</div>
<div id="ref-towell2023reliability">
<p>[16] A. Towell, “Reliability estimation in series systems: Maximum likelihood techniques for right-censored and masked failure data.” 2023. Available: <a href="https://github.com/queelius/reliability-estimation-in-series-systems">https://github.com/queelius/reliability-estimation-in-series-systems</a></p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><span class="math inline">\(T_i\)</span> is non-negative and continuous, <span class="math inline">\(R_{T_i}(t;\boldsymbol{\theta})\)</span> is a well-defined,
continuous, and differential function for <span class="math inline">\(t &gt; 0\)</span>, and <span class="math inline">\(\int_0^\infty R_{T_i}(t;\boldsymbol{\theta}) dt\)</span>
converges.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>A “fat-tailed” distribution refers to a probability distribution with tails that
decay more slowly than those of the exponential family, such as the case with the Weibull
when its shape parameter is less than <span class="math inline">\(1\)</span>. This means that extreme values are more
likely to occur, and the distribution is more prone to “black swan” events or rare occurrences.
In the context of reliability, a fat-tailed distribution might imply a higher likelihood of
unusually long lifetimes, which can skew measures like the MTTF. <span class="citation">[<a href="#ref-taleb2007black" role="doc-biblioref">2</a>]</span><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>In some likelihood models, there may be more than two possible values
for <span class="math inline">\(\delta_i\)</span>, but in this paper, we only consider the case where <span class="math inline">\(\delta_i\)</span> is
binary. Future work could consider the case where <span class="math inline">\(\delta_i\)</span> is categorical by
including more types of censoring events and more types of component cause of
failure masking.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>When doing maximum likelihood estimation, we are interested in the
parameter values that maximize the likelihood function. Since <span class="math inline">\(\beta_i\)</span> is not
a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>, it does not affect the location of the maximum of the
likelihood function, and so we can drop it from the likelihood function.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The choice of 125 iterations was also made for practical reasons.
Since we are generating millions of samples and trying to find an MLE for
each in our simulation study, if we did not limit the number of iterations, the
simulation study would have taken too long to run.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The exponential distribution is a special case of the Weibull distribution
when <span class="math inline">\(k_j = 1\)</span>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The bias-variance trade-off is a fundamental trade-off in statistics
that states that as the bias of an estimator decreases, its variance
increases, and vice versa. An estimator with more variance is
more sensitive to the data, while an estimator with more bias is less
sensitive to the data. If the assumption of homogeneity is reasonable, then
the bias can be quite small while the variance is also small, potentially
making the homogenous shape parameter model a good choice in such cases.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The fully parametric bootstrap is not appropriate for our likelihood
model because we do not assume a parametric form for the distribution of the
candidate sets.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
