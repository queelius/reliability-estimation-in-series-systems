## Bias, variance, and MSE of the MLE {#sec:acc_prec}

First, we estimate the bias, variance, and MSE of the MLE under various scenarios.
This is useful for understanding the accuracy and precision of the MLE under
different conditions. It is unrelated to the bootstrap method, but it is useful to
compute these quantities before we assess the bootstrapped variance and confidence
intervals.

A measure of the accuracy of $\v{\hat\theta}$ is the bias, which is defined as
$$
\operatorname{b}(\v{\hat\theta}) = E(\v{\hat\theta}) - \v\theta.
$$
We cannot analytically derive the bias, so we estimate the bias using the empirical
sampling distribution,
$$
\hat{\operatorname{b}}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}(\v{\hat\theta}) - \theta_j.
$$

We estimate the precision of $\hat\theta_j$ with the variance and MSE. The variance
of $\v{\hat\theta}$ is defined as
$$
\operatorname{Var}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - E_{\hat{\v\theta} \sim \text{data}}(\hat\theta_j))^2\bigr),
$$
where the expectation is taken with respect to the empirical sampling distribution.
The mean squared error is a measure of estimator error that incorporates both the
bias and the variance, and is defined as
$$
\operatorname{MSE}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - \theta_j)^2\bigr).
$$




### Absolute bias vs. sample size with a masking probability but no right-censoring {-}

In this scenario, we want to see the bias of the MLE as a function of the sample
size $n$ from $n = 30$ to $n = 800$ for a fixed masking probability $p = 0.2$ and no
right-censoring $(\tau = \infty)$. Recall that the masking probability is the
probability of including each non-failed component.


```{r plot-bias-p-0.2-vs-sample-size, echo =F, fig.cap="Bias vs. sample size (masking probability 0.2)", fig.align="center"}
knitr::include_graphics("image/plot-p-0.2-bias-vs-sample-size.pdf")
```

In Figure \ref{fig:plot-bias-p-0.2-vs-sample-size}, we plot the absolute bias
$|\operatorname{bias}(\hat\theta)|$ on a log scale against the sample size. 
However, because the absolute bias is quite large for small sample sizes and small
for large sample sizes, we use a log scale. Furthermore, we show the absolute bias
for the shape and scale parameters separately, since the scale parameters are much
larger than the shape parameters.

Here are some important observations Figure \ref{fig:plot-bias-p-0.2-vs-sample-size} reveals:

1. For both shape and scale parameters, we see that the absolute bias seems to be
decreasing to zero as the sample size increases. This is not surprising since we
expect the MLE to be consistent, i.e., $\v{\hat\theta}$ converges in probability
to $\v\theta$ as the sample size increases to infinity. Still, it is reassuring to
see that the bias seems to be behaving as expected.

2. For the shape parameters, which are small (the shape parameters have true values
a little larger than 1), the bias is relatively large for sample sizes up to
$100$.

3. For the scale parameters, which are quite large (the scale parameters have true
values around 1000). Like with the shape parameters, the bias is relatively large for
sample sizes up to $100$, but seems to stabilize and reach relatively small values
after that.

### Scenario: Bias vs. sample size and masking probability and no right-censoring {-}

Now, we take a larger view and plot the bias (without taking its absolute value
as we had done previously) against the masking probabilities $p = 0$
(no masking) to $p = 0.4$ (significant masking) for sample sizes 100, 400, and 800.

For the shape parameters, at a sample of size 100, we see significant bias and we
also see that it is very sensitive to the masking probability. See Figure
\ref{fig:plot-bias-shapes-vs-p-sample-size-100-400-800}. However, for sample sizes
of 400 and 800, the bias is relatively small and unaffected by the masking
probability.

```{r plot-bias-shapes-vs-p-sample-size-100-400-800, echo =F, fig.cap="Shape Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-shapes-p-vs-sample-size-100-400-800.pdf")
```

For the scale parameters, a similar pattern emerges, although we see that even
for sample size 400, there is evidence that the bias is still affected by the masking
probability. See Figure \ref{fig:plot-bias-scales-vs-p-sample-size-100-400-800}.

The smallest bias, as expected, occurs for sample sizes of $800$. The bias 
for $\lambda_1$ (scale parameter 1) at the masking probability $0.3$ is an interesting
case, since it jumps up at that point for some reason. We used only $R = 100$
replications, so it is plausible it would decrease with more replications.
Regardless, the overall trend is that the bias decreases as the sample size increases, and its
dependence on the masking probability is relatively small with sufficiently large
sample sizes.

```{r plot-bias-scales-vs-p-sample-size-100-400-800, echo =F, fig.cap="Scale Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-scales-p-vs-sample-size-100-400-800.pdf")
```

#### Scenario: Bias vs. right-censoring time and sample size with a fixed masking probability {-}

In this scenario, we want to isolate the effect of the right-censoring time $\tau$
on the bias. We fix the masking probability to $p = 0.215$, in line with the masking
probability we estimate for the Table 2 data set in [@Huairu-2013].

We plot the bias against the right-censoring
time for sample sizes 50, 150, and 300. See Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}.
On the $x$-axis, we report the right-censoring time as a quantile of the
Weibull series distribution so that we can more clearly see the effect of the right-censoring
on the bias, e.g., the $50\%$ quantile is the time at which $50\%$ of the
systems are expected to fail.

```{r plot-bias-tau-vs-sample-size-50-150-300, echo =F, fig.cap="Bias vs. right-censoring time and sample sizes 50, 150, and 300", fig.align="center"}
knitr::include_graphics("image/plot-bias-tau-vs-sample-size-50-150-300.pdf")
```

A few observations about Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}:

1. The bias decreases as the right-censoring time increases. This is expected since we have
more information about the system when the right-censoring time is larger.

2. The bias decreases as the sample size increases, which is also expected since we
have more information about the system when the sample size is larger.

3. The bias is relatively small for sample sizes 150 and 300, but for sample size 50,
the bias is quite large, particularly for the shape parameters. This is not
surprising since the sample size is quite small, and so we do not expect the MLE to
be very accurate.


### Variance

```{r var.plot, echo=F, fig.cap="Variance vs. sample size", fig.align="center"}
library(ggplot2)
library(dplyr)
library(readr)
library(tidyverse)
library(latex2exp)

# Load the data
sim_data <- read_csv("results/data/data-fim.csv")

# rename columns for plotting
sim_data <- sim_data %>% rename(
  "Shape 1" = mle.1,
  "Scale 1" = mle.2,
  "Shape 2" = mle.3,
  "Scale 2" = mle.4,
  "Shape 3" = mle.5,
  "Scale 3" = mle.6,
  "Shape 4" = mle.7,
  "Scale 4" = mle.8,
  "Shape 5" = mle.9,
  "Scale 5" = mle.10
)

# Filter data
filtered_data <- sim_data %>% filter(p == 0 & q == 1)

# Compute variances
shape_data <- filtered_data %>% select(n, "Shape 1", "Shape 2", "Shape 3", "Shape 4", "Shape 5")
scale_data <- filtered_data %>% select(n, "Scale 1", "Scale 2", "Scale 3", "Scale 4", "Scale 5")

shape_variances <- shape_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)
scale_variances <- scale_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)

# Reshape the data for ggplot2
shape_variances_long <- shape_variances %>% gather(key = "Parameter", value = "Variance", -n)
scale_variances_long <- scale_variances %>% gather(key = "Parameter", value = "Variance", -n)

# Plotting
ggplot(shape_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Shape Parameter MLEs with respect to Sample Size") +
  theme_minimal()

ggplot(scale_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Scale Parameter MLEs with respect to Sample Size") +
  theme_minimal()
```
















## TEST -- INCORRECT ANALYSIS BUT MAY BE USEFUL FOR REF FOR WRITING
Average Width of Confidence Interval (CI) for shape.1 as a function of $p$:
The plot shows that as the candidate masking probability ($p$) increases, the average width of the confidence interval also increases. This suggests that as the masking of the component cause increases, our uncertainty about the estimated shape parameter also increases. This is consistent with the expectation that more masking leads to less precise estimates, resulting in wider confidence intervals.

Coverage Probability for shape.1 as a function of $p$:
The coverage probability represents the proportion of confidence intervals that contain the true value of the parameter. The plot shows that the coverage probability fluctuates around
$0.92$ to $0.95$ and does not have a clear trend with respect to 
$p$. This suggests that, despite the increasing width of the confidence intervals, the method still maintains a high coverage probability. However, there is a slight drop in coverage probability at 
$p=0.5$, which might be an area to investigate further.

Expected MLE for shape.1 as a function of $p$:
The plot shows that the expected value of the MLE for shape.1 increases as 
$p$ increases, suggesting that the estimate is biased and the bias increases with 
$p$. The true value is consistently lower than the expected MLE, indicating an overestimation bias.

The plots reveal that while the method maintains a high coverage probability, the increasing width of the confidence intervals and
the bias in the MLE estimates could be areas of concern, particularly at higher values of 
$p$. It might be beneficial to investigate ways to reduce the bias and improve the precision of the estimates, especially in scenarios with high masking probability.


## Coverage probability vs. sample size with a fixed masking probability and no right-censoring

We want to isolate the effect of the coverage probability as a function of the sample
size $n$. We fix the masking probability to $p = 0.2$ and without right-censoring $(\tau = \infty)$
and vary the sample size from $n = 30$ to $n = 800$. See Figure \ref{fig:plot-coverage-p-three-vs-sample-size}.

```{r plot-coverage-p-three-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size for masking probability $0.3$", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p_0.3-vs-sample-size.pdf")
```

Here are some key observations:

1. It is immediately obvious that the scale parameters (dashed lines) have a much
   lower coverage probability than the shape parameters (solid lines), particularly
   for small sample sizes less than $n = 200$.
   
   In general, the scale parameters appear to be more difficult to estimate than the
   shape parameters. 

2. As the sample size increases, the coverage probability for the shape parameters
   and scale parameters approaches the nominal level, $95\%$.
   
   This suggests that the sampling distribution of the MLE is converging in
   distribution to a multivariate normal distribution with mean $\v\theta$ and
   variance-covariance given by the inverse of the FIM, consistent with the
   asymptotic theory.

## Coverage probability vs. sample size and masking probability without right-censoring

We want to get a larger picture of how the coverage probability depends on the
sample size $n$ and masking probability $p$. We fix the right-censoring time to
$\tau = \infty$ and vary the sample size from $n = 30$ to $n = 800$ and vary the
masking probability from $p = 0$ (no masking) to $p = 0.4$ and then compute the
coverage probability for each combination of sample size $n$ and masking probability
$p$.

```{r plot-coverage-p-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p-vs-sample-size.pdf")
```

The results of this analysis are summarized by Figure \ref{fig:plot-coverage-p-vs-sample-size}.
Here are some key observations:

1. For sample sizes $n \leq 100$, the coverage probability for the shape parameters
is close to the nominal level, $95\%$, only for small masking probabilities. However,
as the sample size increases, the coverage probability for the shape parameters
quickly approaches the nominal level, $95\%$, for all masking probabilities reported
here.

2. For the scale parameters, the coverage probability is too low for all sample sizes
$n < 200$ for all masking probabilities reported here. For small sample sizes, the
confience intervals particularly for the scale parameters, should probably be taken
with a grain of salt.

In Section \ref{sec:boot}, we explore an alternative way to construct confidence
intervals using the bootstrap method, which is generally a more accurate way to
compute confidence intervals. Unlike the inverse of the observed FIM, it does not
assume that the sampling distribution of the MLE is asymptotically normal, and so
it is more robust to violations of this assumption.
