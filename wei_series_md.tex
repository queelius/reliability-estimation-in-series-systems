% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Estimating the reliability of components in a series systems from masked failure data},
  pdfauthor={Alex Towell},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{tikz}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {}%
  {\par}

\title{Estimating the reliability of components in a series systems from
masked failure data}
\author{Alex Towell}
\date{}

\begin{document}
\maketitle
\begin{abstract}
We estimate the reliability of series systems from masked failure data
consisting of right-censored system lifetimes and masked component cause
of failures. Under a minimal set of conditions that permit us to ignore
how the component cause of failures are masked, we estimate the sampling
distribution of the MLE in a variety of situations. We find that as long
as the masking does not always include a particular component as a
potential cause of failure whenever another component is also a
potential cause, the MLE is unique and consistent. However, this is
often not a realistic situation in industrial settings. For example, a
system may be repaired by replacing an entire circuit board consisting
of multiple components, in which case whenever one of the components on
that circuit board fails, all of the components on the circuit board are
potential component causes of failure. In this case, we have a
non-unique MLE, which provides less information about the reliability of
the series system and its components.
\end{abstract}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newcommand{\T}{T}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\renewcommand{\v}[1]{\boldsymbol{#1}}
\numberwithin{equation}{section}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Accurate reliability specifications of components of a multi-component
system, such as mean time to failure (MTTF), can be quite valuable. For
instance, such information may be used to help identify the component
cause of system failure. However, this information is often not
available, or if it is available, it may not be very accurate. In this
case, we may try to estimate the reliability of each component from
system failure data.

We are interested in estimating the reliability of \emph{series
systems}. In a series system, the system fails whenever any of its
components fail. The famous statistician George Box once opined that all
models are wrong, but some are useful, meaning that a theoretical model
of real phenomenon is incapable of representing its exact behaviour, but
it may still be useful. Indeed, sometimes it is even more useful, since
a simplified model is more understandable and only includes the most
salient features.

Since many real-world systems are greatly impaired whenever a set of its
\emph{critical} components fail, the series system model is often a very
\emph{useful} approximation of its internal structure. For example,
people in a particular experiment may be regarded as series systems
where the components are some relevant set of vital organs. This model
is often a very useful abstraction that averages over many complicated,
unobserved details that may only have a small effect on an average
person's lifespan.

In this paper, the experimental unit is a particular type of series
system that consists of \(m\) components and we seek to estimate the
reliability of the components from \emph{masked data} where we observe
only partial information about the system, e.g., the component lifetimes
are masked (not observed in the sample).

We imagine that we conduct an experiment where we obtain the lifetimes
of the \(m\) components, which has a sample space \((0,\infty)^m\). An
observed result is called an \emph{outcome} of the experiment, e.g.,
\((1.2,0.5,1.1)\). However, we assume that this data is masked in the
following ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The experiments may be right-censored, which means that the experiment
  or observation may be suspended before the system fails.
\item
  We can only observe the lifetime of the component with the minimum
  lifetime. This constraint is imposed by the fact that the system is
  arranged in series and thus stops working whenever one of the
  components fails.
\item
  It may not be easy or possible to identify the failed component. We
  consider the case where we can only observe a set of components that
  includes the failed component. We refer to this set of components as
  the candidate set.\footnote{
  If the candidate set has only a single component, then we know the exact
  component cause of failure.}

  This constraint may be due to a variety of reasons. A canonical
  example is given by supposing that when the series system fails, it is
  repaired by simultaneously replacing some subset of the components,
  thus preventing isolating the specific failed component.
\end{enumerate}

We take a random sample of \(n\) such experiments, where the
right-censored system lifetimes are i.i.d. when conditioned on the
right-censoring time, but the candidate sets are assumed to be
independent but not necessarily identically distributed. In later
sections, we consider a set of realistic conditions that allow us to
ignore the distribution of candidate sets, which allows us to construct
a reduced likelihood function for the masked failure data. Finally, we
find the MLE \(\hat{\boldsymbol{\theta}}\) that maximizes the reduced
likelihood function, which is the same MLE for ``full'' likelihood
function where we know the distribution of candidate sets.

\hypertarget{sec:statmod}{%
\section{Statistical model}\label{sec:statmod}}

Consider a system with \(m\) components, indexed by \(1,2,\ldots,m\).
Each component only has two possible states, \emph{failed} and
\emph{functioning}. The state of the system also only has two possible
states, \emph{failed} and \emph{functioning}. The lifetime of a system
is the elapsed time from when the new, functioning system is put into
operation until it fails for the \emph{first time}. We are not
interested in what happens to a system after it fails, e.g., we do not
consider repairing systems and putting them back into operation under
further observation.

A system that is in a functioning state if and only if at least \(k\) of
the \(m\) components are in a functioning state is denoted a
\(k\)-out-of-\(m\) system. We narrow our scope to \(m\)-out-of-\(m\)
systems, also known as \emph{series systems} (with \(m\) components). A
series system is functioning if and only if every component is
functioning. Consequently, in a series system, precisely one component
is the cause of failure.

Our sample consists of lifetime data for i.i.d. series systems. Since
the component lifetimes are subject to chance variations, we denote the
lifetime of the \(j\)\textsuperscript{th} component of the
\(i\)\textsuperscript{th} system by the random variable \(T_{i j}\). We
assume that the \(m\) component lifetimes, \(T_{i 1},\ldots,T_{i m}\),
are statistically independent and non-identical. In a series system,
whenever any component fails, the system fails. Thus, the lifetime of
the \(i\)\textsuperscript{th} system, \(T_i\), is given by the component
with the smallest lifetime, \[
    T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
\]

We assume that the component lifetimes may be adequately modeled by some
parametric probability distribution. In what follows, matrices and
vectors are denoted in boldface, e.g., \(\boldsymbol{x}\) is a vector.
The \(i\)\textsuperscript{th} column and \((i,j)\)-th element of a
matrix \(\boldsymbol{A}\) is denoted respectively by
\(\boldsymbol{A_j}\) and \(\boldsymbol{A}_{i j}\). We let
\(\boldsymbol{\theta_j}\) denote the parameter vector of the
\(j\textsuperscript{th}\) component, and each \(\boldsymbol{\theta_j}\)
for \(j=1,\ldots,m\) may be different sizes. We define the parameter
vector of the entire series system by \[
    \boldsymbol{\theta }= (\boldsymbol{\theta_1},\ldots,\boldsymbol{\theta_m}).
\]

The cumulative distribution function (cdf) for a random varible \(T\) is
given by \begin{equation}
 F_T(t) = \Pr\{T \leq t\},
\end{equation} where \(\Pr\{T \in E\}\) denotes the probability that a
random variable \(T\) realizes a value in \(E\). If a random variable
\(X\) is discrete, then its probability mass function (pmf) \(f_X\) is
given by\(f_X(x) = \Pr\{X = x\}\). The probability density function
(pdf) for a continuous random varible \(T\) is given by \begin{equation}
f_T(t) = \frac{d}{dt} F_T(t).
\end{equation}

If a random variable \(T\) has a parametric distribution indexed by a
parameter vector \(\boldsymbol{\beta}\), we denote its pdf by
\(f_T(t;\boldsymbol{\beta})\) and likewise for other distribution
functions, e.g., the \(i\)\textsuperscript{th} series system has a cdf
denoted by \(F_{T_i}(t;\boldsymbol{\theta})\). In the case of the
lifetime distribution functions of components, we subscript the
distribution functions by component index instead of the symbol for the
random variable, e.g., the cdf of the \(j\)\textsuperscript{th}
component for the \(i\)-th system is denoted by
\(F_j(t;\boldsymbol{\theta_j})\) instead of
\(F_{T_{i j}}(t;\boldsymbol{\theta_j})\). If it is clear from the
context which random variable a distribution function is for, we may
drop the subscripts, e.g., \(F(t)\) instead of \(F_T(t)\). Finally, as
an abuse of notation, we often write a function as \(f(t)\) when we
really mean that \(f\) is a function of variable \(t\).

There are two particularly important functions in survival analysis, the
survival function and the hazard function.

\begin{definition}
The survival function, $R_T(t)$, of a random lifetime $T$ is the
probability that it realizes a value larger than some specified duration of time $t$,
\begin{equation}
\begin{split}
R_T(t) &= \Pr\{T > t\}\\
     &= 1 - F_T(t).
\end{split}
\end{equation}
In other words, $R_T(t)$ denotes the probability that $T$ survives longer than
$t$.
\end{definition}

The hazard function is a bit more subtle. For a random lifetime \(T\),
the probability that a failure occurs between \(t\) and \(\Delta t\)
given that no failure occurs before time \(t\) is given by \[
\Pr\{T \leq t+\Delta t|T > t\} = \frac{\Pr\{t < T < t+\Delta t\}}{\Pr\{T > t\}}.
\] The \emph{failure rate} is given by the above divided by the length
of the time interval, \(\Delta t\): \[
\frac{\Pr\{t < T < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T > t\}} =
    \frac{R_T(t) - R(t+\Delta t)}{R_T(t)}.
\]

\begin{definition}
\label{def:failure_rate}
The hazard function $h_T(t)$ for a continuous random variable $T$ is the
instantaneous failure rate at time $t$, which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_T(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < T < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T > t\}}\\
       &= \frac{f_T(t)}{R_T(t)}.
\end{split}
\end{equation}
\end{definition}

Two random variables \(X\) and \(Y\) have a joint pdf \(f_{X,Y}(x,y)\).
Given the joint pdf \(f(x,y)\), the marginal pdf of \(X\) is given by \[
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
\] where \(\mathcal{Y}\) is the support of \(Y\). (If \(Y\) is discrete,
replace the integration with a summation over \(\mathcal{Y}\).)

The conditional pdf of \(Y\) given \(X=x\), \(f_{Y|X}(y|x)\), is defined
as \[
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\] We may generalize all of the above to more than two random variables,
e.g., the joint pdf of \(X_1,\ldots,X_m\) is denoted by
\(f(x_1,\ldots,x_m)\).

\hypertarget{sec:data}{%
\subsection{Masked data}\label{sec:data}}

The object of interest is the (unknown) parameter value
\(\boldsymbol{\theta}\). To estimate this \(\boldsymbol{\theta}\), we
need \emph{data}. Generally, we encounter three types of life data in
this context: exact lifetimes, interval lifetimes (in which a lifetime
is known to have occurred between two points in time), and right
censored lifetimes (in which we only know that a lifetime is greater
than some point in time, e.g., the experiment was terminated or
suspended before the system failed).

In this paper, we limit our focus to right censored lifetimes. We
consider a sample of \(n\) i.i.d. series systems, each of which is put
into operation at some time and and observed until either it fails or is
right-censored.

We denote the right-censoring time of the \(i\)\textsuperscript{th}
system by \(\tau_i\). Whether it is right-censored is a random variable
defined as \begin{equation}
    \delta_i = 1_{T_i < \tau_i}
\end{equation} where \(1_{\text{condition}}\) is an indicator function
that outputs \(1\) if \emph{condition} is true and \(0\) otherwise.
Here, \(\delta_i = 1\) indicates the event of interest, a system
failure, was observed. The right-censored time of the
\(i\)\textsuperscript{th} system, \(S_i\), is given by \begin{equation}
    S_i = \min\{\tau_i, T_i\}.
\end{equation}

In addition to the above, we also have masked data about the component
cause of failure in the form of candidate sets. A candidate set consists
of some subset of component indices that plausibly contain the index of
the failed component. The sample space of candidate sets are all subsets
of \(\{1,\ldots,m\}\), thus there are \(2^m\) possible outcomes in the
sample space.

Let's define the candidate set corresponding to the
\(i\)\textsuperscript{th} system as \(c_i\). Since the data generating
process for candidate sets may be subject to chance variations, we model
it as a random set \(\mathcal{C}_i\). We may also represent the
candidate set \(\mathcal{C}_i\) as a random Boolean vector
\(X_{i 1},\ldots,X_{i m}\) where \(X_{i j}=1\) denotes that the
\(j\)\textsuperscript{th} component index is in the candidate set for
the \(i\)\textsuperscript{th} system.

When we refer to the sequence \(a_1,\ldots,a_k\), we use the notation
\(\{a_i\}_{i \leq k}\). If it is clear from the context, we may also use
\(\{a_i\}\). Our random sample of masked data is give by
\begin{equation}
\label{eq:masked_dgp}
    \{(S_i,\delta_i,\mathcal{C}_i)\}_{i \leq n},
\end{equation} where \(\mathcal{C}_i\) is the random candidate set,
\(S_i\) is the right-censored lifetime, and \(\delta_i\) is the
indicator for right-censoring for the \(i\)\textsuperscript{th} series
system. The masked data generation process for the sample in
\eqref{eq:masked_dgp} is illustrated by Figure \ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$(S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data for exact, right-censored system failure times
with candidate sets that mask the component cause of failure can be seen
in Table 1 for a series system with \(3\) components (also known as an
\(3\)-out-of-\(3\) system).

\begin{longtable}[]{@{}llll@{}}
\caption{Right-censored lifetime data with masked component cause of
failure. The \(3\)\textsuperscript{rd} system is right-censored
(\(\delta_3 = 1\)) with a right-censoring time of
\(s_3 = \tau_3 = 5.4\). The candidate set for the
\(3\)\textsuperscript{rd} system is the empty set (\(c_3 = \emptyset\)),
which indicates that the system has no failed components (therefore
there cannot be a plausible subset of the components that contains the
failed component).}\tabularnewline
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
System\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right-censored time (\(S_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right censored (\(\delta_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Candidate set (\(C_i\))\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
System\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right-censored time (\(S_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right censored (\(\delta_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Candidate set (\(C_i\))\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(4.3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{1,2\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(1.3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{2\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
3\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(5.4\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\emptyset\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(2.6\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{2,3\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(3.7\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{1,2,3\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
6\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(10\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\emptyset\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{sec:ttf}{%
\section{Series system lifetime}\label{sec:ttf}}

The previous section served as an introduction to key concepts in
reliability theory: the reliability function, the probability density
function (pdf), and the hazard function. Here, we dive deeper into these
concepts and provide mathematical derivations for the metrics specific
to series systems. These are not purely theoretical explorations; these
derivations are crucial in applying Maximum Likelihood Estimation to our
masked data model.

We begin with the reliability function of the series system, as given by
the following theorem.

\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_i > t\}
$$
which may be rewritten as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\boldsymbol{\theta}) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_reliability_function} shows that the system's
overall reliability is the product of the reliabilities of its
individual components. This property is inherent to series systems and
will be used in the subsequent derivations.

Next, we turn our attention to the pdf of the system lifetime, described
in the following theorem.

\begin{theorem}
\label{thm:sys_pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f(t;\boldsymbol{\theta}) = \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f(t;\boldsymbol{\theta}) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f(t;\boldsymbol{\theta})
    &= -\frac{d}{dt} R_1(t;\boldsymbol{\theta_1})\prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j})\\
    &= f_1(t;\boldsymbol{\theta}) \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f(t;\boldsymbol{\theta}) = \sum_{j=1}^{m-1} f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\boldsymbol{\theta_j}) \frac{d}{dt} R_m(t;\boldsymbol{\theta_m}),
$$
which simplifies to
$$
f(t;\boldsymbol{\theta})= \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}).
$$
\end{proof}

Theorem \ref{thm:sys_pdf} shows the pdf of the system lifetime as a
function of the pdfs and reliabilities of its components.

We continue with the hazard function of the system lifetime, defined in
the next theorem.

\begin{theorem}
\label{thm:sys_failure_rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h(t;\boldsymbol{\theta}) = \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h(t;\boldsymbol{\theta}) = \frac{f_{T_i}(t;\boldsymbol{\theta})}{R_{T_i}(t;\boldsymbol{\theta})}.
$$
Plugging in expressions for these functions results in
$$
  h(t;\boldsymbol{\theta}) = \frac{\sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k})}
      {\prod_{j=1}^m R_j(t;\boldsymbol{\theta_j})},
$$
which can be simplified to
\begin{align*}
h_{T_i}(t;\boldsymbol{\theta})
    &= \sum_{j=1}^m \frac{f_j(t;\boldsymbol{\theta_j})}{R_j(t;\boldsymbol{\theta_j})}\\
    &= \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}).
\end{align*}
\end{proof}

Theorem \ref{thm:sys_failure_rate} reveals that the system's hazard
function is the sum of the hazard functions of its components.

By definition, the hazard function is the ratio of the pdf to the
reliability function, \[
h(t;\boldsymbol{\theta}) = \frac{f(t;\boldsymbol{\theta})}{R(t;\boldsymbol{\theta})},
\] and we can rearrange this to get \begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f(t;\boldsymbol{\theta}) &= h(t;\boldsymbol{\theta}) R(t;\boldsymbol{\theta})\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\boldsymbol{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}) \biggr\},
\end{split}
\end{equation} which we sometimes find to be a more convenient form than
Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's
reliability function, pdf, and hazard function. Next, we build upon
these concepts to derive distributions related to the component cause of
failure.

\hypertarget{sec:comp_cause}{%
\subsection{Component cause of failure}\label{sec:comp_cause}}

Whenver a series system fails, precisely one of the components failed to
cause the system failure. We model the component cause of the series
system failure as a random variable.

\begin{definition}
The component cause of failure of the $i$\textsuperscript{th} series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}

Note that a more succinct way to define \(K_i\) is given by \[
K_i = \operatorname{argmin}_j \bigl\{ T_{i j} : j \in \{1,\ldots,m\}\bigr\}.
\]

The system lifetime and the component cause of failure has a joint
distribution given by the following theorem.

\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\boldsymbol{\theta}) = h_j(t;\boldsymbol{\theta_j}) R_{T_i}(t;\boldsymbol{\theta}),
\end{equation}
where $h_j(t;\boldsymbol{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_{T_i}(t;\boldsymbol{\theta})$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a $3$-out-of-$3$ system.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\boldsymbol{\theta}) = \prod_{j=1}^{3} f_j(t;\boldsymbol{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\boldsymbol{\theta}) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2}) f_3(t_3;\boldsymbol{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2})
        R_3(t;\boldsymbol{\theta_3}) dt_2\\
     &= f_1(t;\boldsymbol{\theta_1}) R_2(t;\boldsymbol{\theta_2}) R_3(t_1;\boldsymbol{\theta_3}).
\end{align*}
Since $h_1(t;\boldsymbol{\theta_1}) = f_1(t;\boldsymbol{\theta_1}) / R_1(t;\boldsymbol{\theta_1})$,
$$
f_1(t;\boldsymbol{\theta_1}) = h_1(t;\boldsymbol{\theta_1}) R_1(t;\boldsymbol{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\boldsymbol{\theta})$
yields
\begin{align*}
f_{K_i,T_i}(j,t;\boldsymbol{\theta})
    &= h_1(t;\boldsymbol{\theta_1}) \prod_{l=1}^m R_l(t;\boldsymbol{\theta_l})\\
    &= h_1(t;\boldsymbol{\theta_1}) R(t;\boldsymbol{\theta}).
\end{align*}
Generalizing from this completes the proof.
\end{proof}

\hypertarget{sec:like_model}{%
\section{Likelihood Model for Masked Data}\label{sec:like_model}}

The likelihood is a function that quantifies the plausibility of
observed data given specific parameter values, denoted here as
\(\boldsymbol{\theta}\). In our model, we will consider two types of
masked data: \emph{right-censored} system lifetime data, which obscures
the true system lifetime, and \emph{masking of the component cause of
failure}, where the exact cause of failure is unknown but the system
lifetime is precisely known. For each type of data, we will derive the
\emph{likelihood contributions}.

Given an i.i.d. random sample of masked data \[
\left\{(S_i,\delta_i,\mathcal{C}_i)\right\}_{i \leq n}
\] parameterized by \(\boldsymbol{\theta }\in \boldsymbol{\Omega}\), the
joint pdf is given by \[
f\bigl(\{(s_i,\delta_i,\mathcal{C}_i)\}_{i \leq n};\boldsymbol{\theta}\bigr)
    = \prod_{i=1}^n f(s_i,\delta_i,\mathcal{C}_i;\boldsymbol{\theta}).
\] The joint pdf computes the likelihood that the observed data,
\(\{s_i,\delta_i,c_i\}_{i \leq n}\), occurs. When we fix the data and
instead allow the parameter \(\boldsymbol{\theta}\) to vary, we obtain
what is called the likelihood function, \[
L(\boldsymbol{\theta}) = \prod_{i=1}^n L_i(\boldsymbol{\theta})
\] where \[
L_i(\boldsymbol{\theta}) = f(s_i,\delta_i,c_i;\boldsymbol{\theta})
\] is the likelihood contribution of the \(i\)\textsuperscript{th}
system.

We present the following theorem for the likelihood contribution model.
In subsequent subsections, we derive this result for each type of masked
data, i.e., right-censored system lifetime data \((\delta_i = 0)\), and
masking of the component cause of failure \((\delta_i = 1)\).

\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
\end{equation}
\end{theorem}

Next, we derive the likelihood contributions for each type of masked
data.

\hypertarget{sec:candmod}{%
\subsection{Masked component cause of failure}\label{sec:candmod}}

Suppose a diagnostician is unable to identify the precise component
cause of the failure, e.g., due to cost considerations he or she
replaced multiple components at once, successfully repairing the system
but failing to precisely identity the failed component. In this case,
the cause of failure is said to be \emph{masked}.

The unobserved component lifetimes may have many covariates, like
ambient operating temperature, but the only covariate we observe in our
masked data model are the system's lifetime and additional masked data
in the form of a candidate set that is somehow correlated with the
unobserved component lifetimes.

The ultimate goal of our analysis is to estimate the parameters
\(\boldsymbol{\theta}\) that maximize the likelihood of our observed
data. To achieve this, we first need to consider the joint distribution
of the continuous system lifetime \(T_i\) and the discrete candidate set
\(\mathcal{C}_i\), which can be written as \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = f_{T_i}(t_i;\boldsymbol{\theta})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\},
\] where \(f_{T_i}(t_i;\boldsymbol{\theta})\) is the pdf \(T_i\) and
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\) is
the conditional pmf of \(\mathcal{C}_i\) given \(T_i = t_i\).

We assume we know the pdf \(f_{T_i}(t_i;\boldsymbol{\theta})\) but we do
not know
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\),
i.e., the data generating process for candidate sets is not known.
However, in order for the masked data \(\mathcal{C}_i\) to provide
information about \(\boldsymbol{\theta}\), \(\mathcal{C}_i\) must be
correlated with the \(i\)\textsuperscript{th} system in some useful way,
in which case the conditional distribution of \(\mathcal{C}_i\) given
\(T_i = t_i\) may be important even though the object of statistical
interest is the series system rather than the candidate sets, which may
be statistically problematic, e.g.,
\(\mathcal{C}_1,\mathcal{C}_2,\ldots,\mathcal{C}_n\) may not be
identically distributed or there may be many important unobserved
covariates.

To make this problem tractable, we assume a set of conditions that make
it unnecessary to estimate the generative processes for candidate sets.
The most important way in which \(\mathcal{C}_i\) is correlated with the
\(i\)\textsuperscript{th} system is given by assuming the following
condition.

\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\boldsymbol{\theta}}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}

Assuming Condition \ref{cond:c_contains_k}, \(\mathcal{C}_i\) must
contain the index of the failed component, but we can say little else
about what other component indices may appear in \(\mathcal{C}_i\).

In order to derive the joint distribution of \(\mathcal{C}_i\) and
\(T_i\) assuming Condition \ref{cond:c_contains_k}, we take the
following approach. We notice that \(\mathcal{C}_i\) and \(K_i\) are
statistically dependent. We denote the conditional pmf of
\(\mathcal{C}_i\) given \(T_i = t_i\) and \(K_i = j\) as \[
\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
\]

Even though \(K_i\) is not observable in our masked data model, we can
still consider the joint distribution of \(T_i\), \(K_i\), and
\(\mathcal{C}_i\). By Theorem \ref{thm:f_k_and_t}, the joint pdf of
\(T_i\) and \(K_i\) is given by \[
f_{T_i,K_i}(t_i,j;\boldsymbol{\theta}) = h_j(t_i;\boldsymbol{\theta_j}) R_{T_i}(t_i;\boldsymbol{\theta}),
\] where \(h_j(t_i;\boldsymbol{\theta_j})\) is the hazard function for
the \(j\)\textsuperscript{th} component and
\(R_{T_i}(t_i;\boldsymbol{\theta})\) is the reliability function of the
system. Thus, the joint pdf of \(T_i\), \(K_i\), and \(\mathcal{C}_i\)
may be written as \begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\boldsymbol{\theta})
    &= f_{T_i,K_i}(t_i,k;\boldsymbol{\theta}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\boldsymbol{\theta_j}) R_{T_i}(t_i;\boldsymbol{\theta})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation} We are going to need the joint pdf of \(T_i\) and
\(\mathcal{C}_i\), which may be obtained by summing over the support
\(\{1,\ldots,m\}\) of \(K_i\) in Equation \eqref{eq:joint_pdf_t_k_c}, \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\] By Condition \ref{cond:c_contains_k},
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0\)
when \(K_i = j\) and \(j \notin c_i\), and so we may rewrite the joint
pdf of \(T_i\) and \(\mathcal{C}_i\) as \begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of \(\boldsymbol{\theta}\) (see Section
\ref{sec:mle}), we solve the simultaneous equations of the MLE and
choose a solution \(\hat{\boldsymbol{\theta}}\) that is a maximum for
the likelihood function. When we do this, we find that
\(\hat{\boldsymbol{\theta}}\) depends on the unknown conditional pmf
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\).
So, we are motivated to seek out more conditions (that approximately
hold in realistic situations) whose MLEs are independent of the pmf
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\).

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}

According to {[}1{]}, in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In
this setting, Condition \ref{cond:equal_prob_failure_cause} generally
holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\) may
be factored out of the summation in Equation \eqref{eq:part1}, and thus
the joint pdf of \(T_i\) and \(\mathcal{C}_i\) may be rewritten as \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j})
\] where \(j' \in c_i\).

If \(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\)
is a function of \(\boldsymbol{\theta}\), the MLEs are still dependent
on the unknown
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\).
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\) to
find an MLE of \(\boldsymbol{\theta}\). The last condition we assume
achieves this result.

\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $T_i$ and component cause
of failure $K_i$ are not functions of $\boldsymbol{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\boldsymbol{\theta}$.
\end{condition}

When Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}
are satisfied, the joint pdf of \(T_i\) and \(\mathcal{C_i}\) is given
by \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \beta_i R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}).
\] When we fix the sample and allow \(\boldsymbol{\theta}\) to vary, we
obtain the contribution to the likelihood \(L\) from the
\(i\)\textsuperscript{th} observation when the system lifetime is
exactly known (i.e., \(\delta_i = 1\)) but the component cause of
failure is masked by a candidate set \(c_i\): \begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta}) \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta},
if we observe an exact system failure time for the \(i\)-th system
(\(\delta_i = 1\)), but the component that failed is masked by a
candidate set \(c_i\), then its likelihood contribution is given by
Equation \eqref{eq:likelihood_contribution_masked}.

\hypertarget{conditional-distribution-of-k_i-given-t_i-and-mathcalc_i}{%
\subsubsection{\texorpdfstring{Conditional distribution of \(K_i\) given
\(T_i\) and
\(\mathcal{C}_i\)}{Conditional distribution of K\_i given T\_i and \textbackslash mathcal\{C\}\_i}}\label{conditional-distribution-of-k_i-given-t_i-and-mathcalc_i}}

This subsection is not necessary in our likelihood model, but it derives
a useful result for making predictions about the component cause of
failure.

Suppose we have jointly observed a candidate set and a series system
failure time and we are interested in the probability that a particular
component is the cause of the observed system failure.

\begin{theorem}
Assuming Conditions \ref{cond:c_contains_k} and \ref{cond:equal_prob_failure_cause},
the conditional probability of $K_i$ given $\mathcal{C}_i = c_i$ and $T_i = t_i$
is given by
\begin{equation}
\label{eq:cond_prob_k_given_t_and_c}
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
 \frac{h_j(t_i|\boldsymbol{\theta_j})}{\sum_{l \in c_i} h_l(t_i|\boldsymbol{\theta_l})} 1_{\{j \in c_i\}}.
\end{equation}
\end{theorem}
\begin{proof}
The conditional probability $\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\}$ may be
written as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} f_{K_i,T_i}(j,t_i;\boldsymbol{\theta})}
    {\sum_{j=1}^m \Pr{}_{\!\boldsymbol{\theta}}
        \{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} f_{K_i,T_i}(j,t_i;\boldsymbol{\theta})}.
$$
By Theorem \ref{thm:f_k_and_t},
$f_{K_i,T_i}(j,t_i;\boldsymbol{\theta}) = h_j(t_i;\boldsymbol{\theta})R_{T_i}(t_i;\boldsymbol{\theta})$.
We may make this substitution in the above equation and cancel the common
factors $R_{T_i}(t_i;\boldsymbol{\theta})$ in the numerator and denominator, yielding
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} h_j(t_i;\boldsymbol{\theta})}
         {\sum_{j=1}^m \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i\} h_j(t_i;\boldsymbol{\theta})}.
$$
Assuming Condition \ref{cond:c_contains_k}, we may rewrite the above as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\}
    = \frac{\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} h_j(t_i;\boldsymbol{\theta})}
        {\sum_{l \in c_i} \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = l,T_i=t_i\} h_j(t_i;\boldsymbol{\theta})}.
$$
Assuming Condition \ref{cond:equal_prob_failure_cause}, we may rewrite the above
as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = j',T_i=t_i\} h_j(t_i;\boldsymbol{\theta_j})}
    {\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|K_i = j',T_i=t_i\} {\sum_{l \in c_i} h_l(t_i;\boldsymbol{\theta_l})}},
$$
where $j' \in c_i$.
Finally, we may cancel the common probability masking factors in the numerator
and denominator, obtaining the result
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{h_j(t_i;\boldsymbol{\theta_j})}
    {\sum_{l \in c_i} h_l(t_i;\boldsymbol{\theta_l})}.
$$
\end{proof}

Frequently, we may not have any information at all about the component
cause of failure, but we may still want to estimate the probability that
a particular component is the cause of a system failure at a particular
time. In this case, we may use the following corollary.

\begin{corollary}
The probability that the $j$\textsuperscript{th}
component is the cause of system failure given only that we know a system failure
occured at time $t_i$ is given by
$$
\Pr\{K_i = j|T_i=t_i\} = \frac{h_j(t_i;\boldsymbol{\theta_j})}{\sum_{j=1}^m h_j(t_i;\boldsymbol{\theta})}.
$$
\end{corollary}
\begin{proof}
If we cannot narrow the component cause of failure down to some subset of the
components, then we let $c_i$ contain all $m$ components, $c_i = \{1,\ldots,m\}$, and
plug that into Equation \ref{eq:cond_prob_k_given_t_and_c}. The result immediately
follows.
\end{proof}

\hypertarget{right-censored-data}{%
\subsection{Right-censored data}\label{right-censored-data}}

As described in Section \ref{sec:data}, we observe realizations of
\((S_i,\delta_i,\mathcal{C}_i)\) where \(S_i = \min\{T_i,\tau_i\}\) is
the right-censored system lifetime, \(\delta_i = 1_{\{T_i < \tau_i\}}\)
is the right-censoring indicator, and \(\mathcal{C}_i\) is the candidate
set.

In the previous section, we discussed the likelihood contribution from
an observation of a masked component cause of failure, i.e.,
\(\delta_i = 1\). We now derive the likelihood contribution of a
\emph{right-censored} observation \((\delta_i = 0\)) in our masked data
model.

\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\boldsymbol{\theta}) = R_{T_i}(s_i;\boldsymbol{\theta}).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau_i$, and we only know that
$T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\boldsymbol{\theta}) = \Pr\!{}_{\boldsymbol{\theta}}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\boldsymbol{\theta}) = R_{T_i}(s_i;\boldsymbol{\theta}).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the
likelihood contribution for the \(i\)\textsuperscript{th} system shown
in Theorem \ref{thm:likelihood_contribution}, \[
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
\]

We use this result in the next section to derive the maximum likelihood
estimator of \(\boldsymbol{\theta}\).

\hypertarget{sec:mle}{%
\section{Maximum likelihood estimation}\label{sec:mle}}

We use maximum likelihood estimation (MLE) to estimate the series system
parameter vector \(\boldsymbol{\theta}\) given the masked failure data
described in Section \ref{sec:data}. This is achieved by maximizing the
likelihood function \(L(\boldsymbol{\theta})\) with respect to
\(\boldsymbol{\theta}\) so that, under the assumed model, the observed
data is most likely. The point in the parameter space \(\Omega\) that
maximizes the likelihood function is called the maximum likelihood
estimate.

According to {[}2{]}, a point \(\hat{\boldsymbol{\theta}}\) in
\(\boldsymbol{\Omega}\) at which \(L(\boldsymbol{\theta})\) is a maximum
is called the \emph{maximum likelhood estimate} (MLE) of
\(\boldsymbol{\theta}\). That is, \(\hat{\boldsymbol{\theta}}\) is a
value of \(\boldsymbol{\theta}\) that satisfies \begin{equation}
\label{eq:mle}
L(\hat{\boldsymbol{\theta}}) = \max_{\boldsymbol{\theta }\in \boldsymbol{\Omega}} L(\boldsymbol{\theta}).
\end{equation} Essentially, the MLE is a point in the parameter space
that is a maximum of the likelihood of the observed data, \[
\hat{\boldsymbol{\theta}} \in \arg\max_{\boldsymbol{\theta }\in \boldsymbol{\Omega}} L(\boldsymbol{\theta}).
\]

Any point that maximizes the likelihood function also maximizes the
log-likelihood function. Thus, for both computational and analytical
reasons, we work with the log-likelihood function.

\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\boldsymbol{\theta})$, is given by
the log of the likelihood function for our masked data model,
\begin{equation}
\label{eq:loglike}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\theta})
\end{equation}
where
\begin{equation}
\ell_i(\boldsymbol{\theta}) = 
\begin{cases}
\sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) &\text{ if } \delta_i = 0\\
\sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) +
    \log \bigl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \bigr) &\text{ if } \delta_i = 1.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is just the logarithm of the likelihood function,
$$
\ell(\boldsymbol{\theta}) = \log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n L_i(\boldsymbol{\theta}).
$$
Since $\log(A \cdot B) = \log(A) + \log(B)$, we may rewrite the log-likelihood
function as
$$
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \log L_i(\boldsymbol{\theta}).
$$
By Equation \eqref{eq:like}, $L_i$ is given by
$$
L_i(\boldsymbol{\theta}) =
\begin{cases}
R_{T_i}(s_i;\boldsymbol{\theta}) &\text{ if } \delta_i = 0,\\
R_{T_i}(s_i;\boldsymbol{\theta})
    \beta_i \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) &\text{ if } \delta_i = 1.
\end{cases}
$$

We now consider these two cases separately, then combine them to obtain the
result in Theorem \ref{thm:loglike_total}.

\textbf{Case 1}: If the $i$-th system is right-censored, i.e., $\delta_i = 0$, then
$$
\ell_i(\boldsymbol{\theta}) = \log R_{T_i}(s_i;\boldsymbol{\theta}) = \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the
failure time is known, i.e., $\delta_i = 1$, then
\begin{align*}
\ell_i(\boldsymbol{\theta})
    &= \log R_{T_i}(s_i;\boldsymbol{\theta}) \beta_i \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})\\
    &= \log R_{T_i}(s_i;\boldsymbol{\theta}) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})\bigr).
\end{align*}

Since $\beta_i$ is not a function of $\boldsymbol{\theta}$ (see Condition \ref{cond:masked_indept_theta}),
$\log \beta_i$ is a constant with respect to $\boldsymbol{\theta}$ and so may be ignored in MLE
(when we solve the maximum likelihood equations, any terms that do not depend on
$\boldsymbol{\theta}$ will be eliminated by the gradient operator).
Thus, we may rewrite the above equation as
$$
\ell_i(\boldsymbol{\theta}) = \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) +
        \log \biggl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \biggr).
$$
\end{proof}

\hypertarget{sec:iterative}{%
\subsection{Solving the maximum likelihood
equations}\label{sec:iterative}}

According to {[}2{]}, if \(\boldsymbol{\Omega}\) is a Cartesian product
of \(l\) intervals, partial derivatives of \(L(\boldsymbol{\theta})\)
exist, and the MLEs do not occur on the boundary of
\(\boldsymbol{\Omega}\), then the MLEs will be solutions of the
simultaneous equations \begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\boldsymbol{\theta}) = 0
\end{equation} for \(j=1,\ldots,p\) where \(p\) is the number of
components in \(\boldsymbol{\theta}\). We call these equations the
\emph{maximum likelihood equations}. If multiple solutions to Equation
\eqref{eq:mle_eq} exist, each solution that maximizes the likelihood
function is a valid MLE.

If there is no closed-form solution to the maximum likelihood equations
\eqref{eq:mle_eq}, we may use iterative root-finding methods to
numerically approximate a solution. A general approach is, if we have a
guess \(\boldsymbol{\theta}^{(n)}\), take a step in a ``promising''
direction \(\boldsymbol{d}^{(n)}\) to obtain the next guess,
\begin{equation}
\label{eq:iterative_update}
\boldsymbol{\theta}^{(n+1)} = \boldsymbol{\theta}^{(n)} + \alpha^{(n)} \boldsymbol{d}^{(n)},
\end{equation} where \(\boldsymbol{\theta}^{(0)}\) is an initial guess
in the parameter space \(\Omega\) that is sufficiently close to the MLE
\(\hat{\boldsymbol{\theta}}\). In our case, we use the parameter vector
\(\boldsymbol{\theta}^{(0)} = \boldsymbol{\theta}\) as our initial
guess, since we know the true value \(\boldsymbol{\theta}\) in our
simulation studies, but if plausible initial guesses are not known, then
global methods may be used to find a good initial guess, like Simulated
Annealing.

Assuming that at \(\boldsymbol{\theta}^{(n)}\), a sufficiently small
step in the direction \(\boldsymbol{d}^{(n)}\) results in an improvement
with respect to the log-likelihood function, we say that we
\emph{overshoot} if \[
\ell(\boldsymbol{\theta}^{(n+1)}) < \ell(\boldsymbol{\theta}^{(n)}).
\] The value \(\alpha^{(n)}\) in Equation \eqref{eq:iterative_update} is
a positive real number chosen by a \emph{line search} method so that we
do not overshot, which has an optimal value given by \[
\alpha^{(n)} \in \operatorname{argmax}_{\alpha}
    \ell(\boldsymbol{\theta}^{(n)} + \alpha \boldsymbol{d}^{(n)}).
\] However, this may be too computationally expensive to compute, and so
we use a less optimal but faster method known as \emph{backtracking}. In
the backtracking line search method, we determine \(\alpha^{(n)}\) by
initially letting \(\alpha^{(n)} = 1\) and then, if we overshoot, redo
the update with \(\alpha^{(n)} \gets r \alpha^{(n)}\), \(0 < r < 1\),
repeating until we do not overshoot.

Note that this is not necessarily the best course of action for finding
global maximums, since depending on our initial guess
\(\boldsymbol{\theta}^{(0)}\), we may get stuck in a local maximum.
There are many other methods that are more likely to find a global
maximum, such as sometimes moving in a direction that decreases the
likelihood of the guess. However, for our simulations, we found that we
were always able to find a global maximum using the iterative method
described in Equation \eqref{eq:iterative_update}.

We do as many iterations in Equation \eqref{eq:iterative_update} as
necessary to satisfy some \emph{stopping condition}, which is usually
something simple like the distance between \(\boldsymbol{\theta}^{(n)}\)
and \(\boldsymbol{\theta}^{(n+1)}\) being sufficiently small. Under the
right conditions, for sufficiently large \(n\),
\(\boldsymbol{\theta}^{(n)} \approx \hat{\boldsymbol{\theta}}\).

Two popular iterative techniques are Newton-Raphson method and gradient
ascent, which respectively are defined by letting \[
\boldsymbol{d}^{(n)}_{\text{Newton-Raphson}} = J^{-1}(\boldsymbol{\theta}^{(n)}) \nabla \ell(\boldsymbol{\theta}^{(n)})
\] and \[
\boldsymbol{d}^{(n)}_{\text{gradient ascent}} = \nabla \ell(\boldsymbol{\theta}^{(n)}),
\] where \(J(\boldsymbol{\theta}^{(n)})\) and
\(\nabla \ell(\boldsymbol{\theta}^{(n)})\) are respectively the observed
Fisher information matrix (Hessian of the log-likelihood function) and
the score (gradient of the log-likelihood function), each evaluated at
\(\boldsymbol{\theta}^{(n)}\).

Depending upon the nature of the log-likelihood function, one or the
other may work better in
practice.\footnote{For example, one approach may be more
computationally efficient.} The R code for iterative methods is in
Appendix C.

\hypertarget{properties-of-the-mle}{%
\subsection{Properties of the MLE}\label{properties-of-the-mle}}

According to {[}2{]}, if certain regularity conditions are satisfied,
then solutions of the maximum likelihood equation \eqref{eq:mle_eq} have
the following desirable properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\boldsymbol{\hat\theta}\) exists and is unique.
\item
  \(\boldsymbol{\hat\theta}\) is an asymptotically unbiased estimator.
\item
  \(\boldsymbol{\hat\theta}\) is asymptotically the UMVUE, the uniform
  minimum variance unbiased estimator.
\item
  \(\boldsymbol{\hat\theta}\) is asymptotically normal with a mean
  \(\boldsymbol{\theta}\) and a variance-covariance that is the inverse
  of the Fisher information matrix, whose \((i,j)\)-th component is
  given by \[
   I(\boldsymbol{\theta})_{i j} = n E_{\boldsymbol{\theta}}\biggl(-\frac{\partial^2}{\partial \theta_i \partial \theta_j}
       \log f(S_i,\delta_i,\mathcal{C}_i;\boldsymbol{\theta})\biggr).
  \]
\end{enumerate}

Since it is frequently problematic taking the expectation to derive
\(I(\theta)\), we instead use the \emph{observed} information matrix,
which is conditioned on the observed data, \[
J(\boldsymbol{\theta})_{i j} = -\frac{\partial}{\partial \theta_i \partial \theta_j}
    \ell(\boldsymbol{\theta}).
\]

Since we do not know \(\boldsymbol{\theta}\), we estimate
\(J(\boldsymbol{\theta})\) with \(J(\boldsymbol{\hat\theta})\). Thus,
assuming the regularity conditions are satisfied, then approximately, \[
    \hat{\boldsymbol{\theta}} \sim \mathcal{N}(\hat{\boldsymbol{\theta}},J^{-1}(\hat{\boldsymbol{\theta}}))
\] and as the sample size goes to infinity,
\(\hat{\boldsymbol{\theta}}\) converges in distribution to
\(\mathcal{N}(\hat{\boldsymbol{\theta}},J^{-1}(\hat{\boldsymbol{\theta}}))\).

\hypertarget{sec:weibull}{%
\section{Components with Weibull distributed
lifetimes}\label{sec:weibull}}

Consider a series system in which the components have Weibull
distributed lifetimes. The \(j\)\textsuperscript{th} component of the
\(i\)\textsuperscript{th} has a lifetime distribution given by \[
    T_{i j} \sim \operatorname{WEI}(\boldsymbol{\theta_j})
\] where \(\boldsymbol{\theta_j} = (k_j, \lambda_j)\) for
\(j=1,\ldots,m\). Thus,
\(\boldsymbol{\theta }= (\boldsymbol{\theta_1},\ldots,\boldsymbol{\theta_m})' = \bigl(k_1,\lambda_1,\ldots,k_m,\lambda_m\bigr)\).
The random variable \(T_{i j}\) has a reliability function, pdf, and
hazard function given respectively by \begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align} where \(t > 0\), \(\lambda_j > 0\) is the scale parameter
and \(k_j > 0\) is the shape parameter.

\begin{figure}

{\centering \includegraphics{wei_series_md_files/figure-latex/exp_weib_haz-1} 

}

\caption{Component lifetime plots}\label{fig:exp_weib_haz}
\end{figure}

The shape parameter \(k\) may be understood in the following way:

\begin{itemize}
\tightlist
\item
  If \(k < 1\), then the hazard function decreases with respect to
  system lifetime, which may occur if defective items fail early and are
  weeded out.
\item
  If \(k > 1\), then the hazard function is increases with respect to
  time, which may occur as a result of an aging process.
\item
  If \(k = 1\), then the failure rate is constant, which means it is
  exponentially distributed.
\end{itemize}

See Figure \ref{fig:exp_weib_haz} for plots of the hazard and pdf
functions of five different Weibull distributed components. We will use
these plots to illustrate the different shapes of the hazard and pdf
functions. The first component has a shape parameter \(k=0.25\), which
is less than 1, and so the hazard function decreases with respect to
time. The second component has a shape parameter \(k=1\), and so the
hazard function is constant. The third, fourth, and fifth components
have shape parameters \(k=2.5\), \(k=3\), and \(k=5\), respectively, and
so the hazard functions increase with respect to time.

The lifetime of the series system composed of \(m\) Weibull components
has a reliability function given by \begin{equation}
\label{eq:sys_weibull_reliability_function}
R(t;\boldsymbol{\theta}) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}

\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
R(t;\boldsymbol{\theta}) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R(t;\boldsymbol{\theta})
    &= \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}\\
    &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
\end{proof}

The Weibull series system's hazard function is given by \begin{equation}
\label{eq:sys_weibull_failure_rate_function}
h(t;\boldsymbol{\theta}) =
    \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{equation} whose proof follows from Theorem
\ref{thm:sys_failure_rate}.

\begin{figure}

{\centering \includegraphics{wei_series_md_files/figure-latex/series_haz_pdf-1} 

}

\caption{System lifetime plots}\label{fig:series_haz_pdf}
\end{figure}

In Figure \ref{fig:series_haz_pdf}, we plot the hazard function and the
pdf of the Weibull series system with the component lifetime parameters
considered earlier, \begin{align*}
T_{i 1} &\sim \operatorname{WEI}(3,30)\\
T_{i 2} &\sim \operatorname{WEI}(2,50)\\
T_{i 3} &\sim \operatorname{WEI}(0.5,15)\\
T_{i 4} &\sim \operatorname{WEI}(5,20)\\
T_{i 5} &\sim \operatorname{WEI}(0.25,50)\\
\end{align*} for the \(i\)-th series system where \(i=1,\ldots,n\). By
Theorem \ref{thm:sys_lifetime}, the series system has a random lifetime
given by \[
T_i = \operatorname{min}\{T_{i 1},\ldots,T_{i 5}\}.
\] where \(\boldsymbol{\theta }= (k_1,\lambda_1,\ldots,k_5,\lambda_5)\).

The series system, due to being a mixture of Weibull components with
different shapes, has both a high infant mortality rate and an aging
process, which is reflected in the plot of the hazard function. The
hazard is initially high then decreases to some minimum before
increasing again. This is a pattern we see in nature, e.g., electronic
appliances may fail early due to defects, but those that survive the
initial period of high failure rate can be expected to last for a long
time before finally wearing out due to an aging process.

The pdf of the series system also appears to be multimodal, where the
modes correspond to the high infant mortality rate and the aging
process.

The pdf of the series system is given by \begin{equation}
\label{eq:sys_weibull_pdf}
f(t;\boldsymbol{\theta}) =
\biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{equation}

\begin{proof}
By definition,
$$
f(t;\boldsymbol{\theta}) = h(t;\boldsymbol{\theta}) R(t;\boldsymbol{\theta}).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}

The conditional pmf of \(K_i\) given \(T_i\) is given by
\begin{equation}
\label{eq:weibull_pmf_k_given_s}
f(k;\boldsymbol{\theta}|t) = \frac{\frac{k_k}{\lambda_k}\bigl(\frac{t}{\lambda_k}\bigr)^{k_k-1}}
    {\sum_{j=1}^m \frac{k_j}{\lambda_j}\bigl(\frac{t}{\lambda_j}\bigr)^{k_j-1}}.
\end{equation}

\begin{proof}
By Theorem \ref{thm:f_given_s_form_2},
$$
f(k;\boldsymbol{\theta}|t) = \frac{h_k(t;\boldsymbol{\theta_k})}{\sum_{j=1}^m h_j(t;\boldsymbol{\theta_j})}
$$
where $h_1,\ldots,h_m$ are the failure rate functions of the $m$ Weibull
component lifetimes.
\end{proof}

The joint pdf of \(K_i\) and \(T_i\) is given by \begin{equation}
\label{eq:weibull_joint_k_s}
f(k,t;\boldsymbol{\theta}) = \frac{k_k}{\lambda_j}\biggl(\frac{t}{\lambda_k}\biggr)^{k_k-1}
\exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}

\begin{proof}
Plugging in the conditional probability and the marginal probability given
respectively by Equations \eqref{eq:expo_prob_K_given_S} and
\eqref{eq:expo_sys_pdf} completes the proof.

By Theorem \ref{thm:f_k_and_t}, the joint pdf of $K_i$ and $T_i$ is given by
$$
f(k,t;\boldsymbol{\theta}) = h_k(t;\boldsymbol{\theta_k}) R_{T_i}(t;\boldsymbol{\theta})
$$
where $R_{T_i}(t;\boldsymbol{\theta})$ is given by Equation
\eqref{eq:sys_weibull_reliability_function} and $h_k$ is the failure rate
function of the $k$\textsuperscript{th} Weibull component.
\end{proof}

\hypertarget{weibull-likelihood-model-for-masked-data}{%
\subsection{Weibull Likelihood Model for Masked
Data}\label{weibull-likelihood-model-for-masked-data}}

In Section \ref{sec:like_model}, we discussed two separate kinds of
likelihood contributions, masked component cause of failure data (with
exact system failure times) and right-censored data. The likelihood
contribution of the \(i\)\textsuperscript{th} system is given by the
following theorem.

\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\boldsymbol{\theta}) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\boldsymbol{\theta}) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\boldsymbol{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the
following result.

\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\ell_i(\boldsymbol{\theta}) =
\begin{cases}
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} + \log \biggl\{    
    \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
\biggr\} & \text{if } \delta_i = 1,\\
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} & \text{if } \delta_i = 0,
\end{cases}
\end{equation}
where we drop any terms that do not depend on $\boldsymbol{\theta}$ since they do not
affect the MLE.
\end{corollary}

Since the systems are independent, the log-likelihood of the entire
sample of \(n\) observations is given by \begin{equation}
\label{eq:weibull_log_likelihood}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\theta}).
\end{equation}

\hypertarget{maximum-likelihood-estimation}{%
\subsection{Maximum likelihood
estimation}\label{maximum-likelihood-estimation}}

We may find an MLE by solving the maximum likelihood equation
\eqref{eq:mle_eq}, i.e., a point
\(\boldsymbol{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)\)
satisfying
\(\nabla_{\theta} \ell(\boldsymbol{\hat\theta}) = \boldsymbol{0}\),
where \(\nabla_{\boldsymbol{\theta}}\) is the gradient of the
log-likelihood function with respect to \(\boldsymbol{\theta}\),
otherwise known as the score.

To solve these ML equations, we use the Newton-Raphson method described
in Section \ref{sec:iterative}. In order to use the Newton-Raphson
method, we need to compute the gradient and Hessian of the
log-likelihood function.

We analytically derive the gradient of the log-likelihood function
(score), since it is useful to have for the Newton-Raphson method, but
we do not do the same for the Hessian of the log-likelihood for the
following reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The Hessian is not necessarily needed since we often use some faster
  method to approximate the Hessian, e.g., the BFGS method. Technically,
  we could also numerically approximate the gradient too, but the
  gradient is much easier to derive than the Hessian, and moreover,
  knowing the score precisely also enables us to more accurately
  approximate the Hessian by taking the Jacobian of the gradient.
\item
  The Hessian is more difficult to derive than the score, and so it is
  more likely that we will make a mistake when deriving the Hessian.
\end{enumerate}

The following theorem derives the score function.

\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\boldsymbol{\theta}) = \biggl(
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_1},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_m},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_r} = 
\begin{cases}
    -\bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}       
        \log\bigl(\frac{t_i}{\lambda_r}\bigr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
            & \text{if } \delta_i = 1 \text{ and } r \in c_i,\\
    -\bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}\log\bigl(\frac{t_i}{\lambda_r}\bigr)
    & \text{if } \delta_i = 0 \text{ or } r \notin c_i\\
\end{cases}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_r} = 
\begin{cases}
    \frac{k_r}{\lambda_r} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r} -
    \frac{k_r^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r-1}}
        {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
    & \text{if } \delta_i = 1 \text{ and } r \in c_i,\\
    \frac{k_r}{\lambda_r} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
    & \text{if } \delta_i = 0 \text{ or } r \notin c_i.
\end{cases}
\end{equation}
\end{theorem}

These results follow from taking the partial derivatives of the
log-likelihood contribution of the \(i\)-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. The proof is omitted for
brevity, but the results have been verified by using a very precise
numerical approximation of the gradient and verifying equality with the
analytical gradient on different data sets and parameter values.

By the linearity of differentiation, the gradient of a sum of functions
is the same of their gradients, and so the score function of the entire
sample is given by \begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\boldsymbol{\theta}) = \sum_{i=1}^n \nabla \ell_i(\boldsymbol{\theta}).
\end{equation}

\hypertarget{simulation-study-weibull-series-system}{%
\section{Simulation study: Weibull series
system}\label{simulation-study-weibull-series-system}}

In the real world, systems are quite complex:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  They are not perfect series systems.
\item
  The components in a system are not independent.
\item
  The lifetimes of the systems (in the population) are not precisely
  modeled by any known probability distributions.
\item
  The components may depend on many other unobserved factors.
\end{enumerate}

With these caveats in mind, we model the data as coming from a Weibull
series system of \(m = 5\) components, and other factors, like ambient
temperature, are either negligible (on the distribution of component
lifetimes) or are more or less constant. Then, our task is to use our
likelihood model to find the best fit using maximum likelihood
estimation.

\hypertarget{bernoulli-candidate-set-model}{%
\subsection{Bernoulli candidate set
model}\label{bernoulli-candidate-set-model}}

In our simulation study, we must generate data that satisfies the
masking conditions. In other words, we must generate data that satisfies
the following conditions described in Section \ref{sec:candmod}.

There are many ways to satisfying the masking conditions. We choose the
simplest method, which we call the \emph{Bernoulli candidate set model}.
In this model, each non-failed component is included in the candidate
set with a fixed probability \(p\), independently of all other
components and independently of \(\boldsymbol{\theta}\), and the failed
component is always included in the candidate set.

\hypertarget{identifiability}{%
\subsection{Identifiability}\label{identifiability}}

When estimating the parameters of latent components, we must be careful
to ensure that the parameters are identifiable. In other words, we must
ensure that the likelihood function is maximized at a unique point. If
the likelihood function is not maximized at a unique point, then the MLE
is not unique, and a lot of the theory we have developed so far breaks
down.

One way in which this problem may arise is if the data is not
informative enough. For example, if we have a series system with \(m\)
components, and in the sample component \(1\) is in the candidate set if
and only if component \(2\) is in candidate set, then we do not have
enough information to estimate the parameters of component \(1\) and
component \(2\) separately. In this case, we could combine these two
components into one component, and then we would have \(m-1\) components
to estimate instead. We lagely avoid this problem by using the Bernoulli
candidate set model, but sometimes it may still arise by chance.

\hypertarget{sec:opt_rescale}{%
\subsection{Optimization issue: parameter
rescaling}\label{sec:opt_rescale}}

When the parameters under investigation span different orders of
magnitude, parameter rescaling can significantly improve the performance
and reliability of optimization algorithms.

Parameter rescaling gives an optimizer a sense of the typical size of
each parameter, enabling it to adjust its steps accordingly. This is
crucial in scenarios like ours, where shape and scale parametes are a
few orders of magnitude apart. Without rescaling, the optimization
routine may struggle, taking numerous small steps for larger parameters
and overshooting for smaller ones.

In the \texttt{optim} algorithm in the R package \texttt{stats}, we
achieve this result by assigning a \texttt{parscale} vector in line with
the parameter magnitudes. It does not matter what the values of the
\texttt{parscale} vector are, only their relative magnitudes. It does
not need to be very precise, but since we are doing a simulation study,
we know the true parameter values and can use that information to scale
them appropriately. We found that this allowed for convergence to MLEs
more quickly and reliably.

\hypertarget{simulation-study-design}{%
\subsection{Simulation study design}\label{simulation-study-design}}

Here is an outline of the simulation study analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Set up simulation parameters:

  \begin{itemize}
  \item
    The true parameter value \(\boldsymbol{\theta}\) for the Weibull
    series system (5 components) has shape parameters given by \[
      k_1 = 1.2576,
      k_2 = 1.1635,
      k_3 = 1.1308,
      k_4 = 1.1802,
      k_5 = 1.3311
     \] and scale parameters given by \[
      \lambda_1 = 994.3661,
      \lambda_2 = 908.9458,
      \lambda_3 = 840.1141,
      \lambda_4 = 940.1141,
      \lambda_5 = 836.1123.
     \]
  \item
    We generate MLEs for a range of sample sizes from \(n = 10\) to
    \(n = 200\).
  \item
    We generate MLEs for a range of probabilities from \(p = 0\) (no
    masking) to \(p = 0.3\) (significant masking) for including each
    non-failed component in the candidate set using the Bernoulli
    candidate set model.
  \item
    We use only two separate right-censoring times, \(\tau\) set to
    \(\infty\) (no right censoring) to and \(\tau\) set to the
    \(0.75\)-quantile of the Weibull series system.
  \end{itemize}
\item
  For each scenario (combination of \(n\), \(p\), and \(\tau\)), we
  generate \(R = 200\) data sets.
\item
  We estimate the parameters for each replication, giving us \(R\)
  estimates of the parameters for each scenario. We use these replicates
  as an empirical estimate of the sampling distribution of the MLE for
  each scenario.
\item
  Using the empirical sampling distributions, we estimate the bias,
  variance, MSE, and coverage probability of the MLE for each scenario.
\item
  We analyze and visualize the results, e.g., by plotting the bias,
  variance, MSE, and coverage probability as a function of \(n\) and
  \(p\) for each \(\tau\).

  We then interpret the results and discuss the performance of the MLE
  estimator under various conditions. We expect that as
  \(n \to \infty\), the bias and MSE will go to \(0\) and the coverage
  probability will go to \(0.95\) (when constructing \(95\%\) confidence
  intervals). Of course, we do not expect these results to hold for
  finite \(n\), but we would like to see how the bias, MSE, and coverage
  probability change as a function of \(n\) and \(p\) for each \(\tau\).
\end{enumerate}

\hypertarget{sec:acc_prec}{%
\subsection{Estimates of bias, variance, and coverage
probability}\label{sec:acc_prec}}

A measure of the accuracy of \(\boldsymbol{\hat\theta}\) is the bias,
which is defined as \[
\operatorname{b}(\boldsymbol{\hat\theta}) = E(\boldsymbol{\hat\theta}) - \boldsymbol{\theta}.
\] We cannot analytically derive the bias, so we estimate the bias using
the empirical sampling distribution, \[
\hat{\operatorname{b}}(\boldsymbol{\hat\theta}) =
    E_{\hat{\boldsymbol{\theta}} \sim \text{data}}(\boldsymbol{\hat\theta}) - \boldsymbol{\theta}.
\]

We estimate the precision of \(\boldsymbol{\hat\theta}\) with the
variance-covariance matrix, \[
\operatorname{Var}(\boldsymbol{\hat\theta}) =
    E\bigl(
        (\hat{\boldsymbol{\theta}} - E(\hat{\boldsymbol{\theta}}))
        (\hat{\boldsymbol{\theta}} - E(\hat{\boldsymbol{\theta}}))'
    \bigr),
\] which is a \(p\)-by-\(p\) matrix, where \(p\) is the number of
parameters, and again the the expectation is taken with respect to the
empirical sampling distribution. We will principally use the diagonal
elements of the variance-covariance matrix, which are the variances of
the individual components of \(\boldsymbol{\hat\theta}\).

We will use the inverse of the observed Fisher information matrix (FIM),
which was defined in Section \ref{}, to construct \(95\%\)-confidence
intervals for \(\boldsymbol{\theta}\) to compute the coverage
probability of the MLE for each scenario. A confidence interval is said
to be \emph{well-calibrated} if the coverage probability is close to the
nominal level, \(95\%\).

The mean squared error, \(\operatorname{MSE}\), is a measure of
estimator error that incorporates both the bias and the variance, and is
defined as \[
\operatorname{MSE}(\boldsymbol{\hat\theta}) =
    E\bigl((\boldsymbol{\hat\theta} - \boldsymbol{\theta})(\boldsymbol{\hat\theta} - \boldsymbol{\theta})'\bigr) =
    \operatorname{Var}(\boldsymbol{\hat\theta}) +
    \operatorname{b}(\boldsymbol{\hat\theta})\operatorname{b}(\boldsymbol{\hat\theta})',
\] which is a \(p\)-by-\(p\) matrix, where \(p\) is the number of
parameters, and again the expectation is with respect to the empirical
sampling distribution. The diagonal elements of the MSE matrix are the
mean squared errors of the individual components of
\(\boldsymbol{\hat\theta}\). Asymptotically, the MSE is equal to the
variance, but for finite \(n\), the MSE is a more informative measure of
estimator error than the variance alone.

\hypertarget{sec:boot}{%
\subsection{Bootstrapping the sampling distribution of the
MLE}\label{sec:boot}}

In a real-world scenario, we would not know how to generate the data
from the underlying data generating process, and so we would not be able
to compute the empirical sampling distribution of the MLE.

However, we can use the bootstrap method. The bootstrap method is a
general method for estimating the sampling distribution of a statistic,
in our case the MLE. The most common form of the bootstrap is the
non-parametric bootstrap. In the non-parametric bootstrap, the random
data is created by resampling with replacement from the original data.
Since we do not know (nor attempt to model) the distribution of
candidate sets, this non-parametric form is ideal, since we can simply
resample from the original data to approximate its empirical sampling
distribution of the MLE.

Since the asymptotic distribution of the MLE may not be achieved for
finite \(n\), the bootstrap method is a useful tool for estimating the
sampling distribution of the MLE for finite \(n\), in particular, for
estimating the confidence intervals of the MLE.

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-Fran-1991}{}%
{[}1{]} \textsc{Guess}, F. M., \textsc{Hodgson}, T. J. and
\textsc{Usher}, J. S. (1991). Estimating system and component
reliabilities under partial information on cause of failure.
\emph{Journal of Statistical Planning and Inference} \textbf{29} 75--85.

\leavevmode\hypertarget{ref-bain}{}%
{[}2{]} \textsc{Engelhardt}, B. (1992). Introduction to probability and
mathematical statistics. 294.
\end{cslreferences}

\end{document}
