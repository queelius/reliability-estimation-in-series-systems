% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Bootstrapping statistics of the maximum likelihood estimator of components in a series systems from masked failure data},
  pdfauthor={Alex Towell},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{tikz}
\usepackage[]{natbib}
\bibliographystyle{apalike}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {}%
  {\par}

\title{Bootstrapping statistics of the maximum likelihood estimator of
components in a series systems from masked failure data}
\author{Alex Towell}
\date{}

\begin{document}
\maketitle
\begin{abstract}
We estimate the parameters of a series system with Weibull component
lifetimes from relatively small samples consisting of right-censored
system lifetimes and masked component cause of failure. Under a set of
conditions that permit us to ignore how the component cause of failures
are masked, we assess the bias and variance of the estimator. Then, we
assess the accuracy of the boostrapped variance and calibration of the
confidence intervals of the MLE under a variety of scenarios.
\end{abstract}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newcommand{\T}{T}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\renewcommand{\v}[1]{\boldsymbol{#1}}
\numberwithin{equation}{section}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Accurately estimating the reliability of individual components in
multi-component systems is an important problem in many engineering
domains. However, component lifetimes and failure causes are often not
directly observable. In a series system, only the system-level failure
time may be recorded along with limited information about which
component failed. Such \emph{masked} data poses challenges for
estimating component reliability.

In this paper, we develop a maximum likelihood approach to estimate
component reliability in series systems using right-censored lifetime
data and candidate sets that contain the failed component. The key
contributions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Deriving a likelihood model that accounts for right-censoring and
  masked failure causes through candidate sets. This allows the
  available masked data to be used for estimation.
\item
  Validating the accuracy, precision, and robustness of the maximum
  likelihood estimator through an extensive simulation study under
  different sample sizes, masking probabilities, and censoring levels.
\item
  Demonstrating that bootstrapping provides well-calibrated confidence
  intervals for the MLEs even with small samples.
\end{enumerate}

Together, these contributions provide a statistically rigorous
methodology for learning about latent component properties from series
system data. The methods are shown to work well even when failure
information is significantly masked. This capability expands the range
of applications where component reliability can be quantified from
limited observations.

The remainder of this paper is organized as follows. First, we detail
the series system and masked data models. Next, we present the
likelihood construction and maximum likelihood theory. We then describe
the bootstrap approach for variance and confidence interval estimation.
Finally, we validate the methods through simulation studies under
various data scenarios and sample sizes.

\hypertarget{sec:statmod}{%
\section{Series System Model}\label{sec:statmod}}

Consider a system composed of \(m\) components arranged in a series
configuration. Each component and system has two possible states,
functioning or failed. We have \(n\) systems whose lifetimes are
independent and identically distributed (i.i.d.). The lifetime of the
\(i\)\textsuperscript{th} system denoted by the random variable
\(T_{i}\). The lifetime of the \(j\)\textsuperscript{th} component in
the \(i\)\textsuperscript{th} system is denoted by the random variable
\(T_{i j}\). We assume the component lifetimes in a single system are
statistically independent and non-identically distributed. Here,
lifetime is defined as the elapsed time from when the new, functioning
component (or system) is put into operation until it fails for the first
time. A series system fails when any component fails, thus the lifetime
of the \(i\)\textsuperscript{th} system is given by the component with
the shortest lifetime, \[
    T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
\]

There are three particularly important distribution functions in
survival analysis: the survival function, the probability density
function, and the hazard function. The survival function,
\(R_{T_i}(t)\), is the probability that the \(i\)\textsuperscript{th}
system has a lifespan larger than a duration \(t\), \begin{equation}
R_{T_i}(t) = \Pr\{T_i > t\}\\
\end{equation} The probability density function (pdf) of \(T_i\) is
denoted by \(f_{T_i}(t)\) and may be defined as \[
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
\] Next, we introduce the hazard function. The probability that a
failure occurs between \(t\) and \(\Delta t\) given that no failure
occurs before time \(t\) is given by \[
\Pr\{T_i \leq t+\Delta t|T_i > t\} = \frac{\Pr\{t < T_i < t+\Delta t\}}{\Pr\{T_i > t\}}.
\] The failure rate is given by the dividing this equation by the length
of the time interval, \(\Delta t\): \[
\frac{\Pr\{t < T < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T > t\}} =
    \frac{R_T(t) - R(t+\Delta t)}{R_T(t)}.
\] The hazard function \(h_{T_i}(t)\) for \(T_i\) is the instantaneous
failure rate at time \(t\), which is given by \begin{equation}
\label{eq:failure_rate}
\begin{split}
h_{T_i}(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}}\\
       &= \frac{f_{T_i}(t)}{R_{T_i}(t)}.
\end{split}
\end{equation} \textbackslash end\{definition\}

The lifetime of the \(j\)\textsuperscript{th} component is assumed to
follow a parametric distribution indexed by a parameter vector
\(\boldsymbol{\theta_j}\). The parameter vector of the overall system is
defined as \[
    \boldsymbol{\theta }= (\boldsymbol{\theta_1},\ldots,\boldsymbol{\theta_m}).
\]

When a random variable \(T\) is parameterized by a particular
\(\boldsymbol{\theta}\), we denote the reliability function by
\(R_T(t;\boldsymbol{\theta})\), and the same for other distribution
functions. If it is clear from the context which random variable a
distribution function is for, we drop the subscripts, e.g., \(R(t)\)
instead of \(R_T(t)\). As a special case, we denote the pdf of the
\(j\)\textsuperscript{th} component by \(f_j(t;\boldsymbol{\theta_j})\)
and its reliability function by \(R_j(t;\boldsymbol{\theta_j})\).

Two random variables \(X\) and \(Y\) have a joint pdf \(f_{X,Y}(x,y)\).
Given the joint pdf \(f(x,y)\), the marginal pdf of \(X\) is given by \[
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
\] where \(\mathcal{Y}\) is the support of \(Y\). (If \(Y\) is discrete,
replace the integration with a summation over \(\mathcal{Y}\).)

The conditional pdf of \(Y\) given \(X=x\), \(f_{Y|X}(y|x)\), is defined
as \[
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\] We may generalize all of the above to more than two random variables,
e.g., the joint pdf of \(X_1,\ldots,X_m\) is denoted by
\(f(x_1,\ldots,x_m)\).

Next, we dive deeper into these concepts and provide mathematical
derivations for the reliability function, pdf, and hazard function of
the series system. We begin with the reliability function of the series
system, as given by the following theorem.

\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_i > t\}
$$
which may be rewritten as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R(t;\boldsymbol{\theta}) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\boldsymbol{\theta}) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R(t;\boldsymbol{\theta}) = \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_reliability_function} shows that the system's
overall reliability is the product of the reliabilities of its
individual components. This property is inherent to series systems and
will be used in the subsequent derivations.

Next, we turn our attention to the pdf of the system lifetime, described
in the following theorem.

\begin{theorem}
\label{thm:sys_pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f(t;\boldsymbol{\theta}) = \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f(t;\boldsymbol{\theta}) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f(t;\boldsymbol{\theta})
    &= -\frac{d}{dt} R_1(t;\boldsymbol{\theta_1})\prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j})\\
    &= f_1(t;\boldsymbol{\theta}) \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}) -
      R_1(t;\boldsymbol{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\boldsymbol{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f(t;\boldsymbol{\theta}) = \sum_{j=1}^{m-1} f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\boldsymbol{\theta_j}) \frac{d}{dt} R_m(t;\boldsymbol{\theta_m}),
$$
which simplifies to
$$
f(t;\boldsymbol{\theta})= \sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k}).
$$
\end{proof}

Theorem \ref{thm:sys_pdf} shows the pdf of the system lifetime as a
function of the pdfs and reliabilities of its components.

We continue with the hazard function of the system lifetime, defined in
the next theorem.

\begin{theorem}
\label{thm:sys_failure_rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h(t;\boldsymbol{\theta}) = \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By Equation \eqref{eq:failure_rate}, the $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h(t;\boldsymbol{\theta}) = \frac{f_{T_i}(t;\boldsymbol{\theta})}{R_{T_i}(t;\boldsymbol{\theta})}.
$$
Plugging in expressions for these functions results in
$$
  h(t;\boldsymbol{\theta}) = \frac{\sum_{j=1}^m f_j(t;\boldsymbol{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\boldsymbol{\theta_k})}
      {\prod_{j=1}^m R_j(t;\boldsymbol{\theta_j})},
$$
which can be simplified to
$$
h_{T_i}(t;\boldsymbol{\theta}) = \sum_{j=1}^m \frac{f_j(t;\boldsymbol{\theta_j})}{R_j(t;\boldsymbol{\theta_j})} = \sum_{j=1}^m h_j(t;\boldsymbol{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_failure_rate} reveals that the system's hazard
function is the sum of the hazard functions of its components.

By definition, the hazard function is the ratio of the pdf to the
reliability function, \[
h(t;\boldsymbol{\theta}) = \frac{f(t;\boldsymbol{\theta})}{R(t;\boldsymbol{\theta})},
\] and we can rearrange this to get \begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f(t;\boldsymbol{\theta}) &= h(t;\boldsymbol{\theta}) R(t;\boldsymbol{\theta})\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\boldsymbol{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\boldsymbol{\theta_j}) \biggr\},
\end{split}
\end{equation} which we sometimes find to be a more convenient form than
Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's
reliability function, pdf, and hazard function. Next, we build upon
these concepts to derive distributions related to the component cause of
failure.

\hypertarget{sec:comp_cause}{%
\subsection{Component Cause of Failure}\label{sec:comp_cause}}

Whenever a series system fails, precisely one of the components is the
cause. We model the component cause of the series system failure as a
random variable.

\begin{definition}
The component cause of failure of a series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}

The system lifetime and the component cause of failure has a joint
distribution given by the following theorem.

\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\boldsymbol{\theta}) = h_j(t;\boldsymbol{\theta_j}) R_{T_i}(t;\boldsymbol{\theta}),
\end{equation}
where $h_j(t;\boldsymbol{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_{T_i}(t;\boldsymbol{\theta})$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a series system with $3$ components.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\boldsymbol{\theta}) = \prod_{j=1}^{3} f_j(t;\boldsymbol{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\boldsymbol{\theta}) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2}) f_3(t_3;\boldsymbol{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\boldsymbol{\theta_1}) f_2(t_2;\boldsymbol{\theta_2})
        R_3(t;\boldsymbol{\theta_3}) dt_2\\
     &= f_1(t;\boldsymbol{\theta_1}) R_2(t;\boldsymbol{\theta_2}) R_3(t_1;\boldsymbol{\theta_3}).
\end{align*}
Since $h_1(t;\boldsymbol{\theta_1}) = f_1(t;\boldsymbol{\theta_1}) / R_1(t;\boldsymbol{\theta_1})$,
$$
f_1(t;\boldsymbol{\theta_1}) = h_1(t;\boldsymbol{\theta_1}) R_1(t;\boldsymbol{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\boldsymbol{\theta})$
yields
\begin{align*}
f_{K_i,T_i}(j,t;\boldsymbol{\theta})
    &= h_1(t;\boldsymbol{\theta_1}) \prod_{l=1}^m R_l(t;\boldsymbol{\theta_l})\\
    &= h_1(t;\boldsymbol{\theta_1}) R(t;\boldsymbol{\theta}).
\end{align*}
Generalizing from this completes the proof.
\end{proof}

\hypertarget{sec:like_model}{%
\section{Likelihood Model for Masked Data}\label{sec:like_model}}

The object of interest is the (unknown) parameter value
\(\boldsymbol{\theta}\). To estimate this \(\boldsymbol{\theta}\), we
need \emph{data}. In our case, we call it \emph{masked data} because we
do not necessarily observe the event of interest, say a system failure,
directly. We consider two types of masking: masking the system failure
lifetime and masking the component cause of failure.

We generally encounter three types of system failure lifetime masking:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A system failure is observed at a particular point in time.
\item
  A system failure is observed to occur within a particular interval of
  time.
\item
  A system failure is not observed, but we know that the system survived
  at least until a particular point in time. This is known as
  \emph{right-censoring} and can occur if, for instance, an experiment
  is terminated while the system is still functioning.
\end{enumerate}

We generally encounter two types of component cause of failure masking:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The component cause of failure is observed.
\item
  The component cause of failure is not observed, but we know that the
  failed component is in some set of components. This is known as
  \emph{masking} the component cause of failure.
\end{enumerate}

Thus, the component cause of failure masking will take the form of
candidate sets. A candidate set consists of some subset of component
labels that plausibly contains the label of the failed component. The
sample space of candidate sets are all subsets of \(\{1,\ldots,m\}\),
thus there are \(2^m\) possible outcomes in the sample space.

In this paper, we limit our focus to observing \emph{right censored}
lifetimes and exact lifetimes but with masked component cause of
failures. We consider a sample of \(n\) i.i.d. series systems, each of
which is put into operation at some time and and observed until either
it fails or is right-censored. We denote the right-censoring time of the
\(i\)\textsuperscript{th} system by \(\tau_i\). We do not directly
observe the system lifetime, \(T_i\), but rather, we observe the
right-censored lifetime, \(S_i\), which is given by \begin{equation}
    S_i = \min\{\tau_i, T_i\},
\end{equation} We also observe a right-censoring indicator,
\(\delta_i\), which is given by \begin{equation}
    \delta_i = 1_{T_i < \tau_i}
\end{equation} where \(1_{\text{condition}}\) is an indicator function
that outputs \(1\) if \emph{condition} is true and \(0\) otherwise.
Here, \(\delta_i = 1\) indicates the event of interest, a system
failure, was observed.

If a system failure lifetime is observed, then we also observe a
candidate set that contains the component cause of failure. We denote
the candidate set for the \(i\)\textsuperscript{th} system by
\(\mathcal{C}_i\), which is a subset of \(\{1,\ldots,m\}\). Since the
data generating process for candidate sets may be subject to chance
variations, it as a random set.

Consider we have an independent and identically distributed (i.i.d.)
random sample of masked data, \(D = \{D_1, \ldots, D_n\}\), where each
\(D_i\) contanis the following:

\begin{itemize}
\tightlist
\item
  \(S_i\), the system lifetime of the \(i\)\textsuperscript{th} system.
\item
  \(\delta_i\), the right-censoring indicator of the
  \(i\)\textsuperscript{th} system.
\item
  \(\mathcal{C}_i\), the set of candidate component causes of failure
  for the \(i\)\textsuperscript{th} system.
\end{itemize}

The masked data generation process is illustrated by Figure
\ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$D_i = (S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data \(D\) for exact, right-censored system failure
times with candidate sets that mask the component cause of failure can
be seen in Table 1 for a series system with \(m=3\) components.

\begin{longtable}[]{@{}llll@{}}
\caption{Right-censored lifetime data with masked component cause of
failure.}\tabularnewline
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
System\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right-censoring time (\(S_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right censoring indicator (\(\delta_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Candidate set (\(\mathcal{C}_i\))\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
System\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right-censoring time (\(S_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Right censoring indicator (\(\delta_i\))\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Candidate set (\(\mathcal{C}_i\))\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(4.3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{1,2\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(1.3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{2\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
3\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(5.4\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\emptyset\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(2.6\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{2,3\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(3.7\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\{1,2,3\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
6\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(10\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\emptyset\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In our model, we assume the data is governed by a pdf, which is
determined by a specific parameter, represented as
\(\boldsymbol{\theta}\) within the parameter space
\(\boldsymbol{\Omega}\). The joint pdf of the data \(D\) can be
represented as follows: \[
f(D ; \boldsymbol{\theta}) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\boldsymbol{\theta}),
\] where \(s_i\) is the observed system lifetime of the
\(i\)\textsuperscript{th} system, \(\delta_i\) is the observed
right-censoring indicator of the \(i\)\textsuperscript{th} system, and
\(c_i\) is the observed candidate set of the \(i\)\textsuperscript{th}
system.

This joint pdf tells us how likely we are to observe the particular
data, \(D\), given the parameter \(\boldsymbol{\theta}\). When we keep
the data constant and allow the parameter \(\boldsymbol{\theta}\) to
vary, we obtain what is called the likelihood function \(L\), defined as
\[
L(\boldsymbol{\theta}) = \prod_{i=1}^n L_i(\boldsymbol{\theta})
\] where \[
L_i(\boldsymbol{\theta}) = f(s_i,\delta_i,c_i;\boldsymbol{\theta})
\] is the likelihood contribution of the \(i\)\textsuperscript{th}
system. In other words, the likelihood function quantifies how likely
different parameter values \(\boldsymbol{\theta}\) are, given the
observed data.

For each type of data, right-censored data and masked component cause of
failure data, we will derive the \emph{likelihood contribution} \(L_i\),
which refers to the part of the likelihood function that this particular
piece of data contributes to.

We present the following theorem for the likelihood contribution model.

\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1,
\end{cases}
\end{equation}
where $\delta_i = 0$ indicates the $i$\textsuperscript{th} system is
right-censored at time $s_i$ and $\delta_i = 1$ indicates the $i$\textsuperscript{th} system
is observed to have failed at time $s_i$ and the component cause of failure
is masked by the candidate set is $c_i$.
\end{theorem}

In the follow subsections, we prove this result for each type of masked
data, right-censored system lifetime data \((\delta_i = 0)\) and masking
of the component cause of failure \((\delta_i = 1)\).

\hypertarget{sec:candmod}{%
\subsection{Masked Component Cause of Failure}\label{sec:candmod}}

Suppose a diagnostician is unable to identify the precise component
cause of the failure, e.g., due to cost considerations he or she
replaced multiple components at once, successfully repairing the system
but failing to precisely identity the failed component. In this case,
the cause of failure is said to be \emph{masked}.

The unobserved component lifetimes may have many covariates, like
ambient operating temperature, but the only covariate we observe in our
masked data model are the system's lifetime and additional masked data
in the form of a candidate set that is somehow correlated with the
unobserved component lifetimes.

The key goal of our analysis is to estimate the parameters,
\(\boldsymbol{\theta}\), which maximize the likelihood of the observed
data, and to estimate the precision and accuracy of this estimate using
the Bootstrap method.

To achieve this, we first need to assess the joint distribution of the
system's continuous lifetime, \(T_i\), and the discrete candidate set,
\(\mathcal{C}_i\), which can be written as \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = f_{T_i}(t_i;\boldsymbol{\theta})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\},
\] where \(f_{T_i}(t_i;\boldsymbol{\theta})\) is the pdf of \(T_i\) and
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\) is
the conditional pmf of \(\mathcal{C}_i\) given \(T_i = t_i\).

We assume the pdf \(f_{T_i}(t_i;\boldsymbol{\theta})\) is known, but we
do not have knowledge of
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i\}\),
i.e., the data generating process for candidate sets is unknown.

However, it is critical that the masked data, \(\mathcal{C}_i\), is
correlated with the \(i\)\textsuperscript{th} system. This way, the
conditional distribution of \(\mathcal{C}_i\) given \(T_i = t_i\) may
provide information about \(\boldsymbol{\theta}\), despite our
Statistical interest being primarily in the series system rather than
the candidate sets.

To make this problem tractable, we assume a set of conditions that make
it unnecessary to estimate the generative processes for candidate sets.
The most important way in which \(\mathcal{C}_i\) is correlated with the
\(i\)\textsuperscript{th} system is given by assuming the following
condition.

\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\boldsymbol{\theta}}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}

Assuming Condition \ref{cond:c_contains_k}, \(\mathcal{C}_i\) must
contain the index of the failed component, but we can say little else
about what other component indices may appear in \(\mathcal{C}_i\).

In order to derive the joint distribution of \(\mathcal{C}_i\) and
\(T_i\) assuming Condition \ref{cond:c_contains_k}, we take the
following approach. We notice that \(\mathcal{C}_i\) and \(K_i\) are
statistically dependent. We denote the conditional pmf of
\(\mathcal{C}_i\) given \(T_i = t_i\) and \(K_i = j\) as \[
\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
\]

Even though \(K_i\) is not observable in our masked data model, we can
still consider the joint distribution of \(T_i\), \(K_i\), and
\(\mathcal{C}_i\). By Theorem \ref{thm:f_k_and_t}, the joint pdf of
\(T_i\) and \(K_i\) is given by \[
f_{T_i,K_i}(t_i,j;\boldsymbol{\theta}) = h_j(t_i;\boldsymbol{\theta_j}) R_{T_i}(t_i;\boldsymbol{\theta}),
\] where \(h_j(t_i;\boldsymbol{\theta_j})\) is the hazard function for
the \(j\)\textsuperscript{th} component and
\(R_{T_i}(t_i;\boldsymbol{\theta})\) is the reliability function of the
system. Thus, the joint pdf of \(T_i\), \(K_i\), and \(\mathcal{C}_i\)
may be written as \begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\boldsymbol{\theta})
    &= f_{T_i,K_i}(t_i,k;\boldsymbol{\theta}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\boldsymbol{\theta_j}) R_{T_i}(t_i;\boldsymbol{\theta})
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation} We are going to need the joint pdf of \(T_i\) and
\(\mathcal{C}_i\), which may be obtained by summing over the support
\(\{1,\ldots,m\}\) of \(K_i\) in Equation \eqref{eq:joint_pdf_t_k_c}, \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\] By Condition \ref{cond:c_contains_k},
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0\)
when \(K_i = j\) and \(j \notin c_i\), and so we may rewrite the joint
pdf of \(T_i\) and \(\mathcal{C}_i\) as \begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\boldsymbol{\theta_j}) \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of \(\boldsymbol{\theta}\) (see Section
\ref{sec:mle}), we solve the simultaneous equations of the MLE and
choose a solution \(\hat{\boldsymbol{\theta}}\) that is a maximum for
the likelihood function. When we do this, we find that
\(\hat{\boldsymbol{\theta}}\) depends on the unknown conditional pmf
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\).
So, we are motivated to seek out more conditions (that approximately
hold in realistic situations) whose MLEs are independent of the pmf
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\).

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}

According to \citep{Fran-1991}, in many industrial problems, masking
generally occurred due to time constraints and the expense of failure
analysis. In this setting, Condition \ref{cond:equal_prob_failure_cause}
generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\) may
be factored out of the summation in Equation \eqref{eq:part1}, and thus
the joint pdf of \(T_i\) and \(\mathcal{C}_i\) may be rewritten as \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j})
\] where \(j' \in c_i\).

If \(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\)
is a function of \(\boldsymbol{\theta}\), the MLEs are still dependent
on the unknown
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\).
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
\(\Pr{}_{\!\boldsymbol{\theta}}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}\) to
find an MLE of \(\boldsymbol{\theta}\). The last condition we assume
achieves this result.

\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $T_i$ and component cause
of failure $K_i$ are not functions of $\boldsymbol{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\boldsymbol{\theta}$.
\end{condition}

When Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}
are satisfied, the joint pdf of \(T_i\) and \(\mathcal{C_i}\) is given
by \[
f_{T_i,\mathcal{C}_i}(t_i,c_i;\boldsymbol{\theta}) =
    \beta_i R_{T_i}(t_i;\boldsymbol{\theta})
    \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}).
\] When we fix the sample and allow \(\boldsymbol{\theta}\) to vary, we
obtain the contribution to the likelihood \(L\) from the
\(i\)\textsuperscript{th} observation when the system lifetime is
exactly known (i.e., \(\delta_i = 1\)) but the component cause of
failure is masked by a candidate set \(c_i\): \begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\boldsymbol{\theta}) = R_{T_i}(t_i;\boldsymbol{\theta}) \sum_{j \in c_i} h_j(t_i;\boldsymbol{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta},
if we observe an exact system failure time for the \(i\)-th system
(\(\delta_i = 1\)), but the component that failed is masked by a
candidate set \(c_i\), then its likelihood contribution is given by
Equation \eqref{eq:likelihood_contribution_masked}.

\hypertarget{right-censored-data}{%
\subsection{Right-Censored Data}\label{right-censored-data}}

As described in Section \ref{sec:like_model}, we observe realizations of
\((S_i,\delta_i,\mathcal{C}_i)\) where \(S_i = \min\{T_i,\tau_i\}\) is
the right-censored system lifetime, \(\delta_i = 1_{\{T_i < \tau_i\}}\)
is the right-censoring indicator, and \(\mathcal{C}_i\) is the candidate
set.

In the previous section, we discussed the likelihood contribution from
an observation of a masked component cause of failure, i.e.,
\(\delta_i = 1\). We now derive the likelihood contribution of a
\emph{right-censored} observation \((\delta_i = 0\)) in our masked data
model.

\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\boldsymbol{\theta}) = R_{T_i}(s_i;\boldsymbol{\theta}).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau_i$, and we only know that
$T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\boldsymbol{\theta}) = \Pr\!{}_{\boldsymbol{\theta}}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\boldsymbol{\theta}) = R_{T_i}(s_i;\boldsymbol{\theta}).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the
likelihood contribution for the \(i\)\textsuperscript{th} system shown
in Theorem \ref{thm:likelihood_contribution}, \[
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
\]

We use this result in the next section to derive the maximum likelihood
estimator of \(\boldsymbol{\theta}\).

\hypertarget{sec:mle}{%
\section{Maximum Likelihood Estimation}\label{sec:mle}}

In our analysis, we use maximum likelihood estimation (MLE) to estimate
the series system parameter \(\boldsymbol{\theta}\) from the masked data
\citep{bain, casella2002statistical}. The MLE finds parameter values
that maximize the likelihood of the observed data under the assumed
model. The maximum likelihood estimate, \(\hat{\boldsymbol{\theta}}\),
is the solution of:

\begin{equation}
\label{eq:mle}
L(\hat{\boldsymbol{\theta}}) = \max_{\boldsymbol{\theta }\in \boldsymbol{\Omega}} L(\boldsymbol{\theta}),
\end{equation}

where \(L(\boldsymbol{\theta})\) is the likelihood function of the
observed data. For computational and analytical simplicity, we work with
the log-likelihood function, denoted as \(\ell(\boldsymbol{\theta})\),
instead of the likelihood function \citep{casella2002statistical}.

\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\boldsymbol{\theta})$, for our masked data model is the sum of the log-likelihoods for each observation,

\begin{equation}
\label{eq:loglike}
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\theta}),
\end{equation}
where $\ell_i(\boldsymbol{\theta})$ is the log-likelihood contribution for the $i$\textsuperscript{th} observation:
\begin{equation}
\ell_i(\boldsymbol{\theta}) = \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) +
    \delta_i \log \bigl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \bigr).
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is the logarithm of the likelihood function,
$$
\ell(\boldsymbol{\theta}) = \log L(\boldsymbol{\theta}) = \log \prod_{i=1}^n L_i(\boldsymbol{\theta}) = \sum_{i=1}^n \log L_i(\boldsymbol{\theta}).
$$
Substituting $L_i(\boldsymbol{\theta})$ from Equation \eqref{eq:like} and separating the two cases of $\delta_i$, we get

\textbf{Case 1}: If the $i$-th system is right-censored ($\delta_i = 0$),
$$
\ell_i(\boldsymbol{\theta}) = \log R_{T_i}(s_i;\boldsymbol{\theta}) = \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the failure time is known ($\delta_i = 1$),
\begin{align*}
\ell_i(\boldsymbol{\theta})
    &= \log R_{T_i}(s_i;\boldsymbol{\theta}) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})\bigr) \\
    &= \sum_{j=1}^m \log R_j(s_i;\boldsymbol{\theta_j}) + \log \bigl(\sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j}) \biggr).
\end{align*}
By Condition \ref{cond:masked_indept_theta}, we may ignore the term $\log \beta_i$ in the MLE since it does not
depend on $\boldsymbol{\theta}$. This gives us the result in Theorem \ref{thm:loglike_total}.
\end{proof}

The MLE, \(\hat{\boldsymbol{\theta}}\), is often found by solving a
system of equations derived from setting the derivative of the
log-likelihood function to zero, i.e., \begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\boldsymbol{\theta}) = 0,
\end{equation} for each component \(\theta_j\) of the parameter
\(\boldsymbol{\theta}\) \citep{bain}. When there's no closed-form
solution, we resort to numerical methods like the Newton-Raphson method.

MLE has desirable asymptotic properties that underpin statistical
inference, namely that it is asymptotically unbiased, unique, and
normally distributed, with a variance given by the inverse of the Fisher
Information Matrix (FIM) \citep{casella2002statistical}. However, for
smaller samples or complex models, these asymptotic properties may not
yield accurate approximations. Hence, we propose to use the bootstrap
method to offer an empirical approach for estimating the sampling
distribution of the MLE.

\hypertarget{sec:boot}{%
\section{Bias-Corrected and Accelerated Bootstrap Confidence
Intervals}\label{sec:boot}}

We utilize the non-parametric bootstrap to approximate the sampling
distribution of the MLE. In the non-parametric bootstrap, we resample
from the observed data with replacement to generate a bootstrap sample.
The MLE is then computed for the bootstrap sample. This process is
repeated \(B\) times, giving us \(B\) bootstrap replicates of the MLE.
The sampling distribution of the MLE is then approximated by the
empirical distribution of the bootstrap replicates of the MLE.

The method we use to generate confidence intervals is known as
Bias-Corrected and Accelerated Bootstrap Confidence Intervals (BCa),
which applies two corrections to the standard bootstrap method:

\begin{itemize}
\item
  Bias correction: This adjusts for bias in the bootstrap distribution
  itself. This bias is measured as the difference between the mean of
  the bootstrap distribution and the observed statistic.
\item
  Acceleration: This adjusts for the rate of change of the statistic as
  a function of the true, unknown parameter. This correction is
  important when the shape of the statistic's distribution changes with
  the true parameter.
\end{itemize}

Thus, BCa provides a more accurate estimate of confidence intervals,
particularly for small sample sizes or skewed distributions, compared to
other methods. Since we are primarly interested in generating confidence
intervals for small samples (otherwise the inverse FIM would be a good
approximation), we use BCa in our analysis. For more details on BCa, see
\citep{efron1987better}.

In our simulation study, we will assess the performance of the
bootstrapped confidence intervals by computing the coverage probability
of the confidence intervals. A 95\% confidence interval should contain
the true value 95\% of the time. If the confidence interval is too
narrow, it will have a coverage probability less than 95\%, which
conveys a sort of false confidence in the precision of the MLE. If the
confidence interval is too wide, it will have a coverage probability
greater than 95\%, which conveys a lack of confidence in the precision
of the MLE. Thus, we want the confidence interval to be as narrow as
possible while still having a coverage probability close to the nominal
level, 95\%.

\hypertarget{issues-with-resampling-from-the-observed-data}{%
\subsubsection*{Issues with Resampling from the Observed
Data}\label{issues-with-resampling-from-the-observed-data}}
\addcontentsline{toc}{subsubsection}{Issues with Resampling from the
Observed Data}

While the bootstrap method provides a robust and flexible tool for
statistical estimation, its effectiveness can be influenced by several
factors \citep{efron1994introduction}.

Firstly, instances of non-convergence in our bootstrap samples were
observed. Such cases can occur when the estimation method, like the MLE
used in our analysis, fails to converge due to the specifics of the
resampled data \citep{casella2002statistical}. This issue can
potentially introduce bias or reduce the effective sample size of our
bootstrap distribution.

Secondly, the bootstrap's accuracy can be compromised with small sample
sizes, as the method relies on the law of large numbers to approximate
the true sampling distribution. For small datasets, the bootstrap
samples might not adequately represent the true variability in the data,
leading to inaccurate results \citep{efron1994introduction}.

Thirdly, our data involves right censoring and a masking of the
component cause of failure when a system failure is observed. These
aspects can cause certain data points or trends to be underrepresented
or not represented at all in our data, introducing bias in the bootstrap
distribution \citep{klein2005survival}.

Despite these challenges, we found the bootstrap method useful in
approximating the sampling distribution of the MLE, taking care in
interpreting the results, particularly as it relates to coverage
probabilities.

\hypertarget{sec:weibull}{%
\section{Series System with Weibull Components}\label{sec:weibull}}

In the real world, systems are quite complex:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  They are not perfect series systems.
\item
  The components in a system are not independent.
\item
  The lifetimes of the components are not precisely modeled by any named
  probability distributions.
\item
  The components may depend on many other unobserved factors.
\end{enumerate}

With these caveats in mind, we model the data as coming from a Weibull
series system of \(m = 5\) components, and other factors, like ambient
temperature, are either negligible (on the distribution of component
lifetimes) or are more or less constant.

The \(j\)\textsuperscript{th} component of the \(i\)\textsuperscript{th}
has a lifetime distribution given by \[
    T_{i j} \sim \operatorname{WEI}(\boldsymbol{\theta_j})
\] where \(\boldsymbol{\theta_j} = (k_j, \lambda_j)\) for
\(j=1,\ldots,m\). Thus,
\(\boldsymbol{\theta }= (\boldsymbol{\theta_1},\ldots,\boldsymbol{\theta_m})' = \bigl(k_1,\lambda_1,\ldots,k_m,\lambda_m\bigr)\).
The random variable \(T_{i j}\) has a reliability function, pdf, and
hazard function given respectively by \begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align} where \(t > 0\) is the lifetime, \(\lambda_j > 0\) is the
scale parameter and \(k_j > 0\) is the shape parameter. The shape
parameters \(k_1, \ldots, k_m\) have the following interpretations:

\begin{enumerate}
\item[$k_j < 1$] The hazard function decreases with respect to time. For instance,
  this may occur as a result of defective components being weeded out early. This
  is known as the *infant mortality* phase.
\item[$k_j = 1$] The hazard function is constant with respect to time. This is an
  idealized case that is rarely observed in practice, but may be useful for modeling
  purposes.
\item[$k_j > 1$] The hazard function increases with respect to time. For instance,
  this may occur as a result of components wearing out. This is known as the
  *aging* phase.
\end{enumerate}

The lifetime of the series system composed of \(m\) Weibull components
has a reliability function given by \begin{equation}
\label{eq:sys_weibull_reliability_function}
R(t;\boldsymbol{\theta}) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}

\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
R(t;\boldsymbol{\theta}) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R(t;\boldsymbol{\theta})
    &= \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}\\
    &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
\end{proof}

The Weibull series system's hazard function is given by \begin{equation}
\label{eq:sys_weibull_failure_rate_function}
h(t;\boldsymbol{\theta}) =
    \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{equation} whose proof follows from Theorem
\ref{thm:sys_failure_rate}.

The pdf of the series system is given by \begin{equation}
\label{eq:sys_weibull_pdf}
f(t;\boldsymbol{\theta}) =
\biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{equation}

\begin{proof}
By definition,
$$
f(t;\boldsymbol{\theta}) = h(t;\boldsymbol{\theta}) R(t;\boldsymbol{\theta}).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}

\hypertarget{weibull-likelihood-model-for-masked-data}{%
\subsection{Weibull Likelihood Model for Masked
Data}\label{weibull-likelihood-model-for-masked-data}}

In Section \ref{sec:like_model}, we discussed two separate kinds of
likelihood contributions, masked component cause of failure data (with
exact system failure times) and right-censored data. The likelihood
contribution of the \(i\)\textsuperscript{th} system is given by the
following theorem.

\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\boldsymbol{\theta}) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\boldsymbol{\theta}) =
\begin{cases}
    R_{T_i}(s_i;\boldsymbol{\theta})                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\boldsymbol{\theta})
        \sum_{j\in c_i} h_j(s_i;\boldsymbol{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\boldsymbol{\theta}) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\boldsymbol{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the
following result.

\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\boldsymbol{\theta}) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\boldsymbol{\theta}$ since they do not
affect the MLE.
\end{corollary}

We find an MLE by solving \eqref{eq:mle_eq}, i.e., a point
\(\boldsymbol{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)\)
satisfying
\(\nabla_{\theta} \ell(\boldsymbol{\hat\theta}) = \boldsymbol{0}\),
where \(\nabla_{\boldsymbol{\theta}}\) is the gradient of the
log-likelihood function (score) with respect to \(\boldsymbol{\theta}\).

To solve this system of equations, we use the Newton-Raphson method,
which requires the score and the Hessian of the log-likelihood function.
We analytically derive the score since it is useful to have for the
Newton-Raphson method, but we do not do the same for the Hessian of the
log-likelihood for the following reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The gradient is relatively easy to derive, and it is useful to have
  for computing gradients efficiently and accurately, which will be
  useful for numerically approximating the Hessian.
\item
  The Hessian is tedious and error prone to derive, and Newton-like
  methods often do not require the Hessian to be explicitly computed.
\end{enumerate}

The following theorem derives the score function.

\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\boldsymbol{\theta}) = \biggl(
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_1},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_m},
    \frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\boldsymbol{\theta})}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the
log-likelihood contribution of the \(i\)-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation
so the proof has been omitted, but the result has been verified by using
a very precise numerical approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions
is the sum of their gradients, and so the score function conditioned on
the entire sample is given by \begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\boldsymbol{\theta}) = \sum_{i=1}^n \nabla \ell_i(\boldsymbol{\theta}).
\end{equation}

\hypertarget{simstudy}{%
\section{Simulation Study}\label{simstudy}}

We derived the likelihood model for masked data for the Weibull series
system in Section \ref{sec:weibull}. In this section, we describe the
design of our simulation study, and we assess the performance of the
bootstrap method for estimating the sampling distribution of the MLE for
a Weibull series system with \(m=5\) components under our proposed
likelihood model.

\hypertarget{realistic-system-designs}{%
\subsection{Realistic System Designs}\label{realistic-system-designs}}

A series system is only as reliable as its least reliable component. In
order to make the simulation study representative of real-world
scenarios, at least for systems designed to be reliable, we choose
parameter values that are representative of real-world systems where
there is no single component that is much less reliable than the others.

One way to define reliability is by the mean time to failure (MTTF),
which is the expected value of the lifetime, which for the Weibull
distribution is given by \[
\text{MTTF} = \lambda \, \Gamma(1 + 1/k),
\] where \(\Gamma\) is the gamma function.

We consider the data from \citep{Huairu-2013}, which includes a study of
the reliability of a series system with three Weibull components with
shape and scale parameters given by \begin{equation}
\begin{aligned}
    k_1 = 1.2576 &\quad \lambda_1 = 994.3661\\
    k_2 = 1.1635 &\quad \lambda_2 = 908.9458\\
    k_3 = 1.1308 &\quad \lambda_3 = 840.1141.
\end{aligned}
\end{equation}

Our approach is to extend this system to a five component system by
adding two more components with shape and scale parameters given by
\begin{equation}
\begin{aligned}
    k_4 = 1.1802 &\quad \lambda_4 = 940.1141\\
    k_5 = 1.3311 &\quad \lambda_5 = 836.1123.
\end{aligned}
\end{equation}

\begin{longtable}[]{@{}lr@{}}
\caption{Meean Time To Failure of Weibull Components and Series
System}\tabularnewline
\toprule
& MTTF\tabularnewline
\midrule
\endfirsthead
\toprule
& MTTF\tabularnewline
\midrule
\endhead
Component 1 & 924.8693\tabularnewline
Component 2 & 862.1568\tabularnewline
Component 3 & 803.5639\tabularnewline
Component 4 & 888.2181\tabularnewline
Component 5 & 768.6793\tabularnewline
Series System & 223.0336\tabularnewline
\bottomrule
\end{longtable}

As shown by Table 2, there are no components that are significantly less
reliable than any of the others. Note that a series system in which,
say, one of the components does have a significantly shorter MTTF would
also pose significant challenges to estimating the parameters of the
system from our masked failure data, since the failure time of the
series system would be dominated by the failure time of the least
reliable component. See Section \ref{sec:opt_rescale} for further
discussion.

\hypertarget{verification}{%
\subsubsection*{Verification}\label{verification}}
\addcontentsline{toc}{subsubsection}{Verification}

To verify that our likelihood model is correct, we load the Table 2 data
from \citep{Huairu-2013} and fit the Weibull series model to the data to
see if we can recover the MLE they reported. When we fit the Weibull
series model to this data by maximizing the likelihood function, we
obtain the following fit for the shape and scale parameters given
respectively by \[
    \hat{k}_1 = 1.2576,
    \hat{k}_2 = 1.1635,
    \hat{k}_3 = 1.1308,
\] and \[
    \hat{\lambda}_1 = 994.3661,
    \hat{\lambda}_2 = 908.9458,
    \hat{\lambda}_3 = 840.1141,
\] which is in agreement with the MLE they reported. Satisfied that our
likelihood model is correct, we proceed with the simulation study.

\hypertarget{data-generating-process}{%
\subsection{Data Generating Process}\label{data-generating-process}}

In this section, we describe the data generating process for our
simulation study. It consists of three parts: the series system, the
candidate set model, and the right-censoring model.

\hypertarget{weibull-series-system-lifetime}{%
\subsubsection*{Weibull Series System
Lifetime}\label{weibull-series-system-lifetime}}
\addcontentsline{toc}{subsubsection}{Weibull Series System Lifetime}

We generate data from a Weibull series system with \(m=5\) components.
As described in Section \ref{sec:weibull}, the \(j\)\textsuperscript{th}
component of the \(i\)\textsuperscript{th} system has a lifetime
distribution given by \[
    T_{i j} \sim \operatorname{WEI}(k_j, \lambda_j)
\] and the lifetime of the series system composed of \(m\) Weibull
components is defined as \[
    T_i = \min\{T_{i 1}, \ldots, T_{i m}\}.
\]

To generate a data set, we first generate the \(m\) component failure
times, by efficiently sampling from their respective distributions, and
we then set the failure time \(t_i\) of the system to the minimum of the
component failure times.

\hypertarget{right-censoring-model}{%
\subsubsection*{Right-Censoring Model}\label{right-censoring-model}}
\addcontentsline{toc}{subsubsection}{Right-Censoring Model}

We employ a very simple right-censoring model, where the right-censoring
time \(\tau\) is fixed at some known value, e.g., an experiment is run
for a fixed amount of time \(\tau\), and all systems that have not
failed by the end of the experiment are right-censored. The censoring
time \(S_i\) of the \(i\)\textsuperscript{th} system is thus given by \[
    S_i = \min\{T_i, \tau\}.
\] So, after we generate the system failure time \(T_i\), we generate
the censoring time \(S_i\) by taking the minimum of \(T_i\) and
\(\tau\).

\hypertarget{masking-model-for-component-cause-of-failure}{%
\subsubsection*{Masking Model for Component Cause of
Failure}\label{masking-model-for-component-cause-of-failure}}
\addcontentsline{toc}{subsubsection}{Masking Model for Component Cause
of Failure}

We must generate data that satisfies the masking conditions described in
Section \ref{sec:candmod}. There are many ways to satisfying the masking
conditions. We choose the simplest method, which we call the
\emph{Bernoulli candidate set model}. In this model, each non-failed
component is included in the candidate set with a fixed probability
\(p\), independently of all other components and independently of
\(\boldsymbol{\theta}\), and the failed component is always included in
the candidate set.

\hypertarget{sec:opt_rescale}{%
\subsection{Issues with Convergence to the MLE}\label{sec:opt_rescale}}

\hypertarget{identifiability}{%
\subsubsection*{Identifiability}\label{identifiability}}
\addcontentsline{toc}{subsubsection}{Identifiability}

When estimating the parameters, we must be careful to ensure that the
parameters are identifiable such that the likelihood function is
maximized at a unique point. If the likelihood function is not maximized
at a unique point, then the MLE is not unique, and a lot of the theory
we have developed so far breaks down, particularly when the likelihood
surface is flat or has multiple local maxima
\citep{mclachlan2007algorithm}.

One way in which this problem may arise is if the data is not
informative enough. For example, if we have a series system and in the
observed masked data component \(1\) is in the candidate set if and only
if component \(2\) is in candidate set, then we do not have enough
information to estimate the parameters of component \(1\) and component
\(2\) separately.

Another way is if the the series system has a component that is the
least reliable by a significant margin and is most likely the component
cause of failure. In this case, our data is not informative enough to
estimate the parameters of the other components. We constructed a quick
experiment to demonstrate this phenomenon in Figure
\ref{fig:flat-loglike-prof}.

\begin{figure}

{\centering \includegraphics{image/last_plot} 

}

\caption{Log-likelihood profile of a flat surface (non-unique MLE)}\label{fig:flat-loglike-prof}
\end{figure}

We simply made the MTTF of the first component much smaller than the
others, and for even large samples there was not generally enough
information to estimate the parameters of the other components. In this
case, the MLE is not unique, and the likelihood surface is flat, as
shown.

We encountered this issue in our simulation study for small samples due
to right-censored and masked component cause of failure data, despite
parameterizing a series system with components that have similar MTTFs.
Our decision was to simply exclude these data sets from our analysis,
since they are not informative enough to estimate the parameters of the
system.

\hypertarget{parameter-rescaling}{%
\subsubsection*{Parameter rescaling}\label{parameter-rescaling}}
\addcontentsline{toc}{subsubsection}{Parameter rescaling}

When the parameters under investigation span different orders of
magnitude, parameter rescaling can significantly improve the performance
and reliability of optimization algorithms. Parameter rescaling gives an
optimizer a sense of the typical size of each parameter, enabling it to
adjust its steps accordingly. This is crucial in scenarios like ours,
where shape and scale parametes are a few orders of magnitude apart.
Without rescaling, the optimization routine may struggle, taking
numerous small steps for larger parameters and overshooting for smaller
ones. For more information, see \citep{nocedal2006numerical}.

Speed of convergence was particularly important in our case, since in
our simulation study, we employ the bootstrap method to estimate the
sampling distribution of the MLE, which requires us to estimate the MLE
for many data sets. We found that parameter rescaling significantly
improved the speed of convergence, which allowed us to run our
simulation study in a tractable amount of time.

\hypertarget{assessing-the-bootstrapped-confidence-intervals}{%
\subsection{Assessing the Bootstrapped Confidence
Intervals}\label{assessing-the-bootstrapped-confidence-intervals}}

Our primary interest is in assessing the performance of the BCa
confidence intervals for the MLE. We will assess the performance of the
BCa confidence intervals by computing the coverage probability of the
confidence intervals. Under a variety of scenarios, we will bootstrap a
\(95\%\)-confidence interval for \(\boldsymbol{\theta}\) using the BCa
method, and we will evaluate its calibration by computing the coverage
probability and its precision by assessing the width of the confidence
interval.

The coverage probability is defined as the proportion of times that the
true value of \(\boldsymbol{\theta}\) falls within the confidence
interval. We will compute the coverage probability by generating \(R\)
datasets from the Data Generating Process (DGP) and computing the
coverage probability for each dataset. We will then aggregate this
information across all \(R\) datasets to estimate the coverage
probability.

\hypertarget{simulation-scenarios}{%
\subsection{Simulation Scenarios}\label{simulation-scenarios}}

We parameterize \(\tau\) by quantiles of the series system, e.g., if
\(q = 0.8\), then \(\tau(0.8)\) is the \(80\%\) quantile of the series
system such that \(80\%\) of the sytems are expected to fail before time
\(\tau(0.8)\) and \(20\%\) of the series systems are expected to be
right-censored.

We define a simulation scenario to be some comination of \(n\) and
\(p\). We are interested in choosing a small number of scenarios that
are representative of real-world scenarios and that are interesting to
analyze.

Here is an outline of the simulation study analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose a scenario (sample size \(n\), masking probability \(p\)
  (\(\tau\) fixed).
\item
  Generate R datasets from the Data Generating Process (DGP). The DGP
  should be compatible with the assumptions in our likelihood model. In
  our case, we use:

  \begin{itemize}
  \item
    Right-censored series lifetimes with \(m = 5\) Weibull components.
  \item
    Mmasking component cause of failure using Bernoulli candidate set
    model.
  \end{itemize}
\item
  For each of these \(R\) datasets, calculate the Maximum Likelihood
  Estimator (MLE).
\item
  For each of these \(R\) datasets, perform bootstrap resampling \(B\)
  times to create a set of bootstrap samples.
\item
  Calculate the MLE for each of these bootstrap samples. This generates
  an empirical distribution of the MLE, which is used to construct a
  confidence interval for the MLE.
\item
  Repeat steps 4 and 5 for each of the \(R\) datasets.
\item
  For each dataset, determine whether the true parameter value falls
  within the computed CI. Aggregate this information across all \(R\)
  datasets to estimate the coverage probability of the CI.
\item
  Interpret the results and discuss the performance of the MLE estimator
  under various scenarios.
\end{enumerate}

For how we generate a scenario, see Appendix A.

\hypertarget{effect-of-right-censoring-on-the-mle}{%
\subsection{Effect of Right-Censoring on the
MLE}\label{effect-of-right-censoring-on-the-mle}}

In all of our simulation studies, we use a fixed right-censoring time
\(\tau = 377.71\), which is the \(82.5\%\) quantile of the series
system. This means that \(82.5\%\) of the series systems are expected to
fail before time \(\tau\) and \(17.5\%\) of the series.

This represents a situation in which an experiment is run for a fixed
amount of time \(\tau\), and all systems that have not failed by the end
of the experiment are right-censored.

Right-censoring introduces a source of bias in the MLE. Right-censoring
has the effect of pushing the MLE to estimate a lower value for the
scale parameters and a higher value for the shape parameters. This is
because when we observe a right-censoring event, we know that the system
failed after the censoring time, but we do not know precisely when it
will fail.

\hypertarget{effect-of-masking-the-component-cause-of-failure-on-the-mle}{%
\subsection{Effect of Masking the Component Cause of Failure on the
MLE}\label{effect-of-masking-the-component-cause-of-failure-on-the-mle}}

The mean time to failure (MTTF) for the \(j\)\textsuperscript{th}
component in a Weibull distribution is given by: \[
\text{MTTF} = \lambda_j \Gamma(1 + 1/k_j).
\]

\hypertarget{scale-paramater-positive-bias}{%
\subsection{Scale Paramater: Positive
Bias}\label{scale-paramater-positive-bias}}

As the scale parameter \(\lambda_j\) increases (keeping the shape
parameter constant), the MTTF increases. When we increase the component
masking, which in our simulation study is determined by the masking
probability \(p\), then there is less certainty about which component
caused the system to fail, but we know it was one of the components in
the candidate set. Therefore, to make it more likely that the component
cause of failure is in the candidate set, we increase the scale
parameters of the components in the candidate set, which increases their
respective MTTFs.

We will demonstrate this effect in Section \ref{sec:p-vs-mttf}.

\hypertarget{shape-parameter-negative-bias}{%
\subsubsection{Shape Parameter: Negative
Bias}\label{shape-parameter-negative-bias}}

As the shape parameter increases (keeping the scale parameter constant),
the quantity \(1/k_j\) decreases. Given that the gamma function
\(\Gamma\) is a monotonically increasing function, a decrease in its
argument (which is the case here since \(1 + 1/k_j\) decreases with an
increase in shape) results in a decrease in the MTTF.

Hence, when the shape parameter increases, the MTTF decreases, assuming
the scale parameter is kept constant. Now, if the MLE for the shape
parameter is positively biased (i.e., \(E(\hat k_j) > k_j\)), then the
estimated shape parameter is larger than the true shape parameter. When
we use this larger estimated shape parameter to compute the MTTF, the
resulting MTTF will be smaller than the actual MTTF (assuming the scale
parameter is kept constant or accurately estimated).

In other words, a positive bias in the estimated shape parameter would
lead to an underestimation of the MTTF, assuming that the scale
parameter is either known or accurately estimated.

\hypertarget{coverage-probability-vs-sample-size}{%
\subsection{Coverage Probability vs Sample
Size}\label{coverage-probability-vs-sample-size}}

In the simulation study, we have generated many different synthetic
samples of different sizes (\(n\)) from a data generating process (DGP)
that is compatible with the assumptions our likelihood model makes about
the data. In particular, right-censored series system lifetimes with a
fixed right-censoring time for the system and five components with
Weibull lifetimes, each with a different shape and scale parameter. For
each observation, we then mask the component cause of failure with
candidate sets that satisfy the three primary conditions of the
likelihood model, e.g., the failed component is always in the candidate
set. For each synthetic data set, we then compute the MLEs of the shape
and scale parameters of the Weibull distribution. We then use the MLEs
to compute the BCa bootstrapped 95\% CIs.

In what follows, we analyze the performance of the BCa bootstrapped CIs
for the shape and scale parameters under different masking conditions
(\(p\)) for the component cause of failure. We will focus on the
following statistics:

\begin{itemize}
\item
  \emph{Coverage Probability (CP)}: The CP is the proportion of the
  bootstrapped CIs that contain the true value of the parameter. The CP
  is a good indicator of the reliability of the estimates as previously
  discussed.
\item
  \emph{Dispersion of MLEs}: The shaded regions representing the 95\%
  probability range of the MLEs get narrower as the sample size
  increases. This is an indicator of the increased precision in the
  estimates as more data is available. We call it a \emph{Confidence
  Band}, but it is actually an estimate of the quantile range of the
  MLEs. The shaded region provides insight into the distribution of the
  MLEs.
\item
  \emph{IQR of Bootstrapped CIs}: The vertical blue bars represent the
  Interquartile Range (IQR) of the actual bootstrapped Confidence
  Intervals (CIs). Since in practice we only have one sample and
  consequently one MLE, we use bootstrapping to resample and compute
  multiple CIs. The IQR then represents the middle 50\% range of these
  bootstrapped CIs.
\item
  \emph{Mean of the MLEs}: The mean of the MLEs is a good indicator of
  the bias in the estimates. If the mean of the MLEs is close to the
  true value, then the MLEs are, on average, unbiased.
\end{itemize}

The distinction between the shaded region (95\% range of MLEs) and the
blue vertical bars (IQR of bootstrapped CIs) is important. The shaded
region provides insight into the distribution of the MLEs, whereas the
blue vertical bars provide information about the variation in the
bootstrapped CIs. Both are relevant for understanding the behavior of
the estimations.

\hypertarget{scale-parameters}{%
\subsubsection*{Scale Parameters}\label{scale-parameters}}
\addcontentsline{toc}{subsubsection}{Scale Parameters}

\begin{figure}

{\centering \includegraphics{image/plot-n-vs-stats-p215-scale} 

}

\caption{Sample Size vs Bootstrapped Scale CI Statistics (p = 0.215)}\label{fig:samp-size-n-vs-stats-215-scale}
\end{figure}

Figure \ref{fig:samp-size-n-vs-stats-p215-scale} shows the distribution
of the MLEs for the shape parameters of the first three components and
the bootstrapped CIs for different sample sizes with a component cause
of failrue masking probaility of \(p = 0.215\) (each non-failed
component is in the candidate set with a \(21.5\%\) probabiltiy).

The distinction between the shaded region (95\% range of MLEs) and the
blue vertical bars (IQR of bootstrapped CIs) is important. The shaded
region provides insight into the distribution of the MLEs, whereas the
blue vertical bars provide information about the variation in the
bootstrapped CIs. Both are relevant for understanding the behavior of
the estimations. Here are several key observations:

\begin{itemize}
\item
  \emph{Coverage Probability (CP)}: The CP is well-calibrated, obtaining
  a value near the nominal 95\% level across different sample sizes.
  This suggests that the bootstrapped CIs will contain the true value of
  the shape parameter with the specified confidence level. The CIs are
  neither too wide nor too narrow.
\item
  \emph{Dispersion of MLEs}: The shaded regions representing the 95\%
  probability range of the MLEs get narrower as the sample size
  increases. This is an indicator of the increased precision in the
  estimates as more data is available.
\item
  \emph{IQR of Bootstrapped CIs}: The IQR (vertical blue bars) reduces
  with an increase in sample size. This suggests that the bootstrapped
  CIs are getting more consistent and focused around a narrower range
  with larger samples while maintaining a good coverage probability. As
  we get more data, the bootstrapped CIs are more likely to be closer to
  each other and the true value of the scale parameter. For small sample
  sizes, they are quite large, but to maintain well-calibrated CIs, this
  was necessary. The estimator is quite sensitive to the data, and so
  the bootstrapped CIs are quite wide to account for this sensitivity
  when the sample size is small and not necessarily representative of
  the true distribution.
\item
  \emph{Mean of MLEs}: The red dashed line indicating the mean of MLEs
  remains stable across different sample sizes and close to the true
  value, suggesting that the MLEs are, on average, reasonably unbiased.
\end{itemize}

Next, we consider what occurs when we increase the masking probability
\(p\) to \(0.333\), which represents a fairly significant masking of the
component cause of failure.

\begin{figure}

{\centering \includegraphics{image/plot-n-vs-stats-p333-scale} 

}

\caption{Sample Size vs Bootstrapped Scale CI Statistics (p = 0.333)}\label{fig:samp-size-n-vs-stats-p333-scale}
\end{figure}

Figure \ref{fig:samp-size-n-vs-stats-p333-scale} shows the distribution
of the MLEs for the shape parameters of the first three components and
the bootstrapped CIs for different sample sizes with a component cause
of failrue masking probaility of \(p = 0.333\) (each non-failed
component is in the candidate set with a \(33.3\%\) probabiltiy).

Here are several key observations, similar to the previous scenario:

\begin{itemize}
\item
  \emph{Coverage Probability (CP)}: The CP is well-calibrated, obtaining
  a value near the nominal 95\% level across different sample sizes.
  Again, this suggests that the bootstrapped CIs will contain the true
  value of the shape parameter with the specified confidence level. In
  general, the CIs are neither too wide nor too narrow, but the CP for
  \(\lambda_1\) at the sample size of 50 is relatively lower at 0.88.
  This might indicate a bit of uncertainty at smaller sample sizes for
  this parameter. Recall in Section \ref{sec:reliability}, the MTTF of
  component 1 is the largest, and so it is the least likely to fail.
  This might explain the lower CP for \(\lambda_1\) at smaller sample
  sizes, as the data has not shown enough failures from component 1 to
  accurately estimate its scale parameter. If we were to increase its
  MTTF to some radically larger value, it would almost never fail, and
  the uncertainty about the scale parameter would increase.
\item
  \emph{Dispersion of MLEs}: As with the previous plots, the shaded
  regions, which represents the 95\% probability range of the MLEs,
  narrows with incresing sample sizes. This indicates an increase in
  precision when more data is available. If this did not occur, then it
  would indicate that the MLEs are not consistent or are likelihood
  model is not appropriate for the data.
\item
  \emph{IQR of Bootstrapped CIs}: The IQR (vertical blue bars) reduces
  with an increase in sample size. This suggests that the bootstrapped
  CIs are getting more consistent and focused around a narrower range
  with larger samples while maintaining a good coverage probability. As
  we get more data, the bootstrapped CIs are more likely to be closer to
  each other and the true value of the scale parameter.
\item
  \emph{Mean of MLEs}: The red dashed line indicating the mean of MLEs
  is less stable across different sample sizes, but it is still close to
  the true value. We will investigate the effect of the masking
  probability in more detail in Section \ref{sec:p-vs-mttf}.
\end{itemize}

\hypertarget{shape-parameters}{%
\subsubsection{Shape Parameters}\label{shape-parameters}}

\begin{figure}

{\centering \includegraphics{image/plot-n-vs-stats-p215-shape} 

}

\caption{Sample Size vs Bootstrapped Shape CI Statistics (p = 0.215)}\label{fig:samp-size-n-vs-stats-p215-shape}
\end{figure}

Figure \ref{fig:samp-size-n-vs-stats-p215-shape} shows the distribution
of the MLEs for the shape parameters of the first three components and
the bootstrapped CIs for different sample sizes with a component cause
of failrue masking probaility of \(p = 0.333\) (each non-failed
component is in the candidate set with a \(33.3\%\) probabiltiy).

Here are several key observations, similar to the previous scenario:

\begin{itemize}
\item
  \emph{Coverage Probability (CP)}: The CP is well-calibrated, obtaining
  a value near the nominal 95\% level across different sample sizes.
  Again, this suggests that the bootstrapped CIs will contain the true
  value of the shape parameter with the specified confidence level. The
  CIs are neither too wide nor too narrow.
\item
  \emph{Dispersion of MLEs}: The shaded regions representing the 95\%
  probability range of the MLEs get narrower as the sample size
  increases. This is an indicator of the increased precision in the
  estimates as more data is available.
\item
  \emph{IQR of Bootstrapped CIs}: The IQR (vertical blue bars) reduces
  with an increase in sample size. This suggests that the bootstrapped
  CIs are getting more consistent and focused around a narrower range
  with larger samples while maintaining a good coverage probability. As
  we get more data, the bootstrapped CIs are more likely to be closer to
  each other and the true value of the scale parameter.
\item
  \emph{Mean of MLEs}: The red dashed line indicating the mean of MLEs
  is less stable across different sample sizes, but it is still close to
  the true value. We will investigate the effect of the masking
  probability in more detail in Section \ref{sec:p-vs-mttf}.
\end{itemize}

\begin{figure}

{\centering \includegraphics{image/plot-n-vs-stats-p333-shape} 

}

\caption{Sample Size vs Bootstrapped Shape CI Statistics (p = 0.333)}\label{fig:samp-size-n-vs-stats-p333-shape}
\end{figure}

\hypertarget{sec:p-vs-mttf}{%
\subsection{Masking Probability for Component Cause of
Failure}\label{sec:p-vs-mttf}}

\begin{quote}
\begin{quote}
Dr.~Agustin: I did this experiment for n = 200 to demonstrate two
effects the masking probability has:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Demonstrate that as masking probability p increases, the precision of
  the MLEs decrease. If the CI maintains a well-calibrated coverage
  probability, this will reslt in wider CIs to account for the added
  uncertainty. Note that this is demonstrated by the plots.
\item
  Demonstrate that as masking probability p increases, the estimates
  become more biased in a way that reduces the MTTF of the components,
  to account for the additional uncertainty about the component cause of
  failure. This is I was seeking, but it's not obvious in the results of
  this experiment. I hypothesize that the sample size chosen (200) is
  too large and infomative to demonstrate the effect.
\end{enumerate}
\end{quote}
\end{quote}

In Figure \ref{fig:masking-prob-vs-stats-scale}, we show the effect of
the masking probability \(p\) on the MLE and the bootstrapped confidence
intervals for the scale parameters for relatively large samples of size
\(n = 200\). With this sample size, we see that the MLE is now largely
unbiased and the bootstrapped BCa CIs for all parameters have
well-calibrated coverage probabilities. In Figure
\ref{fig:masking-prob-vs-stats-shape}, the same pattern occurs for the
effect the masking probability has on the MLE of the shape parameters.

For both shape and scale parameters, we see that as the masking
probability increases, the IQR of the bootstrapped CIs and the
dispersion of the MLEs increase, which indicates that the masking
probability effects the precision of the estimates. As the masking
probability increases, we have less certainty about the component cause
of failure, and thus less certainty about the estimates for the
component parameters.

\begin{figure}

{\centering \includegraphics{image/plot-p-vs-stats-shape} 

}

\caption{Component Cause of Failure Masking (p) vs Shape CI Statistics}\label{fig:masking-prob-vs-stats-shape}
\end{figure}

\begin{figure}

{\centering \includegraphics{image/plot-p-vs-stats-scale} 

}

\caption{Component Cause of Failure Masking (p) vs Scale CI Statistics}\label{fig:masking-prob-vs-stats-scale}
\end{figure}

\begin{quote}
\begin{quote}
Dr.~Agustin: I have a final experiment where I vary the MTTF of
component 3, by changing its scale parameter, \(\lambda_j\). I have data
about how this effects the accuracy of the MLE and can explain why,
e.g., if the MTTF is significantly lower, it domains the component cause
of failure and so we have less information about the other components.
However, I haven't done the actual analysis yet (data is collected). I'm
pretty sure it will cause the CIs for the other component parameters to
be larger, but whether we should include it can be discussed in our
meeting. Tomorrow, I can do the do the visualization, and I can get back
to you on the analysis before then if you're interested.
\end{quote}
\end{quote}

\hypertarget{future-work}{%
\section{Future Work}\label{future-work}}

\hypertarget{parametric-bootstrap}{%
\subsection{Parametric Bootstrap}\label{parametric-bootstrap}}

There is an alternative form of the bootstrap called the parametric
bootstap, where the bootstrap samples are generated from a parametric
distribution. However, this parametric bootstrap is not appropriate for
our analysis because we do not assume a parametric form for the
distribution of the candidate sets \(\mathcal{C}_i\).

However, it would be possible to employ a semi-parametric bootstrap,
where the bootstrap samples are generated from a parametric distribution
for the system lifetimes \(T_i\) and a non-parametric distribution for
the candidate sets \(\mathcal{C}_i | T_i, K_i\), e.g., the empirical
distribution with some discretization of \(T_i\) used.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We have developed a likelihood model for series systems with latent
components and right-censoring. We have provided evidence that, as long
as certain regularity conditions are met, the MLE is asymptotically
unbiased and consistent.

\begin{quote}
Repeat earlier results about how the masking probability effects the MLE
and its bootstrapped CIs and the explanation why. Do the same for the
sample size.
\end{quote}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

Please see below for a full list of references.

\hypertarget{refs}{}
\begin{cslreferences}
\end{cslreferences}

\hypertarget{app}{%
\section{Appendix}\label{app}}

\hypertarget{app:data}{%
\subsection{Data}\label{app:data}}

\hypertarget{simulation-code}{%
\subsection*{Simulation Code}\label{simulation-code}}
\addcontentsline{toc}{subsection}{Simulation Code}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# Simulation data generating process for specified scenario \#}
\CommentTok{\# (n, p, q), where:                                         \#}
\CommentTok{\#    {-} n is a vector of sample sizes                        \#}
\CommentTok{\#     {-} n is a vector of sample sizes                       \#}
\CommentTok{\#     {-} p is a vector of masking probabilities              \#}
\CommentTok{\#     {-} q is a vector of right{-}censoring quantiles of the   \#}
\CommentTok{\#       Weibull series distribution.                        \#}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\CommentTok{\# here is the R libary we developed for this project}
\KeywordTok{library}\NormalTok{(wei.series.md.c1.c2.c3) }

\CommentTok{\# for parallel processing}
\KeywordTok{library}\NormalTok{(parallel)}

\CommentTok{\# you can set a seed for reproducibility of the experimental run}
\CommentTok{\# however, if you use parallel processing, this simple approach will not work.}
\CommentTok{\# set.seed(1234)}

\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# Here is an example of how to run a scenario \#}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\CommentTok{\# set the simulation name to be used in the file names}
\NormalTok{sim.name \textless{}{-}}\StringTok{ "sim{-}2"}
\CommentTok{\# set the sample sizes}
\NormalTok{ns \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{800}\NormalTok{)}
\CommentTok{\# set the masking probabilities}
\NormalTok{ps \textless{}{-}}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
\CommentTok{\# set the right{-}censoring quantiles}
\NormalTok{qs \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.95}\NormalTok{)}
\CommentTok{\# set the number of replicates}
\NormalTok{R \textless{}{-}}\StringTok{ }\DecValTok{100}
\CommentTok{\# set the number of CPU cores to use}
\NormalTok{ncores \textless{}{-}}\StringTok{ }\DecValTok{4}

\CommentTok{\# true parameter values}
\NormalTok{theta \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{shape1 =} \FloatTok{1.2576}\NormalTok{, }\DataTypeTok{scale1 =} \FloatTok{994.3661}\NormalTok{,}
           \DataTypeTok{shape2 =} \FloatTok{1.1635}\NormalTok{, }\DataTypeTok{scale2 =} \FloatTok{908.9458}\NormalTok{,}
           \DataTypeTok{shape3 =} \FloatTok{1.1308}\NormalTok{, }\DataTypeTok{scale3 =} \FloatTok{840.1141}\NormalTok{,}
           \DataTypeTok{shape4 =} \FloatTok{1.1802}\NormalTok{, }\DataTypeTok{scale4 =} \FloatTok{940.1141}\NormalTok{,}
           \DataTypeTok{shape5 =} \FloatTok{1.3311}\NormalTok{, }\DataTypeTok{scale5 =} \FloatTok{836.1123}\NormalTok{)}

\NormalTok{shapes \textless{}{-}}\StringTok{ }\NormalTok{theta[}\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(theta), }\DecValTok{2}\NormalTok{)]}
\NormalTok{scales \textless{}{-}}\StringTok{ }\NormalTok{theta[}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\KeywordTok{length}\NormalTok{(theta), }\DecValTok{2}\NormalTok{)]}

\CommentTok{\# helps the MLE optimization routine converge more quickly and reliably}
\CommentTok{\# by scaling the parameters to be of similar magnitude}
\NormalTok{parscale \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\NormalTok{sim.run \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(sim.name, n, p, q, }\DataTypeTok{R =} \DecValTok{1000}\NormalTok{) \{}
\NormalTok{    mles \textless{}{-}}\StringTok{ }\KeywordTok{list}\NormalTok{()}
\NormalTok{    problems \textless{}{-}}\StringTok{ }\KeywordTok{list}\NormalTok{()}

\NormalTok{    tau \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{qwei\_series}\NormalTok{(}
        \DataTypeTok{p =}\NormalTok{ q, }\DataTypeTok{scales =}\NormalTok{ scales, }\DataTypeTok{shapes =}\NormalTok{ shapes)}

    \KeywordTok{cat}\NormalTok{(}\StringTok{"n ="}\NormalTok{, n, }\StringTok{", p ="}\NormalTok{, p, }\StringTok{", q = "}\NormalTok{, q, }\StringTok{", tau = "}\NormalTok{, tau, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ (r }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{R) \{}
\NormalTok{        result \textless{}{-}}\StringTok{ }\KeywordTok{tryCatch}\NormalTok{(\{}

\NormalTok{            df \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{generate\_guo\_weibull\_table\_2\_data}\NormalTok{(}
                \DataTypeTok{shapes =}\NormalTok{ shapes,}
                \DataTypeTok{scales =}\NormalTok{ scales,}
                \DataTypeTok{n =}\NormalTok{ n,}
                \DataTypeTok{p =}\NormalTok{ p,}
                \DataTypeTok{tau =}\NormalTok{ tau)}

\NormalTok{            sol \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{mle\_nelder\_wei\_series\_md\_c1\_c2\_c3}\NormalTok{(}
                \DataTypeTok{df =}\NormalTok{ df,}
                \DataTypeTok{theta0 =}\NormalTok{ theta,}
                \DataTypeTok{reltol =} \FloatTok{1e{-}7}\NormalTok{,}
                \DataTypeTok{parscale =}\NormalTok{ parscale,}
                \DataTypeTok{maxit =}\NormalTok{ 2000L)}
\NormalTok{            mles \textless{}{-}}\StringTok{ }\KeywordTok{append}\NormalTok{(mles, }\KeywordTok{list}\NormalTok{(sol))}

            \ControlFlowTok{if}\NormalTok{ (r }\OperatorTok{\%\%}\StringTok{ }\DecValTok{10} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
                \KeywordTok{cat}\NormalTok{(}\StringTok{"r = "}\NormalTok{, r, }\StringTok{": "}\NormalTok{, sol}\OperatorTok{$}\NormalTok{par, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{            \}}

\NormalTok{        \}, }\DataTypeTok{error =} \ControlFlowTok{function}\NormalTok{(e) \{}
            \KeywordTok{cat}\NormalTok{(}\StringTok{"Error at iteration"}\NormalTok{, r, }\StringTok{":"}\NormalTok{)}
            \KeywordTok{print}\NormalTok{(e)}
\NormalTok{            problems \textless{}{-}}\StringTok{ }\KeywordTok{append}\NormalTok{(problems, }\KeywordTok{list}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
                \DataTypeTok{error =}\NormalTok{ e, }\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{df =}\NormalTok{ df)))}
\NormalTok{        \})}
\NormalTok{    \}}
  
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(mles) }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \KeywordTok{saveRDS}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{mles =}\NormalTok{ mles),}
            \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"./results/"}\NormalTok{, sim.name, }\StringTok{"/results\_"}\NormalTok{, n, }\StringTok{"\_"}\NormalTok{, p, }\StringTok{"\_"}\NormalTok{, q, }\StringTok{".rds"}\NormalTok{))}
\NormalTok{    \}}

    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(problems) }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \KeywordTok{saveRDS}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{problems =}\NormalTok{ problems),}
            \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"./problems/"}\NormalTok{, sim.name, }\StringTok{"/problems\_"}\NormalTok{, n, }\StringTok{"\_"}\NormalTok{, p, }\StringTok{"\_"}\NormalTok{, q, }\StringTok{".rds"}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{params \textless{}{-}}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ ns, }\DataTypeTok{p =}\NormalTok{ ps, }\DataTypeTok{q =}\NormalTok{ qs)}
\NormalTok{result \textless{}{-}}\StringTok{ }\KeywordTok{mclapply}\NormalTok{(}
    \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(params),}
    \ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{sim.run}\NormalTok{(sim.name, params}\OperatorTok{$}\NormalTok{n[i], params}\OperatorTok{$}\NormalTok{p[i], params}\OperatorTok{$}\NormalTok{q[i], R),}
    \DataTypeTok{mc.cores =}\NormalTok{ ncores)}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix-b-simulation-of-scenarios-using-the-bootstrap-method}{%
\subsection*{Appendix B: Simulation of scenarios using the Bootstrap
method}\label{appendix-b-simulation-of-scenarios-using-the-bootstrap-method}}
\addcontentsline{toc}{subsection}{Appendix B: Simulation of scenarios
using the Bootstrap method}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# in this scenario, we want to see how we can use the bootstrap}
\CommentTok{\# method to estimate the confidence intervals more precisely (better calibration}
\CommentTok{\# of confidence intervals) for small sample sizes.}
\CommentTok{\# we\textquotesingle{}ll use it to construct a 95\% confidence interval for the estimator. we\textquotesingle{}ll}
\CommentTok{\# compare this result to the asymptotic theory confidence interval.}
\CommentTok{\# finally, we\textquotesingle{}ll generate CIs by each method, asymptotic (inverse FIM) and }
\CommentTok{\# bootstrap (cov), and compare the coverage probabilities.}
\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{library}\NormalTok{(parallel)}
\KeywordTok{library}\NormalTok{(wei.series.md.c1.c2.c3)}

\NormalTok{theta \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{shape1 =} \FloatTok{1.2576}\NormalTok{, }\DataTypeTok{scale1 =} \FloatTok{994.3661}\NormalTok{,}
           \DataTypeTok{shape2 =} \FloatTok{1.1635}\NormalTok{, }\DataTypeTok{scale2 =} \FloatTok{908.9458}\NormalTok{,}
           \DataTypeTok{shape3 =} \FloatTok{1.1308}\NormalTok{, }\DataTypeTok{scale3 =} \FloatTok{840.1141}\NormalTok{,}
           \DataTypeTok{shape4 =} \FloatTok{1.1802}\NormalTok{, }\DataTypeTok{scale4 =} \FloatTok{940.1141}\NormalTok{,}
           \DataTypeTok{shape5 =} \FloatTok{1.3311}\NormalTok{, }\DataTypeTok{scale5 =} \FloatTok{836.1123}\NormalTok{)}

\NormalTok{shapes \textless{}{-}}\StringTok{ }\NormalTok{theta[}\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(theta), }\DecValTok{2}\NormalTok{)]}
\NormalTok{scales \textless{}{-}}\StringTok{ }\NormalTok{theta[}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\KeywordTok{length}\NormalTok{(theta), }\DecValTok{2}\NormalTok{)]}

\CommentTok{\# number of CPU cores to use in bootstrap for parallel processing}
\NormalTok{ncores \textless{}{-}}\StringTok{ }\DecValTok{4}

\CommentTok{\# helps the MLE optimization routine converge more quickly and reliably}
\NormalTok{parscale \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\CommentTok{\#set.seed(134849131)}

\CommentTok{\# sample sizes}
\NormalTok{ns \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\CommentTok{\# masking probabilities, no masking and 21.5\% masking}
\NormalTok{ps \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.215}\NormalTok{)}
\CommentTok{\# quantiles of weibull series distribution, no right{-}censoring and 25\% right{-}censoring}
\NormalTok{qs \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.75}\NormalTok{)}

\NormalTok{sim.name \textless{}{-}}\StringTok{ "sim{-}1{-}boot"}

\NormalTok{sim.boot.run \textless{}{-}}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n, p, q, }\DataTypeTok{R =} \DecValTok{1000}\NormalTok{) \{}

\NormalTok{    problems \textless{}{-}}\StringTok{ }\KeywordTok{list}\NormalTok{()}

\NormalTok{    tau \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{qwei\_series}\NormalTok{(}
        \DataTypeTok{p =}\NormalTok{ q, }\DataTypeTok{scales =}\NormalTok{ scales, }\DataTypeTok{shapes =}\NormalTok{ shapes)}

    \KeywordTok{cat}\NormalTok{(}\StringTok{"n ="}\NormalTok{, n, }\StringTok{", p ="}\NormalTok{, p, }\StringTok{", q = "}\NormalTok{, q, }\StringTok{", tau = "}\NormalTok{, tau, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{    result \textless{}{-}}\StringTok{ }\KeywordTok{tryCatch}\NormalTok{(\{}
\NormalTok{        df \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{generate\_guo\_weibull\_table\_2\_data}\NormalTok{(}
            \DataTypeTok{shapes =}\NormalTok{ shapes,}
            \DataTypeTok{scales =}\NormalTok{ scales,}
            \DataTypeTok{n =}\NormalTok{ n,}
            \DataTypeTok{p =}\NormalTok{ p,}
            \DataTypeTok{tau =}\NormalTok{ tau)}

\NormalTok{        sol \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{mle\_nelder\_wei\_series\_md\_c1\_c2\_c3}\NormalTok{(}
            \DataTypeTok{df =}\NormalTok{ df,}
            \DataTypeTok{theta0 =}\NormalTok{ theta,}
            \DataTypeTok{reltol =} \FloatTok{1e{-}7}\NormalTok{,}
            \DataTypeTok{parscale =}\NormalTok{ parscale,}
            \DataTypeTok{maxit =}\NormalTok{ 2000L)}

        \KeywordTok{cat}\NormalTok{(}\StringTok{"mle: "}\NormalTok{, sol}\OperatorTok{$}\NormalTok{par, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\NormalTok{        sol.boot \textless{}{-}}\StringTok{ }\KeywordTok{boot}\NormalTok{(df, }\ControlFlowTok{function}\NormalTok{(df, i) \{}
\NormalTok{            sol \textless{}{-}}\StringTok{ }\NormalTok{wei.series.md.c1.c2.c3}\OperatorTok{::}\KeywordTok{mle\_nelder\_wei\_series\_md\_c1\_c2\_c3}\NormalTok{(}
                \DataTypeTok{df =}\NormalTok{ df[i, ],}
                \DataTypeTok{theta0 =}\NormalTok{ sol}\OperatorTok{$}\NormalTok{par,}
                \DataTypeTok{reltol =} \FloatTok{1e{-}7}\NormalTok{,}
                \DataTypeTok{parscale =}\NormalTok{ parscale,}
                \DataTypeTok{maxit =}\NormalTok{ 1000L)}
            \KeywordTok{cat}\NormalTok{(}\StringTok{"boot: "}\NormalTok{, sol}\OperatorTok{$}\NormalTok{par, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{            sol}\OperatorTok{$}\NormalTok{par}
\NormalTok{        \}, }\DataTypeTok{ncpus =}\NormalTok{ ncores, }\DataTypeTok{R =}\NormalTok{ R)}

        \KeywordTok{saveRDS}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{mle =}\NormalTok{ sol, }\DataTypeTok{mle.boot =}\NormalTok{ sol.boot),}
            \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"./results/"}\NormalTok{, sim.name, }\StringTok{"/results\_"}\NormalTok{, n, }\StringTok{"\_"}\NormalTok{, p, }\StringTok{"\_"}\NormalTok{, q, }\StringTok{".rds"}\NormalTok{))}

\NormalTok{        \}, }\DataTypeTok{error =} \ControlFlowTok{function}\NormalTok{(e) \{}
            \KeywordTok{print}\NormalTok{(e)}
\NormalTok{            problems \textless{}{-}}\StringTok{ }\KeywordTok{append}\NormalTok{(problems, }\KeywordTok{list}\NormalTok{(}\KeywordTok{list}\NormalTok{(}
                \DataTypeTok{error =}\NormalTok{ e, }\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{df =}\NormalTok{ df)))}
\NormalTok{        \})}

    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(problems) }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \KeywordTok{saveRDS}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{q =}\NormalTok{ q, }\DataTypeTok{tau =}\NormalTok{ tau, }\DataTypeTok{problems =}\NormalTok{ problems),}
                \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"./problems/"}\NormalTok{, sim.name, }\StringTok{"/problems\_"}\NormalTok{, n, }\StringTok{"\_"}\NormalTok{, p, }\StringTok{"\_"}\NormalTok{, q, }\StringTok{".rds"}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{\}}
  
\NormalTok{params \textless{}{-}}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ ns, }\DataTypeTok{p =}\NormalTok{ ps, }\DataTypeTok{q =}\NormalTok{ qs)}
\NormalTok{result \textless{}{-}}\StringTok{ }\KeywordTok{mclapply}\NormalTok{(}
    \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(params),}
    \ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{sim.boot.run}\NormalTok{(sim.name, params}\OperatorTok{$}\NormalTok{n[i], params}\OperatorTok{$}\NormalTok{p[i], params}\OperatorTok{$}\NormalTok{q[i]),}
    \DataTypeTok{mc.cores =}\NormalTok{ ncores)}
\end{Highlighting}
\end{Shaded}


  \bibliography{refs.bib}

\end{document}
