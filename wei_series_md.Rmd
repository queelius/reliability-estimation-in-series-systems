---
title: "Bootstrapping confidence intervals (BCa) of the maximum likelihood estimator of components in a series systems from masked failure data"
author: "Alex Towell"
abstract: "We estimate the parameters of a series system with Weibull component lifetimes from relatively small samples consisting of right-censored system lifetimes and masked component cause of failure. Under a set of conditions that permit us to ignore how the component cause of failures are masked, we assess the bias and variance of the estimator. Then, we assess the accuracy of the boostrapped variance and calibration of the confidence intervals of the MLE under a variety of scenarios."
output:
    bookdown::pdf_document2:
    #bookdown::html_document2:
    #bookdown::gitbook:
    #pdf_document:
        #toc: yes
        #toc_depth: 3
        number_sections: true
        #extra_dependencies: ["hyperref", "graphicx","amsthm","amsmath","natbib","tikz"]
        extra_dependencies: ["tikz"]
        df_print: kable
        keep_tex: true
        citation_package: natbib
indent: true
header-includes:
   - \AtBeginDocument{\renewcommand{\v}[1]{\boldsymbol{#1}}}
   - \AtBeginDocument{\newtheorem{condition}{Condition}}
   - \AtBeginDocument{\renewcommand{\refname}{References}}
bibliography: refs.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(md.tools)
library(algebraic.mle)
library(wei.series.md.c1.c2.c3)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(glue)
library(png)
library(kableExtra)

#\newtheorem{definition}{Definition}
#\newtheorem{theorem}{Theorem}
#\newtheorem{corollary}{Corollary}
#\numberwithin{equation}{section}

```

Introduction
============
Accurately estimating the reliability of individual components in
multi-component systems is an important problem in many engineering domains.
However, component lifetimes and failure causes are often not directly
observable. In a series system, only the system-level failure time may be recorded along
with limited information about which component failed. Such *masked* data poses challenges
for estimating component reliability. 

In this paper, we develop a maximum likelihood approach to estimate component reliability
in series systems using right-censored lifetime data and candidate sets that contain the
failed component. The key contributions are:

1. Deriving a likelihood model that accounts for right-censoring and masked failure causes
through candidate sets. This allows the available masked data to be used for estimation.

2. Validating the accuracy, precision, and robustness of the maximum likelihood estimator
through an extensive simulation study under different sample sizes, masking probabilities,
and censoring levels. 

3. Demonstrating that bootstrapping provides well-calibrated confidence intervals for the
MLEs even with small samples.

Together, these contributions provide a statistically rigorous methodology for learning
about latent component properties from series system data. The methods are shown to work
well even when failure information is significantly masked. This capability expands the
range of applications where component reliability can be quantified from limited observations.

The remainder of this paper is organized as follows. First, we detail the series system and
masked data models. Next, we present the likelihood construction and maximum likelihood
theory. We then describe the bootstrap approach for variance and confidence interval
estimation. Finally, we validate the methods through simulation studies under various data
scenarios and sample sizes.


Series System Model {#sec:statmod}
==================================
Consider a system composed of $m$ components arranged in a series configuration.
Each component and system has two possible states, functioning or failed.
We have $n$ systems whose lifetimes are independent and identically distributed (i.i.d.).
The lifetime of the $i$\textsuperscript{th} system denoted by the random variable $T_{i}$.
The lifetime of the $j$\textsuperscript{th} component in the $i$\textsuperscript{th}
system is denoted by the random variable $T_{i j}$.
We assume the component lifetimes in a single system are statistically independent and non-identically distributed.
Here, lifetime is defined as the elapsed time from when the new, functioning component
(or system) is put into operation until it fails for the first time.
A series system fails when any component fails, thus the lifetime of the $i$\textsuperscript{th}
system is given by the component with the shortest lifetime,
$$
    T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
$$

There are three particularly important distribution functions in reliability analysis: the
reliability function, the probability density function, and the hazard function.
The reliability function, $R_{T_i}(t)$, is the
probability that the $i$\textsuperscript{th} system has a lifespan larger than a duration $t$,
\begin{equation}
R_{T_i}(t) = \Pr\{T_i > t\}\\
\end{equation}
The probability density function (pdf) of $T_i$ is denoted by
$f_{T_i}(t)$ and may be defined as
$$
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
$$
Next, we introduce the hazard function.
The probability that a failure occurs between $t$ and $\Delta t$ given that no
failure occurs before time $t$ is given by
$$
\Pr\{T_i \leq t+\Delta t|T_i > t\} = \frac{\Pr\{t < T_i < t+\Delta t\}}{\Pr\{T_i > t\}}.
$$
The failure rate is given by the dividing this equation by the length of the time
interval, $\Delta t$:
$$
\frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}} =
    \frac{R_{T_i}(t) - R_{T_i}(t+\Delta t)}{R_{T_i}(t)}.
$$
The hazard function $h_{T_i}(t)$ for $T_i$ is the instantaneous failure rate at time $t$, which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_{T_i}(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}}\\
       &= \frac{f_{T_i}(t)}{R_{T_i}(t)}.
\end{split}
\end{equation}
\end{definition}

The lifetime of the $j$\textsuperscript{th} component is assumed to follow a parametric distribution indexed
by a parameter vector $\v{\theta_j}$. The parameter vector of the overall system is defined as
$$
    \v\theta = (\v{\theta_1},\ldots,\v{\theta_m}).
$$

When a random variable $X$ is parameterized by a particular $\v\theta$, we denote the
reliability function by $R_X(t;\v\theta)$, and the same for the other distribution functions.
As a special case, for the components in the series system, we subscript by their labels, e.g,
the $j$\textsuperscript{th} component's pdf is denoted by $f_j(t;\v{\theta_j})$.

Two random variables $X$ and $Y$ have a joint pdf $f_{X,Y}(x,y)$.
Given the joint pdf $f(x,y)$, the marginal pdf of $X$ is given by
$$
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
$$
where $\mathcal{Y}$ is the support of $Y$. (If $Y$ is discrete, replace
the integration with a summation over $\mathcal{Y}$.)

The conditional pdf of $Y$ given $X=x$, $f_{Y|X}(y|x)$, is defined as
$$
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
$$
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of $X_1,\ldots,X_m$ is denoted by $f(x_1,\ldots,x_m)$.

Next, we dive deeper into these concepts and provide mathematical derivations for
the reliability function, pdf, and hazard function of the series system.
We begin with the reliability function of the series system, as given by the following theorem.
\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_i > t\}
$$
which may be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\v\theta) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_reliability_function} shows that the system's overall reliability is the product of the reliabilities of its individual components. This property is inherent to series systems and will be used in the subsequent derivations.

Next, we turn our attention to the pdf of the system lifetime, described in the following theorem.
\begin{theorem}
\label{thm:sys_pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f_{T_i}(t;\v\theta) = \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f_{T_i}(t;\v\theta) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f_{T_i}(t;\v\theta)
    &= -\frac{d}{dt} R_1(t;\v{\theta_1})\prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j})\\
    &= f_1(t;\v\theta) \prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f_{T_i}(t;\v\theta) = \sum_{j=1}^{m-1} f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\v{\theta_j}) \frac{d}{dt} R_m(t;\v{\theta_m}),
$$
which simplifies to
$$
f_{T_i}(t;\v\theta)= \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}).
$$
\end{proof}
Theorem \ref{thm:sys_pdf} shows the pdf of the system lifetime as a function of the pdfs and reliabilities of its components.

We continue with the hazard function of the system lifetime, defined in the next theorem.
\begin{theorem}
\label{thm:sys_failure_rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h_{T_i}(t;\v\theta) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By Equation \eqref{eq:failure_rate}, the $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)}.
$$
Plugging in expressions for these functions results in
$$
  h_{T_i}(t;\v\theta) = \frac{\sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k})}
      {\prod_{j=1}^m R_j(t;\v{\theta_j})},
$$
which can be simplified to
$$
h_{T_i}(t;\v\theta) = \sum_{j=1}^m \frac{f_j(t;\v{\theta_j})}{R_j(t;\v{\theta_j})} = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_failure_rate} reveals that the system's hazard function is the sum of the hazard functions of its components.

By definition, the hazard function is the ratio of the pdf to the reliability
function,
$$
h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)},
$$
and we can rearrange this to get
\begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f_{T_i}(t;\v\theta) &= h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta)\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\v{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\v{\theta_j}) \biggr\},
\end{split}
\end{equation}
which we sometimes find to be a more convenient form than Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's reliability function, pdf, and hazard function. Next, we build upon these concepts to derive distributions related to the component cause of failure.

## Component Cause of Failure {#sec:comp_cause}
Whenever a series system fails, precisely one of the components is the cause.
We model the component cause of the series system failure as a random variable.
\begin{definition}
The component cause of failure of a series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}

The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.
\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) \prod_{l=1}^m R_l(t;\v\theta),
\end{equation}
where $h_j(t;\v{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_{T_i}(t;\v\theta)$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a series system with $3$ components.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\v\theta) = \prod_{j=1}^{3} f_j(t;\v{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\v\theta) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2}) f_3(t_3;\v{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2})
        R_3(t;\v{\theta_3}) dt_2\\
     &= f_1(t;\v{\theta_1}) R_2(t;\v{\theta_2}) R_3(t_1;\v{\theta_3}).
\end{align*}
Since $h_1(t;\v{\theta_1}) = f_1(t;\v{\theta_1}) / R_1(t;\v{\theta_1})$,
$$
f_1(t;\v{\theta_1}) = h_1(t;\v{\theta_1}) R_1(t;\v{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\v\theta)$
yields
$$
f_{K_i,T_i}(j,t;\v\theta) = h_1(t;\v{\theta_1}) \prod_{l=1}^m R_l(t;\v{\theta_l})\\
$$
Generalizing from this completes the proof.
\end{proof}

The probability that the $j$\textsuperscript{th} component is the cause of failure
is given by
\begin{equation}
\label{eq:prob_k}
\Pr\{K_i = j\} = E_{\v\theta}
\biggl[
    \frac{h_j(T_i;\v{\theta_j})}
         {\sum_{l=1}^m h_l(T_i ; \v{\theta_l})}
\biggr].
\end{equation}
\begin{proof}
The probability the $j$\textsuperscript{th} component is the cause of failure is given by
marginalizing the joint pdf of $K_i$ and $T_i$ over $T_i$,
$$
\Pr\{K_i = j\} = \int_0^{\infty} f_{K_i,T_i}(j,t;\v\theta) dt.
$$
By Theorem \ref{thm:f_k_and_t}, this is equivalent to
\begin{align*}
\Pr\{K_i = j\}
    &= \int_0^{\infty} h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta) dt\\
    &= \int_0^{\infty} \biggl(\frac{h_j(t;\v{\theta_j})}{h_{T_i}(t ; \v\theta)}\bigr) f_{T_i}(t ; \v\theta) dt\\
    &= E_{\v\theta}\bigl[h_j(T_i;\v{\theta_j}) / \sum_{l=1}^m h_l(T_i ; \v{\theta_l})\bigr].
\end{align*}
\end{proof}

## System and Component Reliabilities {#sec:reliability}

A common measure of reliability is mean time to failure (MTTF). The
MTTF is defined as the expectation of the lifetime,
\begin{equation}
\label{mttf-sys}
\text{MTTF} = E_{\v\theta}\{T_i\},
\end{equation}
which if certain assumptions are satisfied\footnote{$T_i$ is non-negative and continuous, $R_{T_i}(t;\v\theta)$ is a
well-defined, continuous, and differential function for $t > 0$,
and $\int_0^\infty R_{T_i}(t;\v\theta) dt$ converges.}
is equivalent to the integration of the reliability function over its support.

While the MTTF provides a summary measure of reliability, it is not a complete description.
Depending on the failure characteristics, MTTF can be misleading. For example,
a system that has a high likelihood of failing early in its life may still have a
large MTTF if it is fat-tailed.\footnote{A "fat-tailed" distribution refers to a probability
distribution with tails that decay more slowly than those of the exponential family,
such as the case with the Weibull when its shape parameter is greater than $1$. This means
that extreme values are more likely to occur, and the distribution is more prone to
"black swan" events or rare occurrences. In the context of reliability, a fat-tailed
distribution might imply a higher likelihood of unusually long lifetimes, which can
skew measures like the MTTF. See, for example, \cite{taleb2007black}.}

The reliability of the components in the series system determines the reliability
of the system. We denote the MTTF of the $j$\textsuperscript{th} component by
$\text{MTTF}_j$ and, according to Equation \eqref{eq:prob_k}, the probability
that the $j$\textsuperscript{th} component is the cause of failure is given by
$\Pr\{K_i = j\}$. In a well-designed series system, there is no component that is
the "weakest link" that either has a much shorter MTTF or a much higher
probability of being the component cause of failure than any of
the other components, e.g., $\Pr\{K_i = j\} \approx \Pr\{K_i = k\}$ and
and MTTF$_j \approx$ MTTF$_k$ for all $j$ and $k$. This just means that the components
should have similar reliabilities and failure characteristics.

We use these results in the simulation study in Section \ref{sec:sim_study}, where we
assess the sensitivity of the MLE with respect to varying the reliability
of one of the Weibull components. We vary its reliability in two different ways:

1. We vary its shape parameter (keeping its scale parameter constant), which determines
   the failure characteristics of the component and also affects its MTTF.

2. We vary its scale parameter (keeping its shape parameter constant), which scales
   its MTTF while retaining the same failure characteristics.

Likelihood Model for Masked Data {#sec:like_model}
==================================================

The object of interest is the (unknown) parameter value $\v\theta$. 
To estimate this $\v\theta$, we need *data*.
In our case, we call it *masked data* because we do not necessarily observe
the event of interest, say a system failure, directly.
We consider two types of masking: masking the system failure lifetime and
masking the component cause of failure.

We generally encounter three types of system failure lifetime masking:

1. A system failure is observed at a particular point in time.
2. A system failure is observed to occur within a particular interval of time.
3. A system failure is not observed, but we know that the system survived at least
   until a particular point in time. This is known as *right-censoring*
   and can occur if, for instance, an experiment is terminated while the system
   is still functioning.


We generally encounter two types of component cause of failure masking:

1. The component cause of failure is observed.
2. The component cause of failure is not observed, but we know that the failed
   component is in some set of components. This is known as *masking* the
   component cause of failure.

Thus, the component cause of failure masking will take the form of candidate sets. A
candidate set consists of some subset of component labels that plausibly contains the
label of the failed component.
The sample space of candidate sets are all subsets of $\{1,\ldots,m\}$, thus
there are $2^m$ possible outcomes in the sample space.

In this paper, we limit our focus to observing *right censored* lifetimes and exact
lifetimes but with masked component cause of failures.
We consider a sample of $n$ i.i.d. series systems, each of
which is put into operation at some time and and observed until either it fails
or is right-censored.
We denote the right-censoring time of the $i$\textsuperscript{th} system by
$\tau_i$. 
We do not directly observe the system lifetime, $T_i$, but rather, we observe
the right-censored lifetime, $S_i$, which is given by
\begin{equation}
    S_i = \min\{\tau_i, T_i\},
\end{equation}
We also observe a right-censoring indicator, $\delta_i$, which is given by
\begin{equation}
    \delta_i = 1_{T_i < \tau_i}
\end{equation}
where $1_{\text{condition}}$ is an indicator function that outputs $1$ if
*condition* is true and $0$ otherwise.
Here, $\delta_i = 1$ indicates the event of interest, a system failure, was
observed.

If a system failure lifetime is observed, then we also observe a candidate set
that contains the component cause of failure. We denote the candidate set for
the $i$\textsuperscript{th} system by $\mathcal{C}_i$, which is a subset of
$\{1,\ldots,m\}$.
Since the data generating process for candidate sets may be subject to chance
variations, it as a random set.

Consider we have an independent and identically distributed (i.i.d.) random sample of masked data,
$D = \{D_1, \ldots, D_n\}$, where each $D_i$ contanis the following:

- $S_i$, the system lifetime of the $i$\textsuperscript{th} system.
- $\delta_i$, the right-censoring indicator of the $i$\textsuperscript{th} system.
- $\mathcal{C}_i$, the set of candidate component causes of failure for the
  $i$\textsuperscript{th} system.

The masked data generation process is illustrated by Figure \ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$D_i = (S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data $D$ for exact, right-censored system failure times with
candidate sets that mask the component cause of failure can be seen in Table 1
for a series system with $m=3$ components.

System | Right-censoring time ($S_i$) | Right censoring indicator ($\delta_i$) | Candidate set ($\mathcal{C}_i$) |
------ | --------------------------- | --------------------------- | --------------------- |
   1   | $4.3$                       | 1                           | $\{1,2\}$             |
   2   | $1.3$                       | 1                           | $\{2\}$               |
   3   | $5.4$                       | 0                           | $\emptyset$           |
   4   | $2.6$                       | 1                           | $\{2,3\}$             |
   5   | $3.7$                       | 1                           | $\{1,2,3\}$           |
   6   | $10$                        | 0                           | $\emptyset$           |

: Right-censored lifetime data with masked component cause of failure.

In our model, we assume the data is governed by a pdf, which is determined by
a specific parameter, represented as $\v\theta$ within the parameter space $\v\Omega$.
The joint pdf of the data $D$ can be represented as follows:
$$
f(D ; \v\theta) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\v\theta),
$$
where $s_i$ is the observed system lifetime of the $i$\textsuperscript{th} system,
$\delta_i$ is the observed right-censoring indicator of the $i$\textsuperscript{th} system,
and $c_i$ is the observed candidate set of the $i$\textsuperscript{th} system.

This joint pdf tells us how likely we are to observe the particular data, $D$, given
the parameter $\v\theta$. When we keep the data constant and allow the parameter
$\v\theta$ to vary, we obtain what is called the likelihood function $L$, defined as
$$
L(\v\theta) = \prod_{i=1}^n L_i(\v\theta)
$$
where
$$
L_i(\v\theta) = f(s_i,\delta_i,c_i;\v\theta)
$$
is the likelihood contribution of the $i$\textsuperscript{th} system. In other words,
the likelihood function quantifies how likely different parameter values $\v\theta$
are, given the observed data.

For each type of data, right-censored data and masked component cause of
failure data, we will derive the *likelihood contribution* $L_i$, which refers to the
part of the likelihood function that this particular piece of data contributes to.

We present the following theorem for the likelihood contribution model.
\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\v\theta) = R_{T_i}(s_i;\v\theta) \biggl(\beta_i \sum_{j \in c_i} h_j(s_i;\v{\theta_j}) \biggr)^{\delta_i}
\end{equation}
where $R_{T_i}(s_i;\v\theta) = \prod_{j=1}^m R_j(s_i;\v{\theta_j})$
is the reliability function of the series system evaluted at $s_i$, $\delta_i = 0$ indicates the
$i$\textsuperscript{th} system is
right-censored at time $s_i$, and $\delta_i = 1$ indicates the $i$\textsuperscript{th} system
is observed to have failed at time $s_i$ with a component cause of failure
is masked by the candidate set $c_i$.
\end{theorem}

In the follow subsections, we prove this result for each type of masked data, right-censored
system lifetime data $(\delta_i = 0)$ and masking of the component cause of failure
$(\delta_i = 1)$.

## Masked Component Cause of Failure {#sec:candmod}
Suppose a diagnostician is unable to identify the precise component cause of the
failure, e.g., due to cost considerations he or she replaced multiple components
at once, successfully repairing the system but failing to precisely identity
the failed component.
In this case, the cause of failure is said to be *masked*.

The unobserved component lifetimes may have many covariates, like ambient
operating temperature, but the only covariate we observe in our masked data
model are the system's lifetime and additional masked data in the form of
a candidate set that is somehow correlated with the unobserved component
lifetimes.

The key goal of our analysis is to estimate the parameters, $\v{\theta}$, which 
maximize the likelihood of the observed data, and to estimate the precision and
accuracy of this estimate using the Bootstrap method.

To achieve this, we first need to
assess the joint distribution of the system's continuous lifetime, $T_i$, and the
discrete candidate set, $\mathcal{C}_i$, which can be written as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = f_{T_i}(t_i;\v{\theta})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\},
$$
where $f_{T_i}(t_i;\v{\theta})$ is the pdf of $T_i$ and
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$ is the conditional
pmf of $\mathcal{C}_i$ given $T_i = t_i$.

We assume the pdf $f_{T_i}(t_i;\v{\theta})$ is known, but we do not have knowledge
of $\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$, i.e., the data generating
process for candidate sets is unknown.

However, it is critical that the masked data, $\mathcal{C}_i$, is correlated with the
$i$\textsuperscript{th} system. This way, the conditional distribution of $\mathcal{C}_i$
given $T_i = t_i$ may provide information about $\v{\theta}$, despite our Statistical
interest being primarily in the series system rather than the candidate sets.

To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which $\mathcal{C}_i$ is correlated with the
$i$\textsuperscript{th} system is given by assuming the following condition.
\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}
Assuming Condition \ref{cond:c_contains_k}, $\mathcal{C}_i$ must contain the
index of the failed component, but we can say little else about what other
component indices may appear in $\mathcal{C}_i$.

In order to derive the joint distribution of $\mathcal{C}_i$ and $T_i$ assuming
Condition \ref{cond:c_contains_k}, we take the following approach.
We notice that $\mathcal{C}_i$ and $K_i$ are statistically dependent.
We denote the conditional pmf of $\mathcal{C}_i$ given $T_i = t_i$ and
$K_i = j$ as
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$

Even though $K_i$ is not observable in our masked data model, we can still
consider the joint distribution of $T_i$, $K_i$, and $\mathcal{C}_i$.
By Theorem \ref{thm:f_k_and_t}, the joint pdf of $T_i$ and $K_i$ is given by
$$
f_{T_i,K_i}(t_i,j;\v{\theta}) = h_j(t_i;\v{\theta_j}) R_{T_i}(t_i;\v\theta),
$$
where $h_j(t_i;\v{\theta_j})$ is the hazard function for the
$j$\textsuperscript{th} component and $R_{T_i}(t_i;\v{\theta})$ is the
reliability function of the system.
Thus, the joint pdf of $T_i$, $K_i$, and $\mathcal{C}_i$ may be written as
\begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta})
    &= f_{T_i,K_i}(t_i,k;\v{\theta}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\v{\theta_j}) R_{T_i}(t_i;\v\theta)
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation}
We are going to need the joint pdf of $T_i$ and $\mathcal{C}_i$, which
may be obtained by summing over the support $\{1,\ldots,m\}$ of $K_i$ in
Equation \eqref{eq:joint_pdf_t_k_c},
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{T_i}(t_i;\v\theta)
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
$$
By Condition \ref{cond:c_contains_k},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0$ when $K_i = j$ and
$j \notin c_i$, and so we may rewrite the joint pdf of $T_i$ and
$\mathcal{C}_i$ as
\begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of $\v\theta$ (see Section \ref{sec:mle}), we
solve the simultaneous equations of the MLE and choose a solution
$\hat{\v\theta}$ that is a maximum for the likelihood function.
When we do this, we find that $\hat{\v\theta}$ depends on the unknown
conditional pmf $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}
According to [@Fran-1991], in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In this
setting, Condition \ref{cond:equal_prob_failure_cause} generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$ may be factored out of the
summation in Equation \eqref{eq:part1}, and thus the joint pdf of $T_i$ and
$\mathcal{C}_i$ may be rewritten as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j})
$$
where $j' \in c_i$.

If $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ is a function of
$\v{\theta}$, the MLEs are still dependent on the unknown
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ to find an MLE of
$\v{\theta}$. The last condition we assume achieves this result.
\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $T_i$ and component cause
of failure $K_i$ are not functions of $\v{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\v\theta$.
\end{condition}

When Conditions \ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause},
and \ref{cond:masked_indept_theta} are satisfied, the joint pdf of $T_i$ and
$\mathcal{C}_i$ is given by
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \beta_i R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
When we fix the sample and allow $\v{\theta}$ to vary, we obtain the
contribution to the likelihood $L$ from the $i$\textsuperscript{th} observation
when the system lifetime is exactly known (i.e., $\delta_i = 1$) but the
component cause of failure is masked by a candidate set $c_i$:
\begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\v\theta) = R_{T_i}(t_i;\v\theta) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}, 
if we observe an exact system failure time for the $i$-th system ($\delta_i = 1$),
but the component that failed is masked by a candidate set $c_i$, then its likelihood
contribution is given by Equation \eqref{eq:likelihood_contribution_masked}.

## Right-Censored Data

As described in Section \ref{sec:like_model}, we observe realizations of
$(S_i,\delta_i,\mathcal{C}_i)$ where $S_i = \min\{T_i,\tau_i\}$ is the
right-censored system lifetime, $\delta_i = 1_{\{T_i < \tau_i\}}$ is
the right-censoring indicator, and $\mathcal{C}_i$ is the candidate set.

In the previous section, we discussed the likelihood contribution from an
observation of a masked component cause of failure, i.e., $\delta_i = 1$. 
We now derive the likelihood contribution of a *right-censored* observation
$(\delta_i = 0$) in our masked data model.
\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\v\theta) = R_{T_i}(s_i;\v\theta).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau_i$, and we only know that
$T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\v\theta) = \Pr\!{}_{\v\theta}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\v\theta) = R_{T_i}(s_i;\v\theta).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the likelihood
contribution for the $i$\textsuperscript{th} system shown in Theorem
\ref{thm:likelihood_contribution},
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$

We use this result in Section \ref{sec:mle} to derive the maximum likelihood
estimator (MLE) of $\v\theta$.

## Identifiability and Convergence Issues

In our likelihood model, masking and right-censoring can lead to issues related
to identifiability and flat likelihood regions.
Identifiability refers to the unique mapping of the model parameters to the
likelihood function, and lack of identifiability can lead
to multiple sets of parameters that explain the data equally well, making inference
about the true parameters challenging \citep{lehmann1998theory}, while 
flat likelihood regions can complicate convergence \citep{wu1983convergence}.

In our simulation study, we address these challenges in a pragmatic way. Specifically,
failure to converge to a solution within a maximum of 125 iterations is interpreted as
evidence of the aforementioned issues, leading to the discarding of the sample, with
the process then repeated with a new synthetic sample. Note, however, that in Section
\ref{sec:boot} where we discuss the bias-corrected and accelerated (BCa) bootstrap
method for constructing confidence intervals, we do not discard any resamples.

This strategy helps ensure the robustness of the results, while acknowledging the
inherent complexities of likelihood-based estimation in models characterized by
masking and right-censoring.

Maximum Likelihood Estimation {#sec:mle}
========================================
In our analysis, we use maximum likelihood estimation (MLE) to estimate the series
system parameter $\v\theta$ from the masked data \citep{bain, casella2002statistical}.
The MLE finds parameter values that maximize the likelihood of the observed data
under the assumed model. A maximum likelihood estimate, $\hat{\v\theta}$, is a
solution of
\begin{equation}
\label{eq:mle}
L(\hat{\v\theta}) = \max_{\v\theta \in \v\Omega} L(\v\theta),
\end{equation}
where $L(\v\theta)$ is the likelihood function of the observed data. For computational
efficiency and analytical simplicity, we work with the log-likelihood function,
denoted as $\ell(\v\theta)$, instead of the likelihood function \citep{casella2002statistical}.
\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\v\theta)$, for our masked data model is the sum of the log-likelihoods for each observation,
\begin{equation}
\label{eq:loglike}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta),
\end{equation}
where $\ell_i(\v\theta)$ is the log-likelihood contribution for the $i$\textsuperscript{th} observation:
\begin{equation}
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) +
    \delta_i \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \bigr).
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is the logarithm of the likelihood function,
$$
\ell(\v\theta) = \log L(\v\theta) = \log \prod_{i=1}^n L_i(\v\theta) = \sum_{i=1}^n \log L_i(\v\theta).
$$

Substituting $L_i(\v\theta)$ from Equation \eqref{eq:like}, we consider these two cases of $\delta_i$
seperately to obtain the result in Theorem \ref{thm:loglike_total}.

\textbf{Case 1}: If the $i$-th system is right-censored ($\delta_i = 0$),
$$
\ell_i(\v\theta) = \log R_{T_i}(s_i;\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the failure time is known ($\delta_i = 1$),
$$
\ell_i(\v\theta) = \log R_{T_i}(s_i;\v\theta) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j})\bigr).
$$
We replace $R_{T_i}(s_i;\v\theta)$ with its component-wise definition and by Condition \ref{cond:masked_indept_theta},
we may discard\footnote{Adding or substracting a function by a constant does not change where it obtains a maximum, so we
are free to discard such terms from the log-likelihood function.} the $\log \beta_i$ term since it does not depend on
$\v\theta$, giving us the result
$$
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \biggr).
$$

Combining these two cases gives us the result in Theorem \ref{thm:loglike_total}.
\end{proof}

The MLE, $\hat{\v\theta}$, is often found by solving a system of equations derived from setting the derivative of the log-likelihood function to zero, i.e.,
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0,
\end{equation}
for each component $\theta_j$ of the parameter $\v\theta$ \citep{bain}. When there's no closed-form solution,
we resort to numerical methods like the Newton-Raphson method.

Assuming some regularity conditions, such as the likelihood function being identifiable, the MLE has many desirable
asymptotic properties that underpin statistical inference, namely that it is an asymptotically unbiased estimator
of the parameter $\v\theta$ and it is normally distributed with a variance given by the inverse of the Fisher
Information Matrix (FIM) \citep{casella2002statistical}.
However, for smaller samples, these asymptotic properties may not yield accurate approximations. We propose to use
the bootstrap method to offer an empirical approach for estimating the sampling distribution of the MLE, in particular for
computing confidence intervals.

Bias-Corrected and Accelerated Bootstrap Confidence Intervals {#sec:boot}
===============================================================
We utilize the non-parametric bootstrap to approximate the sampling distribution of
the MLE. In the non-parametric bootstrap, we resample from the observed data
with replacement to generate a bootstrap sample. The MLE is then computed for
the bootstrap sample. This process is repeated $B$ times, giving us $B$ bootstrap
replicates of the MLE. The sampling distribution of the MLE is then approximated
by the empirical distribution of the bootstrap replicates of the MLE.

The method we use to generate confidence intervals is known
as Bias-Corrected and Accelerated Bootstrap Confidence Intervals (BCa), which
applies two corrections to the standard bootstrap method:

- Bias correction: This adjusts for bias in the bootstrap distribution itself.
  This bias is measured as the difference between the mean of the bootstrap distribution and the observed statistic.
  It works by transforming the percentiles of the bootstrap distribution to correct for these issues.
  
  This may be a useful transformation in our case since we are dealing with small samples and we have two potential
  sources of bias: right-censoring and masking component cause of failure. They seem to have opposing effects
  on the MLE, but the relationship is difficult to quantify.

- Acceleration: This adjusts for the rate of change of the statistic as a function of the true, unknown parameter.
  This correction is important when the shape of the statistic's distribution changes with the true parameter.

  Since we have a number of different shape parameters, $k_1,\ldots,k_m$, we may expect the shape of the
  distribution of the MLE to change as a function of the true parameter, making this correction potentially useful.

Since we are primarly interested in generating confidence intervals for small samples for a
potentially biased MLE, the BCa method may be a good choice for our analysis. For more details
on BCa, see \cite{efron1987better}.

In our simulation study, we will assess the performance of the bootstrapped
confidence intervals by computing the coverage probability of the confidence
intervals. A well-calibrated 95% confidence interval contains the true
value around 95% of the time. If the confidence interval is too narrow, it will have
a coverage probability less than 95%, which conveys a sort of false confidence
in the precision of the MLE. If the confidence interval is too wide, it will
have a coverage probability greater than 95%, which conveys a lack of confidence
in the precision of the MLE. We want confidence intervals to be as
narrow as possible while still having a coverage probability close to the
nominal level, 95%.


## Additional Performance Metrics
- *Dispersion of MLEs*: The shaded regions representing the 95% probability range of the MLEs
                        get narrower as the sample size increases. This is an indicator of
                        the increased precision in the estimates as more data is available.
                        We call it a *Confidence Band*, but it is actually an estimate of the
                        quantile range of the MLEs. The shaded region provides insight into
                        the distribution of the MLEs.

- *IQR of Bootstrapped CIs*: The vertical blue bars represent the Interquartile Range (IQR)
                             of the actual bootstrapped Confidence Intervals (CIs). Since in
                             practice we only have one sample and consequently one MLE, we
                             use bootstrapping to resample and compute multiple CIs. The IQR
                             then represents the middle 50% range of these bootstrapped CIs.

- *Mean of the MLEs*: The mean of the MLEs is a good indicator of the bias in the estimates.
                      If the mean of the MLEs is close to the true value, then the MLEs are,
                      on average, unbiased.

The distinction between the shaded region (95% range of MLEs) and the blue vertical bars
(IQR of bootstrapped CIs) is important. The shaded region provides insight into the
distribution of the MLEs, whereas the blue vertical bars provide information about the
variation in the bootstrapped CIs. Both are relevant for understanding the behavior of the
estimations.


## Issues with Resampling from the Observed Data

While the bootstrap method provides a robust and flexible tool for statistical
estimation, its effectiveness can be influenced by several factors
\citep{efron1994introduction}.

Firstly, instances of non-convergence in our bootstrap samples were observed.
Such cases can occur when the estimation method, like the MLE used in our
analysis, fails to converge due to the specifics of the resampled data
\citep{casella2002statistical}. This issue can potentially introduce bias or
reduce the effective sample size of our bootstrap distribution.

Secondly, the bootstrap's accuracy can be compromised with small sample sizes,
as the method relies on the law of large numbers to approximate the true sampling
distribution. For small datasets, the bootstrap samples might not adequately
represent the true variability in the data, leading to inaccurate results
\citep{efron1994introduction}.

Thirdly, our data involves right censoring and a masking of the component cause
of failure when a system failure is observed. These aspects can cause certain data points or
trends to be underrepresented or not represented at all in our data, introducing
bias in the bootstrap distribution \citep{klein2005survival}.

Despite these challenges, we found the bootstrap method useful in approximating
the sampling distribution of the MLE, taking care in interpreting the results,
particularly as it relates to coverage probabilities.

Series System with Weibull Components {#sec:weibull}
====================================================

The Weibull distribution, introduced by Waloddi Weibull in 1937, has been
instrumental in reliability analysis due to its ability to model a wide range
of failure behaviors. Reflecting on its utility, Weibull
modestly noted that it "[...] may sometimes render good service."
(Source: "New Weibull Handbook"). In the context of our study, we utilize the Weibull
to model a system as originating from Weibull components in a series configuration,
producing a specific form of the likelihood model described in Section \ref{sec:like_model},
which deals with challenges such as right censoring and masked component cause of failure.
In Section \ref{sec:sim_study}, we conduct a simulation study to assess the sensivity
of this model to variations in masking probabilities, sample size, and differentiated
component reliabilities.

The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j) \qquad \text{for } i = 1,\ldots,n \text{ and } j = 1,\ldots,m,
$$
where $\lambda_j > 0$ is the scale parameter and $k_j > 0$ is the shape parameter.
The $j$\textsuperscript{th} component has a reliability function, pdf, and hazard function
given respectively by
\begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}.
\end{align}

The shape parameter of the Weibull distribtion is of particular importance:

- $k_j < 1$ indicates infant mortality. An example of how this might arise is
a result of defective components being weeded out early, and the remaining
components surviving for a much longer time.
- $k_j = 1$ indicates random failures (independent of age). An example of how
this might arise is a result of random shocks to the system, but otherwise
the system is age-independent.\footnote{The exponential distribution is a special
case of the Weibull distribution when $k_j = 1$.}
- $k_j > 1$ indicates wear-out failures. An example of how this might arise is a
result of components wearing as they age

The lifetime of the series system composed of $m$ Weibull components
has a reliability function given by
\begin{equation}
\label{eq:sys_weibull_reliability_function}
R_{T_i}(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}
\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R_{T_i}(t;\v\theta)
    &= \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}\\
    &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
\end{proof}

The Weibull series system's hazard function is given by
\begin{equation}
\label{eq:sys_weibull_failure_rate_function}
h_{T_i}(t;\v\theta) =
    \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{equation}
whose proof follows from Theorem \ref{thm:sys_failure_rate}.
The pdf of the series system is given by
\begin{equation}
\label{eq:sys_weibull_pdf}
f_{T_i}(t;\v\theta) =
\biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{equation}
\begin{proof}
By definition,
$$
f_{T_i}(t;\v\theta) = h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}

## Reliability

In Section \ref{sec:reliability}, we discussed the concept of reliability.
In the case of Weibull components, the MTTF of the $j$\textsuperscript{th}
component is given by
\begin{equation}
\label{mttf-weibull}
\text{MTTF}_j = \lambda_j \Gamma\biggl(1 + \frac{1}{k_j}\biggr),
\end{equation}
where $\Gamma$ is the gamma function.

We mentioned that the MTTF can sometimes be a poor measure of reliability, e.g.,
the MTTF and the probability of failing early can be large. The Weibull is a good
example of this phenomenon. If $k > 1$, the Weibull is a fat-tailed distribution,
and it can exhibit both a large MTTF and a high probability of failing early.

Components may have similar MTTFs, but some components may be more likely to fail
early and others may be more likely to fail late, depending upon their failure
characterstics (shape parameters), and so the probability of component failure given by
Equation \eqref{eq:prob_k} is a useful measure of component reliability compared to
the other components in the system.

In a well-designed series system, the component failure characteristics are similar:
they have a similar MTTF and a similar probability of being the component cause of
failure, i.e., they have similar shapes and scales, so that system failures are not
dominated by some subset of components.

## Likelihood Model {#sec:sys_weibull_likelihood}
In Section \ref{sec:like_model}, we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
$i$\textsuperscript{th} system is given by the following theorem.
\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\v\theta) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\v{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the following result.
\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\v\theta) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\v\theta$ since they do not
affect the MLE.
\end{corollary}

We find an MLE by solving \eqref{eq:mle_eq},
i.e., a point $\v{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)$ satisfying
$\nabla_{\theta} \ell(\v{\hat\theta}) = \v{0}$, where $\nabla_{\v\theta}$
is the gradient of the log-likelihood function (score) with respect to $\v\theta$.

To solve this system of equations, we use the Newton-Raphson method, which requires
the score and the Hessian of the log-likelihood function.
We analytically derive the score since it is useful to have for the Newton-Raphson
method, but we do not do the same for the Hessian of the log-likelihood for the following reasons:

1. The gradient is relatively easy to derive, and it is useful to have for
computing gradients efficiently and accurately, which will be useful for
numerically approximating the Hessian.

2. The Hessian is tedious and error prone to derive, and Newton-like methods
often do not require the Hessian to be explicitly computed.

The following theorem derives the score function.
\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\v\theta) = \biggl(
    \frac{\partial \ell_i(\v\theta)}{\partial k_1},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\v\theta)}{\partial k_m},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the log-likelihood
contribution of the $i$-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation so the proof
has been omitted, but the result has been verified by using a very precise numerical
approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions is
the sum of their gradients, and so the score function conditioned on the entire
sample is given by
\begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\v\theta) = \sum_{i=1}^n \nabla \ell_i(\v\theta).
\end{equation}


Reduced Model: Weibull Series System Due to Homogeneous Shape Parameters
========================================================================

A series system composed of Weibull components is not generally Weibull unless the
shape parameters of the components are homogeneous.
\begin{theorem}
If the shape parameters of the components are identical, then the series system is Weibull
with a shape parameter $k$ given by the shape parameter of the components and a scale
parameter $\lambda$ given by
\begin{equation}
\label{eq:sys_weibull_scale}
\lambda = \biggl(\sum_{j=1}^{m} \lambda_j^{-k}\biggr)^{-1/k},
\end{equation}
\end{theorem}
where $\lambda_j$ is the scale parameter of the $j$\textsuperscript{th} component.
\begin{proof}
Given $m$ Weibull lifetimes $T_{i 1}, \ldots, T_{i m}$ with the same shape parameter $k$
and scale parameters $\lambda_1, \ldots, \lambda_m$, the reliability function of the series
system is
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k}\biggr\}.
$$
To make this a Weibull system, we need to find a single scale parameter $\lambda$ such that
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\biggl(\frac{t}{\lambda}\biggr)^{k}\biggr\},
$$
which has the solution
$$
\lambda = \frac{1}{\left(\frac{1}{\lambda_1^k} + \ldots + \frac{1}{\lambda_m^k}\right)^{\frac{1}{k}}}.
$$
\end{proof}

\begin{theorem}
If a series system has Weibull components with identical shape parameters, the component
cause of failure is conditionally independent of the system failure time:
$$
    \Pr\{K_i = j | T_i = t_i \} = \Pr\{K_i = j\} = \frac{\lambda_j^{-k}}{\sum_{l=1}^{m} \lambda_l^{-k}}.
$$
\end{theorem}
\begin{proof}
The conditional probability of the $j$\textsuperscript{th} component being the
cause of failure given the system failure time is given by
\begin{align*}
\Pr\{K_i = j | T_i = t\}
    &= \frac{f_{K_i, T_i}(j, t;\v\theta)}{f_{T_i}(t;\v\theta)}
    = \frac{h_j(t;k,\lambda_j) R_{T_i}(t;\v\theta)}
        {h_{T_i}(t;\v{\theta_j}) R_{T_i}(t;\v\theta)}\\
    &= \frac{h_j(t;k,\lambda_j)}{\sum_{l=1}^m h_l(t;k,\lambda_l)}
    = \frac{\frac{k}{\lambda_j}\bigl(\frac{t}{\lambda_j}\bigr)^{k-1}}
        {\sum_{l=1}^m \frac{k}{\lambda_l}\bigl(\frac{t}{\lambda_l}\bigr)^{k-1}}
    = \frac{\bigl(\frac{1}{\lambda_j}\bigr)^k}
        {\sum_{l=1}^m \bigl(\frac{1}{\lambda_l}\bigr)^k}.
\end{align*}
\end{proof}


As we discussed in the previous section, if the shapes are similar, the series system
is approximately Weibull. In our simulation study, we analyze a series system with
Weibull components that have similar shape parameters (except in those studies where
we vary the shape parameters to test the sensitivity of the MLE to different component
failure characteristics).


 This reduces the
series system to a known distribution, the Weibull distribution, which is particularly useful
for analysis and interpretation, especially in the context of small samples, since instead of
estimating $2m$ parameters we only need to estimate $m+1$ parameters, one shape parameter and
$m$ scale parameters.

We denote the full model log-likelihood function by $\ell_F$ and the reduced model log-likelihood
by $\ell_R$. The reduced model is obtained by setting the shape parameter of each component to
be the same, i.e., $k_1 = \cdots = k_m = k$. Thus, the reduced model log-likelihood function is given by
$$
\ell_R(k, \lambda_1, \lambda_2, \cdots, \lambda_m) =
        \ell_F(k, \lambda_1, k, \lambda_2, \ldots, k, \lambda_m),
$$
The same is done for the score and hessian of the log-likelihood functions.

Simulation Study {#sec:sim_study}
================================


```{r sim-study-design, echo = F}
theta <- c(
    shape1 = 1.2576, scale1 = 994.3661,
    shape2 = 1.1635, scale2 = 908.9458,
    shape3 = 1.1308, scale3 = 840.1141,
    shape4 = 1.1802, scale4 = 940.1342,
    shape5 = 1.2034, scale5 = 923.1631
)

shapes <- theta[grepl("shape", names(theta))]
scales <- theta[grepl("scale", names(theta))]
```

In this section, we conduct a simulation study to assess the performance
of the MLE for the likelihood model defined in Section \ref{sec:weibull}.
In this simulation study, we assess the sensitivity of the MLE to
various simulation scenarios. In particular, we assess two important
properties of the MLE with respect to a scenario:

1. Accuracy (Bias): How close is the expected value of the MLE to the true
   parameter values? If the expected value of the MLE is close to the true
   parameter values, the accuracy is high.

2. Precision: How much does the MLE vary from sample to sample? We measure
   this by assessing the 95% confidence intervals (BCa, Bias-Corrected and
   accelerated). If the confidence intervals are both small and have good 
   coverage probability (the proportion of confidence intervals that contain 
   the true parameter values), then the MLE is precise.

We begin by specifying the parameters of the series system that will be
the central object of our simulation study. We consider the data in
@Huairu-2013, in which they study the reliability of a series system with
three components. They fit Weibull components in a series configuration to
the data, resulting in an MLE with shape and scale estimates given by the
first three components in Table \ref{tab:series-sys}. To make the model
slightly more complex, we add two more components to this series system,
with shape and scale parameters given by the last two components in Table
\ref{tab:series-sys}. We will refer to this system as the **base** system.

In Section \ref{sec:reliability}, we defined a well-designed series
system as one that consists of components with similar reliabilities, where we define
reliability in two ways, the mean time to failure (MTTF) and the probability that a
specific component will be the cause of failure. All things else being equal,
components with long MTTFs and with near uniform probability of being the component
cause of failure is preferrable, otherwise we have a weak link in the system.

The base system defined in Table \ref{tab:series-sys} satisfies this definition
of being a well-designed system. We see that there are no components that are
significantly less reliable than any of the others, component 1 being the most reliable
and component 3 being the least reliable. This is a result of the scales and shapes
being similar for each component. In addition, the shapes are larger than $1$, which
means components are unlikely to fail early.

```{r series-sys, table.attr = "style='width:50%;'", echo = F, fig.align = "center"}
mttf.sys <- integrate(function(t) {
    t * dwei_series(t,
        scales = scales,
        shapes = shapes
    )
}, lower = 0, upper = Inf)$value

mttf.sys <- round(mttf.sys, digits = 3)
mttf <- round(gamma(1 + 1 / shapes) * scales, digits = 3)
probs <- round(wei_series_cause(1L:5L, scales = scales, shapes = shapes), digits = 3)
tau <- qwei_series(p = 0.825, scales = scales, shapes = shapes)
surv.sys <- round(surv_wei_series(tau, scales = scales, shapes = shapes), digits = 3)
surv <- round(pweibull(tau, shape = shapes, scale = scales, lower.tail = FALSE), digits = 3)
components <- data.frame(
    Shape = shapes,
    Scale = scales,
    MTTF = mttf,
    Prob = probs,
    Survival = surv,
    row.names = paste("Component", 1:5)
)

components <- rbind(components, "Series System" = c(NA, NA, mttf.sys, NA, surv.sys))
names(components) <- gsub("\\.", " ", names(components))
names(components)[1] <- "Shape ($k_j$)"
names(components)[2] <- "Scale ($\\lambda_j$)"
names(components)[3] <- "MTTF$_j$"
names(components)[4] <- "$\\Pr\\{K_i = j\\}$"
names(components)[5] <- "$R_j(\\tau;k_j,\\lambda_j)$"
knitr::kable(
    components,
    caption = "Weibull Components in Series Configuration",
    escape = FALSE # Add this line to allow raw LaTeX
)
```



## Data Generating Process {#sec:data_gen}

In this section, we describe the data generating process for our simulation studies.
It consists of three parts: the series system, the candidate set model, and the
right-censoring model.

### Weibull Series System Lifetime {-}

We generate data from a Weibull series system with $m$ components.
As described in Section \ref{sec:weibull}, the $j$\textsuperscript{th} component
of the $i$\textsuperscript{th} system has a lifetime distribution given by
$$
    T_{i j} \sim \operatorname{WEI}(k_j, \lambda_j)
$$
and the lifetime of the series system composed of $m$ Weibull components
is defined as
$$
    T_i = \min\{T_{i 1}, \ldots, T_{i m}\}.
$$
To generate a data set, we first generate the $m$ component failure times,
by efficiently sampling from their respective distributions, and we then set
the failure time $t_i$ of the system to the minimum of the component failure times.

### Right-Censoring Model {-}
We employ a simple right-censoring model, where the right-censoring time
$\tau$ is fixed at some known value, e.g., an experiment is run for a fixed
amount of time $\tau$, and all systems that have not failed by the end of the
experiment are right-censored. The censoring time $S_i$ of the
$i$\textsuperscript{th} system is thus given by
$$
    S_i = \min\{T_i, \tau\}.
$$
So, after we generate the system failure time $T_i$, we generate the censoring
time $S_i$ by taking the minimum of $T_i$ and $\tau$.
In our simulation study, we parmaterize the right-censoring time $\tau$ by the
quantile $q = 0.825$ of the series system,
$$
    \tau = F_{T_i}^{-1}(q).
$$
This means that $82.5\%$ of the series systems are expected to fail before time $\tau$
and $17.5\%$ of the series are expected to be right-censored. To solve for the $82.5\%$
quantile of the series system, we define the function $g$ as
$$
g(\tau) = F_{T_i}(\tau;\v\theta) - q
$$
and find its root using the Newton-Raphson method. See Appendix \ref{app:series-quantile} for the R code that
implements this procedure.

### Masking Model for Component Cause of Failure {-}

We must generate data that satisfies the masking conditions described in
Section \ref{sec:candmod}.
There are many ways to satisfying the masking conditions. We choose the simplest
method, which we call the *Bernoulli candidate set model*. In this model, each
non-failed component is included in the candidate set with
a fixed probability $p$, independently of all other components and independently
of $\v\theta$, and the failed component is always included in the candidate set.
See Appendix \@ref(app:cand-model) for the R code that implements this model.


## Full Model Versus Reduced Model

We described the full model in Section \ref{sec:wei-series} and we described the reduced model
in Section \ref{sec:reduced-model}. In this section, we seek to ascertain when the reduced
model is appropriate. In the reduced model, series system to a known distribution, the Weibull
distribution, which is particularly useful for analysis and interpretation, especially in the
context of small samples, since instead of estimating $2m$ parameters we only need to estimate
$m+1$ parameters, one shape parameter and $m$ scale parameters.

All things else being equal, particularly for small samples, we prefer simpler models over
more complex models. The reduced model is simpler than the full model because it has fewer
parameters. However, we must be careful to ensure that the reduced model is not too simple.
If the reduced model is too simple, then it may not be able to adequately describe the data.
In this case, the full model is preferred. This is in keeping with the principle of Ockham's
razor, the simplest model that adequately describes the data is preferred.

### Likelihood Ratio Test
In order to determine if the reduced model is appropriate, we will conduct a hypothesis test.
We wish to conduct a hypothesis test to determine if there is statistically significant
evidence that the shape parameters are homogeneous:
\begin{align*}
H_0 &: \text{all shape parameters equal.}\\
H_1 &: \text{not all shape parameters are equal.}
\end{align*}
We will use the likelihood ratio test (LRT) to test this hypothesis. The LRT statistic is
given by
$$
\Lambda = -2 \log \frac{\ell_R}{\ell_F},
$$
where $\ell_R$ is the log-likelihood function of the null (reduced) model
and $\ell_F$ is the log-likelihood function of the full model. Under the null model,
the LRT statistic is asymptotically distributed chi-squared with $m-1$ degrees of freedom,
where $m$ is the number of components in the series system,
$$
\Lambda \sim \chi^2_{m-1}.
$$
If the LRT statistic is greater than the critical value of the chi-squared distribution with $m-1$
degrees of freedom, $\chi^2_{m-1, 1-\alpha}$, then we find the data to be incompatible with the null
hypothesis $H_0$, otherwise we find the data to be compatible with the null hypothesis $H_0$.

### Sample Size Analysis

We wish to determine the sample size required to detect a statistically significant difference in the
shape parameters of the components for our well-defined series system described in Section \ref{}.
We will use the LRT statistic to determine the sample size required to detect a difference
in the shape parameters of the components with a given power and significance level.

### Conclusion and Justification for Full Model
This analysis further illuminates the properties of the reduced model in relation to the full model. By
examining Weibull homogeneity through the LRT, we gain insights that enrich the overall understanding of
the model for well-designed series systems with Weibull components. We find that the reduced model is
appropriate for well-designed series systems with Weibull components with homogenous shapes for sample
sizes less than $n = 50$ in our DGP. For sample sizes greater than $n = 50$, the reduced model becomes
increasingly incompatible with the data. This is because the reduced model assumes that the shape
parameters of the components are equal, but in our DGP, the shape parameters are only approximately
equal, and these slight differences become more pronounced as the sample size increases. Therefore,
we conclude that the full model is preferred for well-designed series systems with Weibull components
with sample sizes greater than $n = 50$ in our DGP. We will conduct a simulation study where we largely
use sample sizes larger than $n = 50$, so we will use the full model in our simulation study.

## Simulation Scenarios

We define a simulation scenario to be some combination of $n$, $p$, $k_3$, and $\lambda_3$.
We are interested in choosing a small number of scenarios that are
representative of real-world scenarios and that are interesting to analyze.

Here is an outline of the simulation study analysis:

1. Fix a combination of simulation parameters to some value, and vary the remaining parameter, e.g.,
   fix $p$, $k_3$, $\lambda_3$, and vary the sample size $n$.

2. Simulate $R$ datasets from the Data Generating Process (DGP) described in
   Section \ref{sec:data_gen}.

3. For each of these $R = 500$ datasets, compute the MLE.

4. For each of these $R$ datasets, perform bootstrap resampling $B = 1000$ times to create
   a set of bootstrap samples (replicates).

5. Calculate the MLE for each of these bootstrap samples. This generates an empirical
   distribution of the MLE, which is used to construct a confidence interval for
   the MLE.

6. For each dataset, determine whether the true parameter value falls within the
   computed CI. Aggregate this information across all $R$ datasets to estimate the
   coverage probability of the CI and the bias of the MLE at that particular
   combination of simulation parameters.

Once we generate the data for a simulation scenario, we plot the results and discuss
the performance of the MLE estimator under the chosen scenario.
For how we run a simulation scenario, see Appendix \ref{app:sim-code}.

## Scenario: Assessing the Impact of Right-Censoring {#sec:effect-censoring}

In this scenario, we use the well-designed series system described in Section \ref{sec:sim_study},
Table \ref{tab:series-sys}, and we vary the right-censoring quantile ($q$) from $25\%$ (significant
right-censoring) to $100\%$ (no right-censoring), with a component cause of failure masking
probability of $21.5\%$ (moderate masking) and sample size $n = 100$ (moderate sample size).

```{r q-vs-stats-p215-shape, out.width='100%', cache = T, fig.cap=c("Right-Censoring Quantile vs MLE ($p = 0.215, n = 100$)","Right-Qensoring Quantile ($q$) vs MLEs"), fig.align="center", echo = F}
knitr::include_graphics("image/tau/plot-q-vs-mle.pdf")
```

### Analysis {-}

When a right-censoring event occurs, in order to increase the likelihood of the data, the MLE
is nudged in a direction that increases the probability of a right-censoring event at time $\tau$,
which is given by $R_{T_i}(t;\v\theta)$, representing a source of bias in the estimate.

To increase $R_{T_i}(\tau)$, we move in the direction (gradient) of these partial derivatives.
The partial derivatives of $R_{T_i}(\tau)$
are given by
\begin{align*}
\frac{\partial R_{T_i}(\tau)}{\partial \lambda_j} &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \frac{k_j}{\lambda_j},\\
\frac{\partial R_{T_i}(\tau)}{\partial k_j}       &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \left(\log \lambda_j - \log \tau\right),
\end{align*}
for $j = 1, \ldots, m$. We see that these partial derivatives are related to the score of a right-censored likelihood contribution in
Theorem \ref{thm:weibull_score}. Let us analyze these partial derivatives:

- As $\tau$ increases, $R_{T_i}(\tau;\v\theta)$ decreases, and so the effect right-censoring has on the MLE decreases. This is what we see in
  Figure \ref{fig:q-vs-stats-p215-shape}.

- The partial derivatives with respect to the scale parameters are always positive, so right-censoring positively bias the scale parameter
  estimates to make right-censoring events more likely. The more right-censoring, the more the positive bias. We see this in Figure \ref{fig:q-vs-stats-p215-shape},
  where the scale parameter bias decreases as we decrase the probability ($1-q$) of a right-censoring event.

- The partial derivative with respect to the shape parameter of the $j$\textsuperscript{th} component, $k_j$, is
  non-negative if $\lambda_j \geq \tau$ and otherwise negative. In our well-designed series system, the scale parameters
  are large compared to most of the right-censoring times for $\tau(q)$, so the MLE nudges the shape parameter estimates
  in a positive direction to increase the probability of a right-censoring event $R_{T_i}(\tau)$ at time $\tau$. We see this
  in Figure \ref{fig:q-vs-stats-p215-shape}, where the shape parameter estimates are positively biased for most of the quantiles
  $q$.
  
An alternative way to reason about the effect right-censoring has on the MLE is give by the
following. When a right-censoring event occurs, to make the estimates more compatible
with the right-censoring event, the estimate is nudged in a direction that increases the
MTTF of the components and decreases the probability of component
failures before the censoring time $\tau$. In the case of our well-designed system,
for most of the right-censoring quatiles $q$, this is accomplished by nudging the
shape and scale parameters in a positive direction, which causes the MLE to be positively
biased in the prescence of these censoring times. As the right-censoring quantile $q$
increases, the effect of right-censoring on the MLE decreases.

Here are several key observations:

- *Coverage Probability (CP)*: The CP is well-calibrated, obtaining a value near the
  nominal 95% level across different right-censoring quantiles. This suggests that the
  bootstrapped CIs will contain the true value of the parameters with the specified confidence
  level. The CIs are neither too wide nor too narrow.

- *Dispersion of MLEs*: The shaded regions representing the 95% probability range of
  the MLEs get narrower as the right-censoring quantile increases. This is an indicator of the
  increased precision in the estimates as more data is available due to decreased
  censoring.

- *IQR of Bootstrapped CIs*: The IQR (vertical blue bars) reduces with an increase in
  sample size. This suggests that the bootstrapped CIs are getting more consistent and
  focused around a narrower range with larger samples while maintaining a good coverage
  probability. As we get more data, the bootstrapped CIs are more likely to be closer
  to each other and the true value of the parameters.

  For small right-censoring quantiles (small right-censoring times), they are quite
  large, but to maintain well-calibrated CIs, this  was necessary. The estimator is quite
  sensitive to the data, and so the bootstrapped CIs are quite wide to account for this
  sensitivity when the sample contains insufficient information due to censoring.

- *Mean of MLEs*: The red dashed line indicating the mean of MLEs initially is quite biased,
  but quickly diminshes to neglible levels for scale parameters. The estimates for the shape
  parameters never reaches zero, but this is potentially due to masking. At a larger sample
  size, we anticipate the bias in the shape estimates would also decrease to zero.

## Scenario: Assessing the Impact of Sample Size {#sec:effect-samp-size}

In this scenario, we use the well-designed series system described in Section \ref{sec:sim_study},
Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
we fix the right-censoring quantile to $q = 0.825$ (moderate censoring), and we vary the sample
size $n$ from $50$ (small sample size) to $1000$ (very large sample size).

```{r samp-size-n-vs-stats-p215-scale, fig.cap=c("Sample Size vs MLEs ($p = 0.215, q = 0.825$)","Sample Size ($n$) vs MLEs"), fig.align="center", echo = F}
knitr::include_graphics("image/plot-n-vs-stats-p215-scale.pdf")
knitr::include_graphics("image/plot-n-vs-stats-p215-shape.pdf")
```

Here are several key observations:

- *Coverage Probability (CP)*: The CP is well-calibrated, obtaining a value near the
  nominal 95% level across different sample sizes. This suggests that the bootstrapped
  CIs will contain the true value of the shape parameter with the specified confidence
  level. The CIs are neither too wide nor too narrow.

- *Dispersion of MLEs*: The shaded regions representing the 95% probability range of
  the MLEs get narrower as the sample size increases. This is an indicator of the
  increased precision in the estimates as more data is available. 

- *IQR of Bootstrapped CIs*: The IQR (vertical blue bars) reduces with an increase in
  sample size. This suggests that the bootstrapped CIs are getting more consistent and
  focused around a narrower range with larger samples while maintaining a good coverage
  probability. As we get more data, the bootstrapped CIs are more likely to be closer
  to each other and the true value of the scale parameter.

  For small sample sizes, they are quite large, but to maintain well-calibrated CIs, this
  was necessary. The estimator is quite sensitive to the data, and so the bootstrapped
  CIs are quite wide to account for this sensitivity when the sample size is small and
  not necessarily representative of the true distribution.

- *Mean of MLEs for Scales*: The red dashed line indicating the mean of MLEs remains stable across
  different sample sizes and close to the true value, suggesting that the scale MLEs are, on
  average, reasonably unbiased.

- *Mean of MLEs for Shapes*: The red dashed line is the mean of shape MLEs. Unlike the scale MLEs,
  we see that for small samples, particularly less than $200$, we observe a significant
  amount of positive bias for shape MLEs. The MLE for the shape parameters in this
  scenario appear to be more sensitive to the data than the scale parameters.

## Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure {#sec:p-vs-mttf}

In this scenario, we use the well-designed series system described in Section \ref{sec:sim_study},
Table \ref{tab:series-sys}. We fix the sample size to $n = 90$ (reasonable sample size) and 
we fix the right-censoring quantile to $q = 0.825$, and we vary the masking probability 
from $p$ from $0.1$ (very slight masking the component cause of failure) to $0.85$
(extreme masking of the component cause of failure).

Since the effect was identical on each of the components, we only show the results for
the shape and scale parameters of component 1.

### Scale Parameter {-}

In Figures \ref{fig:masking-prob-vs-stats-scale}, we show the effect of the masking probability
$p$ on the MLE and the bootstrapped BCa confidence intervals for the scale parameter of component 1. At this
sample size, we see that the MLE is relatively unbiased for small $p$, but as $p$ increases,
the MLE becomes increasingly positively biased. We also see the confidence interval width seems stable
until the masking becomes significant at $p > 0.4$. The confidence intervals were well-calibrated
at all masking probabilities except $p = 0.85$, at which point the coverage probability was
only 77% despite the very wide CIs.

This Figure is consistent with the our analysis in Section \ref{sec:effect-masking}, where we indicated
we expected a positive bias on the scale parameter to push the component's MTTF estimate upwards.

### Shape Parameters {-}
In Figures \ref{fig:masking-prob-vs-stats-shape}, we show the effect of the masking probability $p$ on the
MLE and bootstrapped BCa confidence intervals for the shape parameter of component 1. We see almost the
exact same pattern as we did for the scale parameter. The MLE is relatively unbiased for small $p$, although
there is a slight positive bias (potentially due to the right-censoring effect). As $p$ increases past
$p = 0.55$, the MLE becomes increasingly positively biased. Again, the confidence intervals are
well-calibrated at all masking probabilities except $p = 0.85$, at which point the coverage probability was
around 65% despite the very wide CIs.

This Figure is also consistent with our analysis in Section \ref{sec:effect-masking}, where we indicated
we expected a positive bias on the shape parameter to push the component's MTTF estimate upwards in the case
of the well-designed series system. Note, however, that we might expect a different result if a component had
a different failure distribution (e.g., a Weibull with a shape parameter less than 1).

Overall, for both parameter types, we see that as the masking probability increases, the IQR of the bootstrapped CIs,
the dispersion of the MLEs,a the bias increases, which indicates that the masking probability effects the precision
and accuracy of the estimates. As the masking probability increases, we have less certainty about the
component cause of failure, and thus less certainty about the estimates for the component parameters.

```{r masking-prob-vs-stats-scale, fig.cap=c("Component Cause of Failure Masking (p) vs Scale CI Statistics", "Component Cause of Failure Masking Probability (p) vs. Coverage Probability, IQR of Bootstrapped CIs and 95% Confidence Band of Scale MLEs"), fig.align="center", echo = F}
knitr::include_graphics("image/plot-p-vs-scales.1-n90-large.pdf")
```

```{r masking-prob-vs-stats-shape, fig.cap=c("Component Cause of Failure Masking (p) vs Shape CI Statistics", "Component Cause of Failure Masking Probability (p) vs. Coverage Probability, IQR of Bootstrapped CIs and 95% Confidence Band of Shape MLEs"), fig.align="center", echo = F}
knitr::include_graphics("image/plot-p-vs-shapes.1-n90-large.pdf")
```


### Analysis

When we observe a system failure, we know that one of the components in the candidate set
caused the system to fail, but we do not know which one. This uncertainty has the effect
of pushing the MLE to estimate either a smaller MTTF or a higher infant mortality rate
for each of the components in the candidate set, depending upon the failure characteristics.

For components that are frequently in candidate sets but proportionally not more
likely to be a component cause of failure, the effect is more pronounced, which may
introduce a source of bias in the MLE for such components.

In our Bernoulli candidate set model, the masking probability $p$ determines how commonly each
non-failed component is in the candidate set, and so we expect that as $p$ increases, this will become a
more pronoucned source of bias.\footnote{In a more complicated candidate set model, it is possible
that masking could introduce a significant source of bias for some components, and none at all
for others.}
However, note that the effect of masking, which pushes
the MLE to estimate a smaller MTTF, has opposite effect to that of right-censoring,
which pushes the MLE to estimate a larger MTTF. As these two sources of bias compete with each other,
it is not clear which one will dominate

In what follows, we explain how the bias induced by masking the component cause of failure
effects the MLE for the shape and scale parameters of a Weibull component.
Assessing Equation \eqref{eq:mttf-weibull}, we see that the MTTF of a Weibull component is
proportional to its scale parameter $\lambda_j$, which means when we decrease
the scale parameter $\lambda_j$ (keeping the shape parameter $k_j$ constant), the MTTF decreases.
Therefore, if the $j$\textsuperscript{th} component is in the candidate set, to make it more likely
to appear in the candidate set, its scale parameter should be decreased, potentially
biasing the MLE for the scale parameter downwards.

The shape parameter is more complicated, since it determines the failure characteristics
of the components. In our well-designed system, the shape parameters are larger than $1$,
and in this case, since the MTTF decreases as we increase the shape parameter $k_j$,
if the $j$\textsuperscript{th} component is in the candidate set, to make it more likely
to appear in in the candidate set, its shape parameter is nudged upwards, potentially
positively biasing the MLE for the shape parameter. However, we can also decrease the
shape parameter, which will increase the MTTF, but also increase the infant morality
rate, which is another way to increase the probability of a component appearing in
the candidate set. It is not clear which of these two effects will dominate in general.



## Scenario: Assessing the Impact of Changing the Scale Parameter of a Component {#sec:scale-vs-mttf}

We see that the MTTF is proportional to the scale parameter $\lambda_j$, which means when we decrease
the scale parameter of a component, we proportionally decrease the MTTF.
In this section, we will explore this phenomenon in more detail by manipulating the MTTF of
component 3 and observing the effect it has on the MLE and the bootstrapped confidence intervals
for component 3 and component 1.\footnote{Since the other components had a similiar MTTF, we
will arbitrarily choose component 1 to represent the other components.}

```{r mttf-vs-ci, fig.cap=c("MTTF vs Parameter Statistics"), fig.align="center", echo = F}
knitr::include_graphics("image/plot-scale3-vs-stats.pdf")
```

In Figure \ref{fig:mttf-vs-ci}, we show the effect of the MTTF of component 3 on the MLE and the bootstrapped confidence intervals for the shape and scale
parameters for components 1 and 3 (the component we are varying).
We simulate samples with a sample size of $n = 100$, a right-censoring quantile of $q = 0.825$, and a masking probability of $p = 0.215$.
(Note that while $q$ is fixed, $\tau$ varies as we change the MTTF of component 3.)
The MTTF of component 3 varies from around $300$ to $1500$ and MTTF of the other components, including component 2, is around $900$.
There are several interesting observations that we can make about Figure \ref{fig:mttf-vs-ci}:

1. When the MTTF of component 3 is much smaller than the other components, the estimate of parameters of component 3 is precise (narrow CIs with high Probability
   coverage) and accurate (the MLE is close to the true value). This is because component 3 is the component cause of failure in nearly every system failure,
   and so the data is very informative about the parameters of component 3. Conversely, the estimates of the parameters of the other components is quite poor,
   with wide CIs and large positive bias. Nonetheless, the coverage probability of the CIs for the other components is still well-calibrated, which means that
   the CIs will contain the true value of the parameter with a probability around the specified confidence level. So, while we may not have a good point estimates
   for the parameters, we can still be confident that CIs contain them. That is to say, we have properly quantified our uncertainty about the parameters of the
   other components.

1. When the MTTF of component 3 is much larger than the MTTF of the other components, then component 3 is much less likely to be the component cause of failure,
   and with a masking probability of $p = 0.215$, it will be in the candidate set with approximately $21.5\%$ probability, but it will generally be a false
   candidate. The end result is that the estimates of the parameters of component 3 are quite poor, with wide CIs and large positive bias. However, the estimates
   of the parameters of the other components are quite good, with narrow CIs and small positive bias. The coverage probability of the CIs for the other components
   are, in comparison, quite good. As the MTTF of component 3 increases and it becomes less likely to be the component cause of failure, the estimates of the
   parameters of the other components become more precise and accurate.

   We also see that the bias is positive for both parameters of component 3. We had not necessarily expected this, but we knew there would be a complex
   relationship given the presence of right-censoring and masking. When a system is right-censored, or the exact time of failure is observed but the component
   cause of failure is masked and component 3 is not in the candidate set, then to make component 3 more likely to not be the component cause of failure,
   its failure rate at that observed time is pushed down and its MTTF is pushed to the right by the MLE. Thus, $\hat\lambda_3$ being positively biased is
   expected. However, $k_3$ being positively biased is not necessarily expected, but the fact is, decreasing $k_3$ only has a small impact on the MTTF compared
   to the scale parameter $\lambda_3$, and the shape parameter may be more particular about when the failures occur. For example, if the shape parameter is
    large, then the failures may be more likely to occur at the beginning of the lifetime, which would cause the MTTF to be pushed to the right. This is
   
   anticipated this: from our preliminary analysis, we had expected that the bias would
   be positive for the scale parameter and negative for the shape parameter. We believed this because if component 3 is not the component cause of failure,
   then the system is more likely to fail due to the failure of one of the other components, which would cause the system to fail sooner. This would cause
   

### Analysis

???

## Scenario: Assessing the Impact of Changing the Shape Parameter of a Component {#sec:shape3-vary}

We vary the shape paramenter of component 3 from $0.1$ to $1.9$ and observe the effect it has on the MLE and the
bootstrapped confidence intervals (BCa). The shape parameter determines the failure characteristics
of component 3. 

When $k_3 < 1$, this indicates infant mortality, with a decreasing failure rate over time, so even though
it has a high failure rate at the beginning of its lifetime, it has a low failure rate at the end of its
lifetime and its MTTF is much higher than the other components even though it has a higher
probability of failing first.

When $k_3 > 1$, this indicates wear-out failures, with an increasing failure rate over time, so even though
it has a low failure rate at the beginning of its lifetime, it has a high failure rate at the end of its
lifetime and it has a lower probability of failing first.

We analyze the effect of component 3's shape parameter on the MLE and the bootstrapped confidence intervals for the
shape and scale parameters of components 1 and 3 (the component we are varying). First, we look at the effect
on the scale parameter.

### Scales
```{r prob3-vs-scales, out.width="100%", fig.cap=c("Probability of Component 3 Failure vs Scale Statistics"), fig.align="center", echo = F}
# show in a 2x1 grid
image1 <- readPNG("image/prob3-vs-scale1.png")
image2 <- readPNG("image/prob3-vs-scale3.png")
image3 <- readPNG("image/prob3-vs-scales-cp.png")
g1 <- rasterGrob(image1)
g2 <- rasterGrob(image2)
g3 <- rasterGrob(image3)
first_row <- arrangeGrob(g1, g2, ncol = 2)
grid.arrange(first_row, g3, nrow = 2)
```

In Figure \ref{fig:prob3-vs-shapes}, we show the effect of the shape parameter of component 3 on the
MLE and the bootstrapped confidence intervals for the shape parameters of components 1 and 3.

We see that the mean MLE, in green, is relatively close to the true value, in red, for the scale parameter
of both components. There is a slight positive bias, which may be due to the fact that the data is right-censored
with moderate masking of the component cause of failure. We see that as the probability of component 3 being the
cause of failure increases, the bootstrapped confidence intervals generally increase in width, with the exception
of when $k_3 < 1$ which causes $\Pr\{K_i = 3\}$ to be very small and as $\Pr\{K_i = 3\}$ approaches $0.2$, all
of the components are approximately equally like to be the component cause of failure, and so the CIs seem to be
fairly small for all scale parameters.

However, for $\Pr\{K_i = 3\} > 0.5$, we see that the the mean MLE begins to increase significantly for $\lambda_3$.
This is somewhat unexpecte; we might think that, because its probabilty of being the component cause of failure
is higher, that we would estimate $\lambda_3$ to be lower to proportionately decrase its MTTF. However, the
fact is that the shape parameter has a much bigger impact.

Also, the coverage probabilites of the confidence intervals for the scale parameters decreases for the
scale parameter of components other than 3 as $\Pr\{K_i = 3\}$ increases, but the coverage probability
for the scale parameter of component 3 increases. This may be because, as $\Pr\{K_i = 3\}$ increases, we
are more likely to observe a failure of component 3, and so we have more information about its parameters
and are able to estimate them more accurately.

### Shapes

Now, we look at the effect of the shape parameter of component 3 on the MLE and the bootstrapped confidence intervals
for the shape parameters of components 1 and 3.

```{r prob3-vs-shapes, out.width="100%", fig.cap=c("Probability of Component 3 Failure vs Shape Statistics"), fig.align="center", echo = F}
image1 <- readPNG("image/prob3-vs-shape1.png")
image2 <- readPNG("image/prob3-vs-shape3.png")
image3 <- readPNG("image/prob3-vs-shapes-cp.png")
g1 <- rasterGrob(image1)
g2 <- rasterGrob(image2)
g3 <- rasterGrob(image3)
first_row <- arrangeGrob(g1, g2, ncol = 2)
grid.arrange(first_row, g3, nrow = 2)
```


In Figure \ref{fig:prob3-vs-shapes}, we show the effect of the shape parameter of component 3 on the
MLE and the bootstrapped confidence intervals for the shape parameters of components 1 and 3.

We see that the bias for $k_1$ slowly increases (positive bias) as $\Pr\{K_i = 3\}$ increases, and the bias for
$k_3$ slowly decreases (positive bias) to $0$ as $\Pr\{K_i = 3\}$ increases.
This makes sense, as a larger positive bias for $k_1$ means that the MLE is nudging the shape parameter
of component 1 to be larger so that component 1 is less likely to be the cause of failure.
Similarly, a smaller positive bias for $k_3$ means that the MLE is nudging the shape parameter
of component 3 to be smaller so that component 3 is more likely to be the cause of failure.

The confidence intervals for $k_1$ also become quite wide as $\Pr\{K_i = 3\}$ increases, which is expected
since we observe fewer failures of component 1 as $\Pr\{K_i = 3\}$ increases, and so we have less information
about its parameters and are less able to estimate them accurately. Conversely, the confidence intervals for
$k_3$ become narrower as $\Pr\{K_i = 3\}$ increases, which is also expected, since we observe more failures
of component 3 as $\Pr\{K_i = 3\}$ increases, and so we have more information about its parameters and are
more able to estimate them accurately. The CI widths for $k_3$ becomes extremely small for $\Pr\{K_i = 3\} > 0.3$.

The coverage probabilities are generally less well-caliibrated for the shape parameters compared to the
scale parameters, but they are still reasonably well-calibrated for $\Pr\{K_i = 3\} < 0.4$. For $k_3$,
the coverage probabilities are very well-calibrated for all values of $\Pr\{K_i = 3\}$, but improve
as $\Pr\{K_i = 3\}$ increases due to the fact that we observe more failures of component 3 as $\Pr\{K_i = 3\}$
increases and thus have more informationa bout $k_3$ for our estimate.

### Analysis

???

# Conclusion

In this paper, we have presented a likelihood model for a series system with masked component...

# Appendix A: R Code For Log-likelihood Function {-}

The following code is the log-likelihood function for the Weibull series system
with a likelihood model that includes masked component cause of failure and right-censoring.
It is implemented in the R library `wei.series.md.c1.c2.c3` and is available on
[GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).

For clarity and brevity, we removed some of the functionality that is not relevant to the
analysis in this paper.

```{r, eval = FALSE}
#' Generates a log-likelihood function for a Weibull series system with respect
#' to parameter `theta` (shape, scale) for masked data with candidate sets
#' that satisfy conditions C1, C2, and C3 and right-censored data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Log-likelihood with respect to `theta` given `df`
loglik_wei_series_md_c1_c2_c3 <- function(df, theta) {
    n <- nrow(df)
    C <- md_decode_matrix(df, candset)
    m <- ncol(C)
    delta <- df[[right_censoring_indicator]]
    t <- df[[lifetime]]
    k <- length(theta)
    shapes <- theta[seq(1, k, 2)]
    scales <- theta[seq(2, k, 2)]

    s <- 0
    for (i in 1:n) {
        s <- s - sum((t[i] / scales)^shapes)
        if (delta[i]) {
            s <- s + log(sum(shapes[C[i, ]] / scales[C[i, ]] *
                (t[i] / scales[C[i, ]])^(shapes[C[i, ]] - 1)))
        }
    }
    s
}
```


# Appendix B: R Code For Score Function {-}

The following code is the score function (gradient of the log-likelihood function with respect to $\v\theta$) for the Weibull series system
with a likelihood model that includes masked component cause of failure and right-censoring.
It is implemented in the R library `wei.series.md.c1.c2.c3` and is available on [GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).

For clarity and brevity, we removed some of the functionality that is not relevant to the
analysis in this paper.

```{r score-code, eval = FALSE}
#' Computes the score function (gradient of the log-likelihood function) for a
#' Weibull series system with respect to parameter `theta` (shape, scale) for masked
#' data with candidate sets that satisfy conditions C1, C2, and C3 and right-censored
#' data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Score with respect to `theta` given `df`
score_wei_series_md_c1_c2_c3 <- function(df, theta) {
    n <- nrow(df)
    C <- md_decode_matrix(df, candset)
    m <- ncol(C)
    delta <- df[[right_censoring_indicator]]
    t <- df[[lifetime]]
    shapes <- theta[seq(1, length(theta), 2)]
    scales <- theta[seq(2, length(theta), 2)]
    shape_scores <- rep(0, m)
    scale_scores <- rep(0, m)

    for (i in 1:n) {
        rt.term.shapes <- -(t[i] / scales)^shapes * log(t[i] / scales)
        rt.term.scales <- (shapes / scales) * (t[i] / scales)^shapes

        # Initialize mask terms to 0
        mask.term.shapes <- rep(0, m)
        mask.term.scales <- rep(0, m)

        if (delta[i]) {
            cindex <- C[i, ]
            denom <- sum(shapes[cindex] / scales[cindex] * (t[i] / scales[cindex])^(shapes[cindex] - 1))

            numer.shapes <- 1 / t[i] * (t[i] / scales[cindex])^shapes[cindex] *
                (1 + shapes[cindex] * log(t[i] / scales[cindex]))
            mask.term.shapes[cindex] <- numer.shapes / denom

            numer.scales <- (shapes[cindex] / scales[cindex])^2 * (t[i] / scales[cindex])^(shapes[cindex] - 1)
            mask.term.scales[cindex] <- numer.scales / denom
        }

        shape_scores <- shape_scores + rt.term.shapes + mask.term.shapes
        scale_scores <- scale_scores + rt.term.scales - mask.term.scales
    }

    scr <- rep(0, length(theta))
    scr[seq(1, length(theta), 2)] <- shape_scores
    scr[seq(2, length(theta), 2)] <- scale_scores
    scr
}
```

# Appendix C: R Code For Simulation of Scenarios {-}

The following code is the Monte-carlo simulation code for running the scenarios
described in Section \ref{sec:sim_study}.

```{r bootstrap-sim-code, eval=FALSE}
#### Setup simulation parameters here ####
theta <- c(
    shape1 = 1.2576, scale1 = 994.3661,
    shape2 = 1.1635, scale2 = 908.9458,
    shape3 = NA, scale3 = 840.1141,
    shape4 = 1.1802, scale4 = 940.1342,
    shape5 = 1.2034, scale5 = 923.1631
)

shapes3 <- c(1.1308) # shape 3 true parameter values to simulate
scales3 <- c(840.1141) # scale 3 true parameter values to simulate
N <- c(30, 60, 100) # sample sizes to simulate
P <- c(.215) # masking probabilities to simulate
Q <- c(.825) # right censoring probabilities to simulate
R <- 1000L # number of simulations per scenario
B <- 1000L # number of bootstrap samples
max_iter <- 125L # max iterations for MLE
max_boot_iter <- 125L # max iterations for bootstrap MLE
n_cores <- detectCores() - 1 # number of cores to use for parallel processing
filename <- "data" # filename prefix for output files

#### Simulation code below here ####
library(tidyverse)
library(parallel)
library(boot)
library(algebraic.mle) # for `mle_boot`
library(wei.series.md.c1.c2.c3) # for `mle_lbfgsb_wei_series_md_c1_c2_c3` etc

file.meta <- paste0(filename, ".txt")
file.csv <- paste0(filename, ".csv")
if (file.exists(file.meta)) {
    stop("File already exists: ", file.meta)
}
if (file.exists(file.csv)) {
    stop("File already exists: ", file.csv)
}

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]
m <- length(shapes)

sink(file.meta)
cat("boostrap of confidence intervals:\n")
cat("   simulated on: ", Sys.time(), "\n")
cat("   type: ", ci_method, "\n")
cat("weibull series system:\n")
cat("   number of components: ", m, "\n")
cat("   scale parameters: ", scales, "\n")
cat("   shape parameters: ", shapes, "\n")
cat("simulation parameters:\n")
cat("   shapes3: ", shapes3, "\n")
cat("   scales3: ", scales3, "\n")
cat("   N: ", N, "\n")
cat("   P: ", P, "\n")
cat("   Q: ", Q, "\n")
cat("   R: ", R, "\n")
cat("   B: ", B, "\n")
cat("   max_iter: ", max_iter, "\n")
cat("   max_boot_iter: ", max_boot_iter, "\n")
cat("   n_cores: ", n_cores, "\n")
sink()

for (scale3 in scales3) {
    for (shape3 in shapes3) {
        for (n in N) {
            for (p in P) {
                for (q in Q) {
                    shapes[3] <- shape3
                    theta["shape3"] <- shape3

                    cat("[starting scenario: scale3 = ", scale3, ",
                        shape3 = ", shape3, ", n = ", n, ", p = ", p, ", q = ", q, "]\n")
                    tau <- qwei_series(p = q, scales = scales, shapes = shapes)

                    # we compute R MLEs for each scenario
                    shapes.mle <- matrix(NA, nrow = R, ncol = m)
                    scales.mle <- matrix(NA, nrow = R, ncol = m)
                    shapes.lower <- matrix(NA, nrow = R, ncol = m)
                    shapes.upper <- matrix(NA, nrow = R, ncol = m)
                    scales.lower <- matrix(NA, nrow = R, ncol = m)
                    scales.upper <- matrix(NA, nrow = R, ncol = m)

                    iter <- 0L
                    repeat {
                        retry <- FALSE
                        tryCatch(
                            {
                                repeat {
                                    df <- generate_guo_weibull_table_2_data(
                                        shapes = shapes, scales = scales, n = n, p = p, tau = tau
                                    )

                                    sol <- mle_lbfgsb_wei_series_md_c1_c2_c3(
                                        theta0 = theta, df = df, hessian = FALSE,
                                        control = list(maxit = max_iter, parscale = theta)
                                    )
                                    if (sol$convergence == 0) {
                                        break
                                    }
                                    cat("[", iter, "] MLE did not converge, retrying.\n")
                                }

                                mle_solver <- function(df, i) {
                                    mle_lbfgsb_wei_series_md_c1_c2_c3(
                                        theta0 = sol$par, df = df[i, ], hessian = FALSE,
                                        control = list(maxit = max_boot_iter, parscale = sol$par)
                                    )$par
                                }

                                # do the non-parametric bootstrap
                                sol.boot <- boot(df, mle_solver, R = B, parallel = "multicore", ncpus = n_cores)
                            },
                            error = function(e) {
                                cat("[error] ", conditionMessage(e), "\n")
                                cat("[retrying scenario: n = ", n, ", p = ", p, ", q = ", q, "\n")
                                retry <<- TRUE
                            }
                        )
                        if (retry) {
                            next
                        }
                        iter <- iter + 1L
                        shapes.mle[iter, ] <- sol$par[seq(1, length(theta), 2)]
                        scales.mle[iter, ] <- sol$par[seq(2, length(theta), 2)]

                        tryCatch(
                            {
                                ci <- confint(mle_boot(sol.boot), type = ci_method, level = ci_level)
                                shapes.ci <- ci[seq(1, length(theta), 2), ]
                                scales.ci <- ci[seq(2, length(theta), 2), ]
                                shapes.lower[iter, ] <- shapes.ci[, 1]
                                shapes.upper[iter, ] <- shapes.ci[, 2]
                                scales.lower[iter, ] <- scales.ci[, 1]
                                scales.upper[iter, ] <- scales.ci[, 2]
                            },
                            error = function(e) {
                                cat("[error] ", conditionMessage(e), "\n")
                            }
                        )
                        if (iter %% 5 == 0) {
                            cat("[iteration ", iter, "] shapes = ", shapes.mle[iter, ], "scales = ", scales.mle[iter, ], "\n")
                        }

                        if (iter == R) {
                            break
                        }
                    }

                    df <- data.frame(
                        n = rep(n, R), rep(scale3, R), rep(shape3, R),
                        p = rep(p, R), q = rep(q, R), tau = rep(tau, R), B = rep(B, R),
                        shapes = shapes.mle, scales = scales.mle,
                        shapes.lower = shapes.lower, shapes.upper = shapes.upper,
                        scales.lower = scales.lower, scales.upper = scales.upper
                    )

                    write.table(df,
                        file = file.csv, sep = ",", row.names = FALSE,
                        col.names = !file.exists(file.csv), append = TRUE
                    )
                }
            }
        }
    }
}
```


# Appendix D: Bernoulli Candidate Set Model {-}

```{r, eval = FALSE}
#' Bernoulli candidate set model is a particular type of *uninformed* model.
#' This model satisfies conditions C1, C2, and C3.
#' The failed component will be in the corresponding candidate set with
#' probability 1, and the remaining components will be in the candidate set
#' with probability `p` (the same probability for each component). `p`
#' may be different for each system, but it is assumed to be the same for
#' each component within a system, so `p` can be a vector such that the
#' length of `p` is the number of systems in the data set (with recycling
#' if necessary).
#'
#' @param df masked data.
#' @param p a vector of probabilities (p[j] is the probability that the jth
#'          system will include a non-failed component in its candidate set,
#'          assuming the jth system is not right-censored).
md_bernoulli_cand_c1_c2_c3 <- function(df, p) {
    n <- nrow(df)
    p <- rep(p, length.out = n)
    Tm <- md_decode_matrix(df, comp)
    m <- ncol(Tm)
    Q <- matrix(p, nrow = n, ncol = m)
    Q[cbind(1:n, apply(Tm, 1, which.min))] <- 1
    Q[!df[[right_censoring_indicator]], ] <- 0
    df %>% bind_cols(md_encode_matrix(Q, prob))
}
```

# Appendix E: Series System Quantile Function {#app:series-quantile}

```{r, eval=FALSE}
#' Quantile function (inverse of the cdf).
#' By definition, the quantile `p` * 100% for a strictly monotonically increasing
#' cdf `F` is the value `t` that satisfies `F(t) - p = 0`.
#' We solve for `t` using newton's method.
#'
#' @param p vector of probabilities.
#' @param shapes vector of weibull shape parameters for weibull lifetime
#'               components
#' @param scales vector of weibull scale parameters for weibull lifetime
#'               components
qwei_series <- function(p, shapes, scales) {
    t0 <- 1
    repeat {
        t1 <- t0 - sum((t0/scales)^shapes) + log(1-p) /
            sum(shapes*t0^(shapes-1)/scales^shapes)
        if (abs(t1 - t0) < tol) {
            break
        }
        t0 <- t1
    }
    return(t1)
}
```
