---
title: "Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data"
author: "Alex Towell"
abstract: "Accurately estimating reliability of individual components in multi-component systems is challenging when only system-level failure data is observable. This paper develops maximum likelihood techniques to estimate component reliability from right-censored lifetimes and candidate sets indicative of masked failure causes in series systems. A likelihood model accounts for right-censoring and candidate sets. Extensive simulation studies demonstrate accurate and robust performance of the maximum likelihood estimator despite small samples and significant masking and censoring. The bias-corrected accelerated bootstrap provides well-calibrated confidence intervals. A reduced model with homogeneous component shapes simplifies analysis for well-designed systems. The methods expand the capability to quantify latent component properties from limited system reliability data. Key contributions include derivations of likelihood models, validation of estimation techniques via simulations, and assessment of a reduced model. Together, these advance rigorous component reliability assessment from masked failure data."
output:
    bookdown::pdf_document2:
    #bookdown::html_document2:
    #bookdown::gitbook:
    #pdf_document:
        #toc: yes
        #toc_depth: 3
        number_sections: true
        #extra_dependencies: ["hyperref", "graphicx","amsthm","amsmath","natbib","tikz"]
        extra_dependencies: ["tikz"]
        df_print: kable
        keep_tex: true
        citation_package: natbib
indent: true
header-includes:
   - \usepackage{caption}
   - \AtBeginDocument{\renewcommand{\v}[1]{\boldsymbol{#1}}}
   - \AtBeginDocument{\newtheorem{condition}{Condition}}
   - \AtBeginDocument{\renewcommand{\refname}{References}}
bibliography: refs.bib
link-citations: true
# let's change the style to a statistics / math / engineering journal
biblio-style: apalike
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(md.tools)
library(algebraic.mle)
library(wei.series.md.c1.c2.c3)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(glue)
library(png)
library(kableExtra)

#\newtheorem{definition}{Definition}
#\newtheorem{theorem}{Theorem}
#\newtheorem{corollary}{Corollary}
#\numberwithin{equation}{section}

```

Introduction
============
Accurately estimating the reliability of individual components in multi-component
systems is an important challenge, as component lifetimes and failure causes are
often not directly observable. In a series system, only system-level failure times
may be recorded along with limited information about the failed component. Such
masked data poses difficulties for assessing component reliability.

This paper develops and validates maximum likelihood techniques to estimate
component reliability from right-censored lifetime data and candidate sets
indicative of component failure causes. The key results are:

- Deriving a likelihood model incorporating right-censoring and candidate sets to
  enable masked data to be used for parameter estimation.

- Demonstrating through simulation studies that the maximum likelihood estimator
  performs well despite small samples and significant masking and right-censoring.
  Estimation of scale parameters is more robust than shape parameters.

- Showing that bootstrapping provides reasonably well-calibrated confidence
  intervals for the maximum likelihood estimates, even with small sample sizes.

- Assessing a reduced model with homogeneous component shapes. This simplified
  model is favored for well-designed systems but deviations in component properties
  can impact adequacy.

The remainder of the paper details the series system and likelihood models,
maximum likelihood estimation methodology, bootstrap confidence interval estimation,
and extensive simulation studies exploring estimator behavior under various sample
sizes, masking levels, and model assumptions.

Together, these contributions provide a statistically rigorous framework for learning about latent component properties from limited observational data on system reliability. The proposed methods expand the capability to quantify component lifetimes in situations where failure data is significantly masked.

Series System Model {#sec:statmod}
==================================
Consider a system composed of $m$ components arranged in a series configuration.
Each component and system has two possible states, functioning or failed.
We have $n$ systems whose lifetimes are independent and identically distributed (i.i.d.).
The lifetime of the $i$\textsuperscript{th} system denoted by the random variable $T_{i}$.
The lifetime of the $j$\textsuperscript{th} component in the $i$\textsuperscript{th}
system is denoted by the random variable $T_{i j}$.
We assume the component lifetimes in a single system are statistically independent and non-identically distributed.
Here, lifetime is defined as the elapsed time from when the new, functioning component
(or system) is put into operation until it fails for the first time.
A series system fails when any component fails, thus the lifetime of the $i$\textsuperscript{th}
system is given by the component with the shortest lifetime,
$$
    T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
$$

There are three particularly important distribution functions in reliability analysis: the
reliability function, the probability density function, and the hazard function.
The reliability function, $R_{T_i}(t)$, is the
probability that the $i$\textsuperscript{th} system has a lifespan larger than a duration $t$,
\begin{equation}
R_{T_i}(t) = \Pr\{T_i > t\}\\
\end{equation}
The probability density function (pdf) of $T_i$ is denoted by
$f_{T_i}(t)$ and may be defined as
$$
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
$$
Next, we introduce the hazard function.
The probability that a failure occurs between $t$ and $\Delta t$ given that no
failure occurs before time $t$ is given by
$$
\Pr\{T_i \leq t+\Delta t|T_i > t\} = \frac{\Pr\{t < T_i < t+\Delta t\}}{\Pr\{T_i > t\}}.
$$
The failure rate is given by the dividing this equation by the length of the time
interval, $\Delta t$:
$$
\frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}} =
    \frac{R_{T_i}(t) - R_{T_i}(t+\Delta t)}{R_{T_i}(t)}.
$$
The hazard function $h_{T_i}(t)$ for $T_i$ is the instantaneous failure rate at time $t$, which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_{T_i}(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}}\\
       &= \frac{f_{T_i}(t)}{R_{T_i}(t)}.
\end{split}
\end{equation}

The lifetime of the $j$\textsuperscript{th} component is assumed to follow a parametric distribution indexed
by a parameter vector $\v{\theta_j}$. The parameter vector of the overall system is defined as
$$
    \v\theta = (\v{\theta_1},\ldots,\v{\theta_m}).
$$

When a random variable $X$ is parameterized by a particular $\v\theta$, we denote the
reliability function by $R_X(t;\v\theta)$, and the same for the other distribution functions.
As a special case, for the components in the series system, we subscript by their labels, e.g,
the $j$\textsuperscript{th} component's pdf is denoted by $f_j(t;\v{\theta_j})$. Two continuous
random variables $X$ and $Y$ have a joint pdf $f_{X,Y}(x,y)$.
Given the joint pdf $f(x,y)$, the marginal pdf of $X$ is given by
$$
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
$$
where $\mathcal{Y}$ is the support of $Y$. (If $Y$ is discrete, replace
the integration with a summation over $\mathcal{Y}$.)

The conditional pdf of $Y$ given $X=x$, $f_{Y|X}(y|x)$, is defined as
$$
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
$$
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of $X_1,\ldots,X_m$ is denoted by $f(x_1,\ldots,x_m)$.

Next, we dive deeper into these concepts and provide mathematical derivations for
the reliability function, pdf, and hazard function of the series system.
We begin with the reliability function of the series system, as given by the following theorem.
\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_i > t\}
$$
which may be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\v\theta) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_reliability_function} shows that the system's overall reliability is the product of the reliabilities of its individual components. This property is inherent to series systems and will be used in the subsequent derivations.

Next, we turn our attention to the pdf of the system lifetime, described in the following theorem.
\begin{theorem}
\label{thm:sys_pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f_{T_i}(t;\v\theta) = \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f_{T_i}(t;\v\theta) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f_{T_i}(t;\v\theta)
    &= -\frac{d}{dt} R_1(t;\v{\theta_1})\prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j})\\
    &= f_1(t;\v\theta) \prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f_{T_i}(t;\v\theta) = \sum_{j=1}^{m-1} f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\v{\theta_j}) \frac{d}{dt} R_m(t;\v{\theta_m}),
$$
which simplifies to
$$
f_{T_i}(t;\v\theta)= \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}).
$$
\end{proof}
Theorem \ref{thm:sys_pdf} shows the pdf of the system lifetime as a function of the pdfs and reliabilities of its components.

We continue with the hazard function of the system lifetime, defined in the next theorem.
\begin{theorem}
\label{thm:sys_failure_rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h_{T_i}(t;\v\theta) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By Equation \eqref{eq:failure_rate}, the $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)}.
$$
Plugging in expressions for these functions results in
$$
  h_{T_i}(t;\v\theta) = \frac{\sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k})}
      {\prod_{j=1}^m R_j(t;\v{\theta_j})},
$$
which can be simplified to
$$
h_{T_i}(t;\v\theta) = \sum_{j=1}^m \frac{f_j(t;\v{\theta_j})}{R_j(t;\v{\theta_j})} = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_failure_rate} reveals that the system's hazard function is the sum of the hazard functions of its components.

By definition, the hazard function is the ratio of the pdf to the reliability
function,
$$
h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)},
$$
and we can rearrange this to get
\begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f_{T_i}(t;\v\theta) &= h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta)\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\v{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\v{\theta_j}) \biggr\},
\end{split}
\end{equation}
which we sometimes find to be a more convenient form than Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's reliability function, pdf, and hazard function. Next, we build upon these concepts to derive distributions related to the component cause of failure.

## Component Cause of Failure {#sec:comp_cause}
Whenever a series system fails, precisely one of the components is the cause.
We model the component cause of the series system failure as a random variable.
\begin{definition}
The component cause of failure of a series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}

The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.
\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) \prod_{l=1}^m R_l(t;\v\theta),
\end{equation}
where $h_j(t;\v{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_l(t;\v{\theta_l})$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a series system with $3$ components.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\v\theta) = \prod_{j=1}^{3} f_j(t;\v{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\v\theta) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2}) f_3(t_3;\v{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2})
        R_3(t;\v{\theta_3}) dt_2\\
     &= f_1(t;\v{\theta_1}) R_2(t;\v{\theta_2}) R_3(t_1;\v{\theta_3}).
\end{align*}
Since $h_1(t;\v{\theta_1}) = f_1(t;\v{\theta_1}) / R_1(t;\v{\theta_1})$,
$$
f_1(t;\v{\theta_1}) = h_1(t;\v{\theta_1}) R_1(t;\v{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\v\theta)$
yields
$$
f_{K_i,T_i}(j,t;\v\theta) = h_1(t;\v{\theta_1}) \prod_{l=1}^m R_l(t;\v{\theta_l})\\
$$
Generalizing from this completes the proof.
\end{proof}

Theorem \ref{thm:f_k_and_t} shows that the joint pdf of the component cause of
failure and system lifetime is a function of the hazard functions and reliability
functions of the components. This result will be used in the Section \ref{sec:like_model}
to derive the likelihood function for the masked data.

The probability that the $j$\textsuperscript{th} component is the cause of failure
is given by the following theorem.
\begin{theorem}
The probability that the $j$\textsuperscript{th} component is the cause of failure
is given by
\begin{equation}
\label{eq:prob_k}
\Pr\{K_i = j\} = E_{\v\theta}
\biggl[
    \frac{h_j(T_i;\v{\theta_j})}
         {\sum_{l=1}^m h_l(T_i ; \v{\theta_l})}
\biggr]
\end{equation}
where $K_i$ is the random variable denoting the component cause of failure of the
$i$\textsuperscript{th} system and $T_i$ is the random variable denoting the
lifetime of the $i$\textsuperscript{th} system.
\end{theorem}
\begin{proof}
The probability the $j$\textsuperscript{th} component is the cause of failure is given by
marginalizing the joint pdf of $K_i$ and $T_i$ over $T_i$,
$$
\Pr\{K_i = j\} = \int_0^{\infty} f_{K_i,T_i}(j,t;\v\theta) dt.
$$
By Theorem \ref{thm:f_k_and_t}, this is equivalent to
\begin{align*}
\Pr\{K_i = j\}
    &= \int_0^{\infty} h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta) dt\\
    &= \int_0^{\infty} \biggl(\frac{h_j(t;\v{\theta_j})}{h_{T_i}(t ; \v\theta)}\biggr) f_{T_i}(t ; \v\theta) dt\\
    &= E_{\v\theta}\biggl[\frac{h_j(T_i;\v{\theta_j})}{\sum_{l=1}^m h_l(T_i ; \v{\theta_l})}\biggr].
\end{align*}
\end{proof}

## System and Component Reliabilities {#sec:reliability}

A common measure of reliability is mean time to failure (MTTF). The
MTTF is defined as the expectation of the lifetime,
\begin{equation}
\label{mttf-sys}
\text{MTTF} = E_{\v\theta}\{T_i\},
\end{equation}
which if certain assumptions are satisfied\footnote{$T_i$ is non-negative and continuous, $R_{T_i}(t;\v\theta)$ is a
well-defined, continuous, and differential function for $t > 0$,
and $\int_0^\infty R_{T_i}(t;\v\theta) dt$ converges.}
is equivalent to the integration of the reliability function over its support.

While the MTTF provides a summary measure of reliability, it is not a complete description.
Depending on the failure characteristics, MTTF can be misleading. For example,
a system that has a high likelihood of failing early in its life may still have a
large MTTF if it is fat-tailed.\footnote{A "fat-tailed" distribution refers to a probability
distribution with tails that decay more slowly than those of the exponential family,
such as the case with the Weibull when its shape parameter is greater than $1$. This means
that extreme values are more likely to occur, and the distribution is more prone to
"black swan" events or rare occurrences. In the context of reliability, a fat-tailed
distribution might imply a higher likelihood of unusually long lifetimes, which can
skew measures like the MTTF. See, for example, \cite{taleb2007black}.}

The reliability of the components in the series system determines the reliability
of the system. We denote the MTTF of the $j$\textsuperscript{th} component by
$\text{MTTF}_j$ and, according to Equation \eqref{eq:prob_k}, the probability
that the $j$\textsuperscript{th} component is the cause of failure is given by
$\Pr\{K_i = j\}$. In a well-designed series system, there is no component that is
the "weakest link" that either has a much shorter MTTF or a much higher
probability of being the component cause of failure than any of
the other components, e.g., $\Pr\{K_i = j\} \approx \Pr\{K_i = k\}$ and
and MTTF$_j \approx$ MTTF$_k$ for all $j$ and $k$. This just means that the components
should have similar reliabilities and failure characteristics.

We use these results in the simulation study in Section \ref{sec:sim_study}, where we
assess the sensitivity of the MLE with respect to varying the reliability
of one of the Weibull components. We vary its reliability in two different ways:

1. We vary its shape parameter (keeping its scale parameter constant), which determines
   the failure characteristics of the component and also affects its MTTF.

2. We vary its scale parameter (keeping its shape parameter constant), which scales
   its MTTF while retaining the same failure characteristics.

Likelihood Model for Masked Data {#sec:like_model}
==================================================

The object of interest is the (unknown) parameter value $\v\theta$. 
To estimate this $\v\theta$, we need *data*.
In our case, we call it *masked data* because we do not necessarily observe
the event of interest, say a system failure, directly.
We consider two types of masking: masking the system failure lifetime and
masking the component cause of failure.

We generally encounter three types of system failure lifetime masking:

1. A system failure is observed at a particular point in time.
2. A system failure is observed to occur within a particular interval of time.
3. A system failure is not observed, but we know that the system survived at least
   until a particular point in time. This is known as *right-censoring*
   and can occur if, for instance, an experiment is terminated while the system
   is still functioning.


We generally encounter two types of component cause of failure masking:

1. The component cause of failure is observed.
2. The component cause of failure is not observed, but we know that the failed
   component is in some set of components. This is known as *masking* the
   component cause of failure.

Thus, the component cause of failure masking will take the form of candidate sets. A
candidate set consists of some subset of component labels that plausibly contains the
label of the failed component.
The sample space of candidate sets are all subsets of $\{1,\ldots,m\}$, thus
there are $2^m$ possible outcomes in the sample space.

In this paper, we limit our focus to observing *right censored* lifetimes and exact
lifetimes but with masked component cause of failures.
We consider a sample of $n$ i.i.d. series systems, each of
which is put into operation at some time and and observed until either it fails
or is right-censored.
We denote the right-censoring time of the $i$\textsuperscript{th} system by
$\tau_i$. 
We do not directly observe the system lifetime, $T_i$, but rather, we observe
the right-censored lifetime, $S_i$, which is given by
\begin{equation}
    S_i = \min\{\tau_i, T_i\},
\end{equation}
We also observe a right-censoring indicator, $\delta_i$, which is given by
\begin{equation}
    \delta_i = 1_{T_i < \tau_i}
\end{equation}
where $1_{\text{condition}}$ is an indicator function that outputs $1$ if
*condition* is true and $0$ otherwise.
Here, $\delta_i = 1$ indicates the event of interest, a system failure, was
observed.

If a system failure lifetime is observed, then we also observe a candidate set
that contains the component cause of failure. We denote the candidate set for
the $i$\textsuperscript{th} system by $\mathcal{C}_i$, which is a subset of
$\{1,\ldots,m\}$.
Since the data generating process for candidate sets may be subject to chance
variations, it as a random set.

Consider we have an independent and identically distributed (i.i.d.) random sample of masked data,
$D = \{D_1, \ldots, D_n\}$, where each $D_i$ contanis the following:

- $S_i$, the system lifetime of the $i$\textsuperscript{th} system.
- $\delta_i$, the right-censoring indicator of the $i$\textsuperscript{th} system.
- $\mathcal{C}_i$, the set of candidate component causes of failure for the
  $i$\textsuperscript{th} system.

The masked data generation process is illustrated by Figure \ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$D_i = (S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data $D$ for exact, right-censored system failure times with
candidate sets that mask the component cause of failure can be seen in Table 1
for a series system with $m=3$ components.

System | Right-censoring time ($S_i$) | Right censoring indicator ($\delta_i$) | Candidate set ($\mathcal{C}_i$) |
------ | --------------------------- | --------------------------- | --------------------- |
   1   | $4.3$                       | 1                           | $\{1,2\}$             |
   2   | $1.3$                       | 1                           | $\{2\}$               |
   3   | $5.4$                       | 0                           | $\emptyset$           |
   4   | $2.6$                       | 1                           | $\{2,3\}$             |
   5   | $3.7$                       | 1                           | $\{1,2,3\}$           |
   6   | $10$                        | 0                           | $\emptyset$           |

: Right-censored lifetime data with masked component cause of failure.

In our model, we assume the data is governed by a pdf, which is determined by
a specific parameter, represented as $\v\theta$ within the parameter space $\v\Omega$.
The joint pdf of the data $D$ can be represented as follows:
$$
f(D ; \v\theta) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\v\theta),
$$
where $s_i$ is the observed system lifetime of the $i$\textsuperscript{th} system,
$\delta_i$ is the observed right-censoring indicator of the $i$\textsuperscript{th} system,
and $c_i$ is the observed candidate set of the $i$\textsuperscript{th} system.

This joint pdf tells us how likely we are to observe the particular data, $D$, given
the parameter $\v\theta$. When we keep the data constant and allow the parameter
$\v\theta$ to vary, we obtain what is called the likelihood function $L$, defined as
$$
L(\v\theta) = \prod_{i=1}^n L_i(\v\theta)
$$
where
$$
L_i(\v\theta) = f(s_i,\delta_i,c_i;\v\theta)
$$
is the likelihood contribution of the $i$\textsuperscript{th} system. In other words,
the likelihood function quantifies how likely different parameter values $\v\theta$
are, given the observed data.

For each type of data, right-censored data and masked component cause of
failure data, we will derive the *likelihood contribution* $L_i$, which refers to the
part of the likelihood function that this particular piece of data contributes to.

We present the following theorem for the likelihood contribution model.
\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\v\theta) = R_{T_i}(s_i;\v\theta) \biggl(\beta_i \sum_{j \in c_i} h_j(s_i;\v{\theta_j}) \biggr)^{\delta_i}
\end{equation}
where $R_{T_i}(s_i;\v\theta) = \prod_{j=1}^m R_j(s_i;\v{\theta_j})$
is the reliability function of the series system evaluted at $s_i$, $\delta_i = 0$ indicates the
$i$\textsuperscript{th} system is
right-censored at time $s_i$, and $\delta_i = 1$ indicates the $i$\textsuperscript{th} system
is observed to have failed at time $s_i$ with a component cause of failure
is masked by the candidate set $c_i$.
\end{theorem}

In the follow subsections, we prove this result for each type of masked data, right-censored
system lifetime data $(\delta_i = 0)$ and masking of the component cause of failure
$(\delta_i = 1)$.

## Masked Component Cause of Failure {#sec:candmod}
Suppose a diagnostician is unable to identify the precise component cause of the
failure, e.g., due to cost considerations he or she replaced multiple components
at once, successfully repairing the system but failing to precisely identity
the failed component.
In this case, the cause of failure is said to be *masked*.

The unobserved component lifetimes may have many covariates, like ambient
operating temperature, but the only covariate we observe in our masked data
model are the system's lifetime and additional masked data in the form of
a candidate set that is somehow correlated with the unobserved component
lifetimes.

The key goal of our analysis is to estimate the parameter $\v{\theta}$, which 
maximize the likelihood of the observed data, and to estimate the precision and
accuracy of this estimate using the Bootstrap method.

To achieve this, we first need to assess the joint distribution of the system's
continuous lifetime, $T_i$, and the discrete candidate set, $\mathcal{C}_i$, which
can be written as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = f_{T_i}(t_i;\v{\theta})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\},
$$
where $f_{T_i}(t_i;\v{\theta})$ is the pdf of $T_i$ and
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$ is the conditional
pmf of $\mathcal{C}_i$ given $T_i = t_i$.

We assume the pdf $f_{T_i}(t_i;\v{\theta})$ is known, but we do not have knowledge
of $\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$, i.e., the data generating
process for candidate sets is unknown.

However, it is critical that the masked data, $\mathcal{C}_i$, is correlated with the
$i$\textsuperscript{th} system. This way, the conditional distribution of $\mathcal{C}_i$
given $T_i = t_i$ may provide information about $\v{\theta}$, despite our Statistical
interest being primarily in the series system rather than the candidate sets.

To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which $\mathcal{C}_i$ is correlated with the
$i$\textsuperscript{th} system is given by assuming the following condition.
\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}
Assuming Condition \ref{cond:c_contains_k}, $\mathcal{C}_i$ must contain the
index of the failed component, but we can say little else about what other
component indices may appear in $\mathcal{C}_i$.

In order to derive the joint distribution of $\mathcal{C}_i$ and $T_i$ assuming
Condition \ref{cond:c_contains_k}, we take the following approach.
We notice that $\mathcal{C}_i$ and $K_i$ are statistically dependent.
We denote the conditional pmf of $\mathcal{C}_i$ given $T_i = t_i$ and
$K_i = j$ as
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$

Even though $K_i$ is not observable in our masked data model, we can still
consider the joint distribution of $T_i$, $K_i$, and $\mathcal{C}_i$.
By Theorem \ref{thm:f_k_and_t}, the joint pdf of $T_i$ and $K_i$ is given by
$$
f_{T_i,K_i}(t_i,j;\v{\theta}) = h_j(t_i;\v{\theta_j}) R_{T_i}(t_i;\v\theta),
$$
where $h_j(t_i;\v{\theta_j})$ is the hazard function for the
$j$\textsuperscript{th} component and $R_{T_i}(t_i;\v{\theta})$ is the
reliability function of the system.
Thus, the joint pdf of $T_i$, $K_i$, and $\mathcal{C}_i$ may be written as
\begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta})
    &= f_{T_i,K_i}(t_i,k;\v{\theta}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\v{\theta_j}) R_{T_i}(t_i;\v\theta)
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation}
We are going to need the joint pdf of $T_i$ and $\mathcal{C}_i$, which
may be obtained by summing over the support $\{1,\ldots,m\}$ of $K_i$ in
Equation \eqref{eq:joint_pdf_t_k_c},
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{T_i}(t_i;\v\theta)
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
$$
By Condition \ref{cond:c_contains_k},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0$ when $K_i = j$ and
$j \notin c_i$, and so we may rewrite the joint pdf of $T_i$ and
$\mathcal{C}_i$ as
\begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of $\v\theta$ (see Section \ref{sec:mle}), we
solve the simultaneous equations of the MLE and choose a solution
$\hat{\v\theta}$ that is a maximum for the likelihood function.
When we do this, we find that $\hat{\v\theta}$ depends on the unknown
conditional pmf $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}
According to [@Fran-1991], in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In this
setting, Condition \ref{cond:equal_prob_failure_cause} generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$ may be factored out of the
summation in Equation \eqref{eq:part1}, and thus the joint pdf of $T_i$ and
$\mathcal{C}_i$ may be rewritten as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j})
$$
where $j' \in c_i$.

If $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ is a function of
$\v{\theta}$, the MLEs are still dependent on the unknown
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ to find an MLE of
$\v{\theta}$. The last condition we assume achieves this result.
\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $T_i$ and component cause
of failure $K_i$ are not functions of $\v{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\v\theta$.
\end{condition}

When Conditions \ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause},
and \ref{cond:masked_indept_theta} are satisfied, the joint pdf of $T_i$ and
$\mathcal{C}_i$ is given by
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \beta_i R_{T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
When we fix the sample and allow $\v{\theta}$ to vary, we obtain the
contribution to the likelihood $L$ from the $i$\textsuperscript{th} observation
when the system lifetime is exactly known (i.e., $\delta_i = 1$) but the
component cause of failure is masked by a candidate set $c_i$:
\begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\v\theta) = R_{T_i}(t_i;\v\theta) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}, 
if we observe an exact system failure time for the $i$-th system ($\delta_i = 1$),
but the component that failed is masked by a candidate set $c_i$, then its likelihood
contribution is given by Equation \eqref{eq:likelihood_contribution_masked}.

### Probability of Component Cause of System Failure {-}
This subsection is not necessary in our likelihood model, but it derives a useful
result for making predictions about the component cause of failure.
Suppose we have observed a candidate set and a series system
failure and we are interested in the probability that a particular component
is the cause of failure.
\begin{theorem}
Assuming Conditions \ref{cond:c_contains_k} and \ref{cond:equal_prob_failure_cause},
the conditional probability of the component cause of failure is component $j$ ($K_i = j$)
given a masked component cause of failure ($\mathcal{C}_i = c_i$) and system lifetime ($T_i = t_i$)
is given by
\begin{equation}
\label{eq:cond_prob_k_given_t_and_c}
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
 \frac{h_j(t_i|\v{\theta_j})}{\sum_{l \in c_i} h_l(t_i|\v{\theta_l})} 1_{\{j \in c_i\}}.
\end{equation}
\end{theorem}
\begin{proof}
The conditional probability $\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\}$ may be
written as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} f_{K_i,T_i}(j,t_i;\v\theta)}
    {\sum_{j=1}^m \Pr{}_{\!\v\theta}
        \{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} f_{K_i,T_i}(j,t_i;\v\theta)}.
$$
By Theorem \ref{thm:f_k_and_t},
$f_{K_i,T_i}(j,t_i;\v\theta) = h_j(t_i;\v\theta)R_{T_i}(t_i;\v\theta)$.
We may make this substitution and simplify:
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} h_j(t_i;\v{\theta_j})}
         {\sum_{j'=1}^m \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|K_i=j',T_i=t_i\} h_{j'}(t_i;\v{\theta_{j'}})}.
$$
Assuming Conditions \ref{cond:c_contains_k} and \ref{cond:equal_prob_failure_cause}, we may rewrite the above
as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} h_j(t_i;\v{\theta_j})}
    {\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|K_i = j,T_i=t_i\} {\sum_{l \in c_i} h_l(t_i;\v{\theta_l})}} =
    \frac{h_j(t_i;\v{\theta_j})}
    {\sum_{l \in c_i} h_l(t_i;\v{\theta_l})}.
$$
\end{proof}

Frequently, we may not have any information at all about the component cause of
failure. In this case, $c_i = \{1,\ldots,m\}$, and we obtain the following corollary.
\begin{corollary}
\label{cor:cond_prob}
The probability that the $j$\textsuperscript{th}
component is the cause of system failure given only that we know a system failure
occured at time $t_i$ is given by
$$
\Pr\{K_i = j|T_i=t_i\} = \frac{h_j(t_i;\v{\theta_j})}{\sum_{l=1}^m h_l(t_i;\v{\theta_l})}.
$$
\end{corollary}


## Right-Censored Data

As described in Section \ref{sec:like_model}, we observe realizations of
$(S_i,\delta_i,\mathcal{C}_i)$ where $S_i = \min\{T_i,\tau_i\}$ is the
right-censored system lifetime, $\delta_i = 1_{\{T_i < \tau_i\}}$ is
the right-censoring indicator, and $\mathcal{C}_i$ is the candidate set.

In the previous section, we discussed the likelihood contribution from an
observation of a masked component cause of failure, i.e., $\delta_i = 1$. 
We now derive the likelihood contribution of a *right-censored* observation
$(\delta_i = 0$) in our masked data model.
\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\v\theta) = R_{T_i}(s_i;\v\theta).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau_i$, and we only know that
$T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\v\theta) = \Pr\!{}_{\v\theta}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\v\theta) = R_{T_i}(s_i;\v\theta).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the likelihood
contribution for the $i$\textsuperscript{th} system shown in Theorem
\ref{thm:likelihood_contribution},
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$

We use this result in Section \ref{sec:mle} to derive the maximum likelihood
estimator (MLE) of $\v\theta$.

## Identifiability and Convergence Issues

In our likelihood model, masking and right-censoring can lead to issues related
to identifiability and flat likelihood regions.
Identifiability refers to the unique mapping of the model parameters to the
likelihood function, and lack of identifiability can lead
to multiple sets of parameters that explain the data equally well, making inference
about the true parameters challenging \citep{lehmann1998theory}, while 
flat likelihood regions can complicate convergence \citep{wu1983convergence}.

In our simulation study, we address these challenges in a pragmatic way. Specifically,
failure to converge to a solution within a maximum of 125 iterations is interpreted as
evidence of the aforementioned issues, leading to the discarding of the sample, with
the process then repeated with a new synthetic sample. Note, however, that in Section
\ref{sec:boot} where we discuss the bias-corrected and accelerated (BCa) bootstrap
method for constructing confidence intervals, we do not discard any resamples.

This strategy helps ensure the robustness of the results, while acknowledging the
inherent complexities of likelihood-based estimation in models characterized by
masking and right-censoring.

Maximum Likelihood Estimation {#sec:mle}
========================================
In our analysis, we use maximum likelihood estimation (MLE) to estimate the series
system parameter $\v\theta$ from the masked data \citep{bain, casella2002statistical}.
The MLE finds parameter values that maximize the likelihood of the observed data
under the assumed model. A maximum likelihood estimate, $\hat{\v\theta}$, is a
solution of
\begin{equation}
\label{eq:mle}
L(\hat{\v\theta}) = \max_{\v\theta \in \v\Omega} L(\v\theta),
\end{equation}
where $L(\v\theta)$ is the likelihood function of the observed data. For computational
efficiency and analytical simplicity, we work with the log-likelihood function,
denoted as $\ell(\v\theta)$, instead of the likelihood function \citep{casella2002statistical}.
\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\v\theta)$, for our masked data model is the sum of the log-likelihoods for each observation,
\begin{equation}
\label{eq:loglike}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta),
\end{equation}
where $\ell_i(\v\theta)$ is the log-likelihood contribution for the $i$\textsuperscript{th} observation:
\begin{equation}
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) +
    \delta_i \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \bigr).
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is the logarithm of the likelihood function,
$$
\ell(\v\theta) = \log L(\v\theta) = \log \prod_{i=1}^n L_i(\v\theta) = \sum_{i=1}^n \log L_i(\v\theta).
$$

Substituting $L_i(\v\theta)$ from Equation \eqref{eq:like}, we consider these two cases of $\delta_i$
seperately to obtain the result in Theorem \ref{thm:loglike_total}.

\textbf{Case 1}: If the $i$-th system is right-censored ($\delta_i = 0$),
$$
\ell_i(\v\theta) = \log R_{T_i}(s_i;\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the failure time is known ($\delta_i = 1$),
$$
\ell_i(\v\theta) = \log R_{T_i}(s_i;\v\theta) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j})\bigr).
$$
We replace $R_{T_i}(s_i;\v\theta)$ with its component-wise definition and by Condition \ref{cond:masked_indept_theta},
we may discard\footnote{Adding or substracting a function by a constant does not change where it obtains a maximum, so we
are free to discard such terms from the log-likelihood function.} the $\log \beta_i$ term since it does not depend on
$\v\theta$, giving us the result
$$
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \biggr).
$$

Combining these two cases gives us the result in Theorem \ref{thm:loglike_total}.
\end{proof}

The MLE, $\hat{\v\theta}$, is often found by solving a system of equations derived from setting the derivative of the log-likelihood function to zero, i.e.,
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0,
\end{equation}
for each component $\theta_j$ of the parameter $\v\theta$ \citep{bain}. When there's no closed-form solution,
we resort to numerical methods like the Newton-Raphson method.

Assuming some regularity conditions, such as the likelihood function being identifiable, the MLE has many desirable
asymptotic properties that underpin statistical inference, namely that it is an asymptotically unbiased estimator
of the parameter $\v\theta$ and it is normally distributed with a variance given by the inverse of the Fisher
Information Matrix (FIM) \citep{casella2002statistical}.
However, for smaller samples, these asymptotic properties may not yield accurate approximations. We propose to use
the bootstrap method to offer an empirical approach for estimating the sampling distribution of the MLE, in particular for
computing confidence intervals.

Bias-Corrected and Accelerated Bootstrap Confidence Intervals {#sec:boot}
===============================================================
We utilize the non-parametric bootstrap to approximate the sampling distribution of
the MLE. In the non-parametric bootstrap, we resample from the observed data
with replacement to generate a bootstrap sample. The MLE is then computed for
the bootstrap sample. This process is repeated $B$ times, giving us $B$ bootstrap
replicates of the MLE. The sampling distribution of the MLE is then approximated
by the empirical distribution of the bootstrap replicates of the MLE.

The method we use to generate confidence intervals is known
as Bias-Corrected and Accelerated Bootstrap Confidence Intervals (BCa), which
applies two corrections to the standard bootstrap method:

- Bias correction: This adjusts for bias in the bootstrap distribution itself.
  This bias is measured as the difference between the mean of the bootstrap distribution and the observed statistic.
  It works by transforming the percentiles of the bootstrap distribution to correct for these issues.
  
  This may be a useful transformation in our case since we are dealing with small samples and we have two potential
  sources of bias: right-censoring and masking component cause of failure. They seem to have opposing effects
  on the MLE, but the relationship is difficult to quantify.

- Acceleration: This adjusts for the rate of change of the statistic as a function of the true, unknown parameter.
  This correction is important when the shape of the statistic's distribution changes with the true parameter.

  Since we have a number of different shape parameters, $k_1,\ldots,k_m$, we may expect the shape of the
  distribution of the MLE to change as a function of the true parameter, making this correction potentially useful.

Since we are primarly interested in generating confidence intervals for small samples for a
potentially biased MLE, the BCa method may be a good choice for our analysis. For more details
on BCa, see \cite{efron1987better}.

In our simulation study, we will assess the performance of the bootstrapped BCa
confidence intervals by computing the coverage probability of the confidence
intervals. A well-calibrated 95% confidence interval contains the true
value around 95% of the time. If the confidence interval is too narrow, it will have
a coverage probability less than 95%, which conveys a sort of false confidence
in the precision of the MLE. If the confidence interval is too wide, it will
have a coverage probability greater than 95%, which conveys a lack of confidence
in the precision of the MLE. We want confidence intervals to be as
narrow as possible while still having a coverage probability close to the
nominal level, 95%.

## Issues with Resampling from the Observed Data

While the bootstrap method provides a robust and flexible tool for statistical
estimation, its effectiveness can be influenced by several factors
\citep{efron1994introduction}.

Firstly, instances of non-convergence in our bootstrap samples were observed.
Such cases can occur when the estimation method, like the MLE used in our
analysis, fails to converge due to the specifics of the resampled data
\citep{casella2002statistical}. This issue can potentially introduce bias or
reduce the effective sample size of our bootstrap distribution.

Secondly, the bootstrap's accuracy can be compromised with small sample sizes,
as the method relies on the law of large numbers to approximate the true sampling
distribution. For small datasets, the bootstrap samples might not adequately
represent the true variability in the data, leading to inaccurate results
\citep{efron1994introduction}.

Thirdly, our data involves right censoring and a masking of the component cause
of failure when a system failure is observed. These aspects can cause certain data points or
trends to be underrepresented or not represented at all in our data, introducing
bias in the bootstrap distribution \citep{klein2005survival}.

Despite these challenges, we found the bootstrap method useful in approximating
the sampling distribution of the MLE, taking care in interpreting the results,
particularly as it relates to coverage probabilities.

Series System with Weibull Components {#sec:weibull}
====================================================

The Weibull distribution, introduced by Waloddi Weibull in 1937, has been
instrumental in reliability analysis due to its ability to model a wide range
of failure behaviors. Reflecting on its utility, Weibull
modestly noted that it "[...] may sometimes render good service." \citep{Abernethy2006}.
In the context of our study, we utilize the Weibull
to model a system as originating from Weibull components in a series configuration,
producing a specific form of the likelihood model described in Section \ref{sec:like_model},
which deals with challenges such as right censoring and masked component cause of failure.

The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j) \qquad \text{for } i = 1,\ldots,n \text{ and } j = 1,\ldots,m,
$$
where $\lambda_j > 0$ is the scale parameter and $k_j > 0$ is the shape parameter.
The $j$\textsuperscript{th} component has a reliability function, pdf, and hazard function
given respectively by
\begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}.
\end{align}

The shape parameter of the Weibull distribtion is of particular importance:

- $k_j < 1$ indicates infant mortality. An example of how this might arise is
a result of defective components being weeded out early, and the remaining
components surviving for a much longer time.
- $k_j = 1$ indicates random failures (independent of age). An example of how
this might arise is a result of random shocks to the system, but otherwise
the system is age-independent.\footnote{The exponential distribution is a special
case of the Weibull distribution when $k_j = 1$.}
- $k_j > 1$ indicates wear-out failures. An example of how this might arise is a
result of components wearing as they age

We show that the lifetime of the series system composed of $m$ Weibull components
has a reliability, hazard, and probability density functions given by the following theorem.
\begin{theorem}
The lifetime of a series system composed of $m$ Weibull components
has a reliability function, hazard function, and pdf respectively given by
\begin{align}
\label{eq:sys_weibull_reliability_function}
R_{T_i}(t;\v\theta) &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
\label{eq:sys_weibull_failure_rate_function}
h_{T_i}(t;\v\theta) &= \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},\\
\label{eq:sys_weibull_pdf}
f_{T_i}(t;\v\theta) &= \biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{align}
\end{theorem}
\begin{proof}
The proof for the reliability function follows from Theorem \ref{thm:sys_reliability_function},
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R_{T_i}(t;\v\theta)
    = \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}
    = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
The proof for the hazard function follows from Theorem \ref{thm:sys_failure_rate},
\begin{align*}
h_{T_i}(t;\v\theta)
    = \sum_{j=1}^{m} h_j(t;\v{\theta_j})
    = \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align*}
The proof for the pdf follows from Theorem \ref{thm:sys_pdf}. By definition,
$$
f_{T_i}(t;\v\theta) = h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}

## Reliability

In Section \ref{sec:reliability}, we discussed the concept of reliability.
In the case of Weibull components, the MTTF of the $j$\textsuperscript{th}
component is given by
\begin{equation}
\label{eq:mttf-weibull}
\text{MTTF}_j = \lambda_j \Gamma\biggl(1 + \frac{1}{k_j}\biggr),
\end{equation}
where $\Gamma$ is the gamma function.

We mentioned that the MTTF can sometimes be a poor measure of reliability, e.g.,
the MTTF and the probability of failing early can be large. The Weibull is a good
example of this phenomenon. If $k > 1$, the Weibull is a fat-tailed distribution,
and it can exhibit both a large MTTF and a high probability of failing early.

Components may have similar MTTFs, but some components may be more likely to fail
early and others may be more likely to fail late, depending upon their failure
characterstics (shape parameters), and so the probability of component failure given by
Equation \eqref{eq:prob_k} is a useful measure of component reliability compared to
the other components in the system.

In a well-designed series system, the component failure characteristics are similar:
they have a similar MTTF and a similar probability of being the component cause of
failure, i.e., they have similar shapes and scales, so that system failures are not
dominated by some subset of components.

## Likelihood Model {#sec:sys_weibull_likelihood}
In Section \ref{sec:like_model}, we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
$i$\textsuperscript{th} system is given by the following theorem.
\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\v\theta) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\v{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the following result.
\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\v\theta) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\v\theta$ since they do not
affect the MLE.
\end{corollary}

We find an MLE by solving \eqref{eq:mle_eq},
i.e., a point $\v{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)$ satisfying
$\nabla_{\theta} \ell(\v{\hat\theta}) = \v{0}$, where $\nabla_{\v\theta}$
is the gradient of the log-likelihood function (score) with respect to $\v\theta$.

To solve this system of equations, we use the Newton-Raphson method, which requires
the score and the Hessian of the log-likelihood function.
We analytically derive the score since it is useful to have for the Newton-Raphson
method, but we do not do the same for the Hessian of the log-likelihood for the following reasons:

1. The gradient is easy to derive, and it is useful to have for
computing gradients efficiently and accurately, which will be useful for
numerically approximating the Hessian.

2. The Hessian is tedious and error prone to derive, and Newton-like methods
often do not require the Hessian to be explicitly computed.

The following theorem derives the score function.
\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\v\theta) = \biggl(
    \frac{\partial \ell_i(\v\theta)}{\partial k_1},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\v\theta)}{\partial k_m},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the log-likelihood
contribution of the $i$-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation so the proof
has been omitted, but the result has been verified by using a very precise numerical
approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions is
the sum of their gradients, and so the score function conditioned on the entire
sample is given by
\begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\v\theta) = \sum_{i=1}^n \nabla \ell_i(\v\theta).
\end{equation}

## Weibull Series System: Homogeneous Shape Parameters {#sec:reduced-weibull}

A series system composed of Weibull components is not generally Weibull unless the
shape parameters of the components are homogeneous.
\begin{theorem}
If the shape parameters of the components are homogenous, then the series system is Weibull
with a shape parameter $k$ given by the identical shape parameters of the components and a scale
parameter $\lambda$ given by
\begin{equation}
\label{eq:sys_weibull_scale}
\lambda = \biggl(\sum_{j=1}^{m} \lambda_j^{-k}\biggr)^{-1/k},
\end{equation}
\end{theorem}
where $\lambda_j$ is the scale parameter of the $j$\textsuperscript{th} component.
\begin{proof}
Given $m$ Weibull lifetimes $T_{i 1}, \ldots, T_{i m}$ with the same shape parameter $k$
and scale parameters $\lambda_1, \ldots, \lambda_m$, the reliability function of the series
system is
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k}\biggr\}.
$$
To make this a Weibull system, we need to find a single scale parameter $\lambda$ such that
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\biggl(\frac{t}{\lambda}\biggr)^{k}\biggr\},
$$
which has the solution
$$
\lambda = \frac{1}{\left(\frac{1}{\lambda_1^k} + \ldots + \frac{1}{\lambda_m^k}\right)^{\frac{1}{k}}}.
$$
\end{proof}

\begin{theorem}
If a series system has Weibull components with homogeneous shape parameters, the component
cause of failure is conditionally independent of the system failure time:
$$
    \Pr\{K_i = j | T_i = t_i \} = \Pr\{K_i = j\} = \frac{\lambda_j^{-k}}{\sum_{l=1}^{m} \lambda_l^{-k}}.
$$
\end{theorem}
\begin{proof}
By Corollary \ref{cor:cond_prob}, the conditional probability of the $j$\textsuperscript{th} component being the
cause of failure given the system failure time is given by
\begin{align*}
\Pr\{K_i = j | T_i = t\}
    &= \frac{f_{K_i, T_i}(j, t;\v\theta)}{f_{T_i}(t;\v\theta)}
    = \frac{h_j(t;k,\lambda_j) R_{T_i}(t;\v\theta)}
        {h_{T_i}(t;\v{\theta_j}) R_{T_i}(t;\v\theta)}\\
    &= \frac{h_j(t;k,\lambda_j)}{\sum_{l=1}^m h_l(t;k,\lambda_l)}
    = \frac{\frac{k}{\lambda_j}\bigl(\frac{t}{\lambda_j}\bigr)^{k-1}}
        {\sum_{l=1}^m \frac{k}{\lambda_l}\bigl(\frac{t}{\lambda_l}\bigr)^{k-1}}
    = \frac{\bigl(\frac{1}{\lambda_j}\bigr)^k}
        {\sum_{l=1}^m \bigl(\frac{1}{\lambda_l}\bigr)^k}.
\end{align*}
\end{proof}

If we have prior knowledge that the shape parameters are sufficiently homogenous, it may
be useful to simplify the likelihood model by assuming the shape parameters are identical,
simplifying the series system to Weibull, facilitating analysis and interpretation.
According to the bias-variance trade-off, we expect the MLE to be more biased but
have lower sampling variance.

We denote the full model log-likelihood function by $\ell_F$ and the reduced model log-likelihood
by $\ell_R$. The reduced model is obtained by setting the shape parameter of each component to
be the same, i.e., $k_1 = \cdots = k_m = k$. Thus, the reduced model log-likelihood function is given by
$$
\ell_R(k, \lambda_1, \lambda_2, \cdots, \lambda_m) =
        \ell_F(k, \lambda_1, k, \lambda_2, \ldots, k, \lambda_m),
$$
The same may be done for the score and hessian of the log-likelihood functions.

Simulation Study: Full Model {#sec:sim_study}
================================

```{r sim-study-design, echo = F}
theta <- c(
    shape1 = 1.2576, scale1 = 994.3661,
    shape2 = 1.1635, scale2 = 908.9458,
    shape3 = 1.1308, scale3 = 840.1141,
    shape4 = 1.1802, scale4 = 940.1342,
    shape5 = 1.2034, scale5 = 923.1631
)

shapes <- theta[grepl("shape", names(theta))]
scales <- theta[grepl("scale", names(theta))]
```

In this section, we conduct a simulation study to assess the performance
of the MLE for the full likelihood model defined in Section \ref{sec:weibull}.
In this simulation study, we assess the sensitivity of the MLE to
various simulation scenarios. In particular, we assess two important
properties of the MLE with respect to a scenario:

1. Accuracy (Bias): How close is the expected value of the MLE to the true
   parameter values? If the expected value of the MLE is close to the true
   parameter values, the accuracy is high.

2. Precision: How much does the MLE vary from sample to sample? We measure
   this by assessing the 95% confidence intervals (BCa, Bias-Corrected and
   accelerated). If the confidence intervals are both small and have good 
   coverage probability (the proportion of confidence intervals that contain 
   the true parameter values), then the MLE is precise.

We begin by specifying the parameters of the series system that will be
the central object of our simulation study. We consider the data in
@Huairu-2013, in which they study the reliability of a series system with
three components. They fit Weibull components in a series configuration to
the data, resulting in an MLE with shape and scale estimates given by the
first three components in Table \ref{tab:series-sys}. To make the model
slightly more complex, we add two more components to this series system,
with shape and scale parameters given by the last two components in Table
\ref{tab:series-sys}. We will refer to this system as the **base** system.

In Section \ref{sec:reliability}, we defined a well-designed series
system as one that consists of components with similar reliabilities, where we define
reliability in two ways, the mean time to failure (MTTF) and the probability that a
specific component will be the cause of failure. All things else being equal,
components with long MTTFs and with near uniform probability of being the component
cause of failure is preferrable, otherwise we have a weak link in the system.

The base system defined in Table \ref{tab:series-sys} satisfies this definition
of being a well-designed system. We see that there are no components that are
significantly less reliable than any of the others, component 1 being the most reliable
and component 3 being the least reliable. This is a result of the scales and shapes
being similar for each component. In addition, the shapes are larger than $1$, which
means components are unlikely to fail early.

```{r series-sys, table.attr = "style='width:50%;'", echo = F, fig.align = "center"}
mttf.sys <- integrate(function(t) {
    t * dwei_series(t,
        scales = scales,
        shapes = shapes
    )
}, lower = 0, upper = Inf)$value

mttf.sys <- round(mttf.sys, digits = 3)
mttf <- round(gamma(1 + 1 / shapes) * scales, digits = 3)
probs <- round(wei_series_cause(1L:5L, scales = scales, shapes = shapes), digits = 3)
tau <- qwei_series(p = 0.825, scales = scales, shapes = shapes)
surv.sys <- round(surv_wei_series(tau, scales = scales, shapes = shapes), digits = 3)
surv <- round(pweibull(tau, shape = shapes, scale = scales, lower.tail = FALSE), digits = 3)
components <- data.frame(
    Shape = shapes,
    Scale = scales,
    MTTF = mttf,
    Prob = probs,
    Survival = surv,
    row.names = paste("Component", 1:5)
)

components <- rbind(components, "Series System" = c(NA, NA, mttf.sys, NA, surv.sys))
names(components) <- gsub("\\.", " ", names(components))
names(components)[1] <- "Shape ($k_j$)"
names(components)[2] <- "Scale ($\\lambda_j$)"
names(components)[3] <- "MTTF$_j$"
names(components)[4] <- "$\\Pr\\{K_i = j\\}$"
names(components)[5] <- "$R_j(\\tau;k_j,\\lambda_j)$"
knitr::kable(
    components,
    caption = "Weibull Components in Series Configuration",
    escape = FALSE # Add this line to allow raw LaTeX
)
```



## Data Generating Process {#sec:data_gen}

In this section, we describe the data generating process for our simulation studies.
It consists of three parts: the series system, the candidate set model, and the
right-censoring model.

### Series System Lifetime {-}

We generate data from a Weibull series system with $m$ components.
As described in Section \ref{sec:weibull}, the $j$\textsuperscript{th} component
of the $i$\textsuperscript{th} system has a lifetime distribution given by
$$
    T_{i j} \sim \operatorname{WEI}(k_j, \lambda_j)
$$
and the lifetime of the series system composed of $m$ Weibull components
is defined as
$$
    T_i = \min\{T_{i 1}, \ldots, T_{i m}\}.
$$
To generate a data set, we first generate the $m$ component failure times,
by efficiently sampling from their respective distributions, and we then set
the failure time $t_i$ of the system to the minimum of the component failure times.

### Right-Censoring Model {-}
We employ a simple right-censoring model, where the right-censoring time
$\tau$ is fixed at some known value, e.g., an experiment is run for a fixed
amount of time $\tau$, and all systems that have not failed by the end of the
experiment are right-censored. The censoring time $S_i$ of the
$i$\textsuperscript{th} system is thus given by
$$
    S_i = \min\{T_i, \tau\}.
$$
So, after we generate the system failure time $T_i$, we generate the censoring
time $S_i$ by taking the minimum of $T_i$ and $\tau$.
In our simulation study, we paramaterize the right-censoring time $\tau$ by the
quantile $q = 0.825$ of the series system,
$$
    \tau = F_{T_i}^{-1}(q).
$$
This means that $82.5\%$ of the series systems are expected to fail before time $\tau$
and $17.5\%$ of the series are expected to be right-censored. To solve for the $82.5\%$
quantile of the series system, we define the function $g$ as
$$
g(\tau) = F_{T_i}(\tau;\v\theta) - q
$$
and find its root using the Newton-Raphson method. See Appendix E for the R code that
implements this procedure.

### Masking Model for Component Cause of Failure {-}

We must generate data that satisfies the masking conditions described in
Section \ref{sec:candmod}.
There are many ways to satisfying the masking conditions. We choose the simplest
method, which we call the *Bernoulli candidate set model*. In this model, each
non-failed component is included in the candidate set with
a fixed probability $p$, independently of all other components and independently
of $\v\theta$, and the failed component is always included in the candidate set.
See Appendix D for the R code that implements this model.

## Simulation Scenarios

We define a simulation scenario to be some combination of $n$ (sample size),
$p$ (masking probability in our Bernoulli candidate set model), $k_3$
(shape parameter of the third component), $\lambda_3$ (scale parameter of the
third component), and $q$ (right-censoring quantile). We are interested in
choosing a small number of scenarios that are representative of real-world
scenarios and that are interesting to analyze.

Here is an outline of the simulation study for a particular scenario:

1. Fix a combination of simulation parameters to some value, and vary the remaining
   parameters. For example, if we want to assess how the sampling distribution of
   the MLE changes with respect to sample size, we might choose some particular
   values for $p$, $k_3$, $\lambda_3$, and $q$, and vary the sample size $n$ over the
   desired range.

2. Simulate $R \geq 300$ datasets from the Data Generating Process (DGP) described in
   Section \ref{sec:data_gen} and compute an MLE for each dataset. We choose $R$ to be
   large enough so that the sampling distribution of the MLE is well approximated by
   the empirical distribution of the $R$ MLEs.

3. For each of these $R$ MLEs, compute some function of the MLE, like the BCa confidence
   intervals or the likelihood ratio test statistic. This will give us $R$ statistics
   as a Monte-carlo estimate of the sampling distribution of the statistic.

4. Use the $R$ statistics to estimate some property of the sampling distribution of the
   statistic, e.g., the mean of the MLE or the coverage probability of the BCa confidence
   intervals, with respect to the parameter(s) we are varying in the scenario, e.g.,
   assess how the coverage probability of the BCa confidence intervals changes with
   respect to sample size.

5. Visualize the results and assess the behavior of estimator under the chosen scenario.
   
For how we run a simulation scenario, see Appendix C.

## Scenario: Assessing the Impact of Right-Censoring {#sec:effect-censoring}

In this scenario, we use the well-designed series system described in Table \ref{tab:series-sys},
and we vary the right-censoring quantile ($q$) from $60\%$ to $100\%$
(no right-censoring), with a component cause of failure masking
probability of $21.5\%$ and sample size $n = 100$.

```{r q-vs-stats, out.width='100%', cache = T, fig.cap=c("Right-Censoring Quantile vs MLE ($p = 0.215, n = 100$)","Right-Qensoring Quantile ($q$) vs MLEs"), fig.align="center", echo = F}
#knitr::include_graphics("image/tau/plot-q-vs-mle.pdf")
knitr::include_graphics("image/5_system_tau_fig.pdf")
```

When a right-censoring event occurs, in order to increase the likelihood of the data, the MLE
is nudged in a direction that increases the probability of a right-censoring event at time $\tau$,
which is given by $R_{T_i}(t;\v\theta)$, representing a source of bias in the estimate.

To increase $R_{T_i}(\tau)$, we move in the direction (gradient) of these partial derivatives.
The partial derivatives of $R_{T_i}(\tau)$
are given by
\begin{align*}
\frac{\partial R_{T_i}(\tau)}{\partial \lambda_j} &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \frac{k_j}{\lambda_j},\\
\frac{\partial R_{T_i}(\tau)}{\partial k_j}       &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \left(\log \lambda_j - \log \tau\right),
\end{align*}
for $j = 1, \ldots, m$. We see that these partial derivatives are related to the score of a right-censored likelihood contribution in
Theorem \ref{thm:weibull_score}. Let us analyze these partial derivatives:

- As $\tau$ increases, $R_{T_i}(\tau;\v\theta)$ decreases, and so the effect right-censoring has on the MLE decreases. This is what we see in
  Figure \ref{fig:q-vs-stats}.

- The partial derivatives with respect to the scale parameters are always positive, so right-censoring positively bias the scale parameter
  estimates to make right-censoring events more likely. The more right-censoring, the more the positive bias. We see this in Figure
  \ref{fig:q-vs-stats}, where the bias of the MLE for the scale parameter decreases as we decrease the probability ($1-q$) of a right-censoring event.

- The partial derivative with respect to the shape parameter of the $j$\textsuperscript{th} component, $k_j$, is
  non-negative if $\lambda_j \geq \tau$ and otherwise negative. In our well-designed series system, the scale parameters
  are large compared to most of the right-censoring times for $\tau(q)$, so the MLE nudges the shape parameter estimates
  in a positive direction to increase the probability of a right-censoring event $R_{T_i}(\tau)$ at time $\tau$. We see this
  in Figure \ref{fig:q-vs-stats}, where the shape parameter estimates are positively biased for most of the quantiles
  $q$.
  
#### Key Observations {-}

- *Coverage Probability (CP)*: The CP is well-calibrated, obtaining a value near the
  nominal 95% level across different right-censoring quantiles. This suggests that the
  bootstrapped CIs will contain the true value of the parameters with the specified confidence
  level. The CIs are neither too wide nor too narrow.

- *Dispersion of MLEs*: The shaded regions representing the 95% probability range of
  the MLEs get narrower as the right-censoring quantile increases. This is an indicator of the
  increased precision in the estimates as more data is available due to decreased
  censoring.

- *IQR of Bootstrapped CIs*: The IQR (vertical blue bars) reduces with an increase in
  sample size. This suggests that the bootstrapped CIs are getting more consistent and
  focused around a narrower range with larger samples while maintaining a good coverage
  probability. As we get more data, the bootstrapped CIs are more likely to be closer
  to each other and the true value of the parameters.

  For small right-censoring quantiles (small right-censoring times), they are quite
  large, but to maintain well-calibrated CIs, this  was necessary. The estimator is quite
  sensitive to the data, and so the bootstrapped CIs are quite wide to account for this
  sensitivity when the sample contains insufficient information due to censoring.

- *Mean of MLEs*: The red dashed line indicating the mean of MLEs initially is quite biased,
  but quickly diminshes to neglible levels for scale parameters. The estimates for the shape
  parameters never reaches zero, but this is potentially due to masking. At a larger sample
  size, we anticipate the bias in the shape estimates would also decrease to zero.

## Scenario: Assessing the Impact of Sample Size {#sec:effect-samp-size}

In this scenario, we use the well-designed series system described in Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
we fix the right-censoring quantile to $q = 0.825$ (moderate censoring), and we vary the sample
size $n$ from $50$ (small sample size) to $1000$ (very large sample size).

```{r samp-size-n-vs-stats, out.width='100%', fig.cap=c("Sample Size vs MLEs ($p = 0.215, q = 0.825$)","Sample Size ($n$) vs MLEs"), fig.align="center", echo = F}
#knitr::include_graphics("image/n-vs-mles.pdf")
knitr::include_graphics("image/5_system_samp_size_fig.pdf")
```

In Figure \ref{fig:samp-size-n-vs-stats}, we show the effect of the sample size $n$ on the MLEs
for the shape and scale parameters. The top four plots only show the effect on the MLEs for the
shape and scale parameters of components $1$ and $4$, since the rest were essentially identical,
and the bottom two plots show the coverage probabilities for all parameters.

#### Key Observations {-}

- *Coverage Probability (CP)*: The CP is well-calibrated, obtaining a value near the
  nominal 95% level across different sample sizes. This suggests that the bootstrapped
  CIs will contain the true value of the shape parameter with the specified confidence
  level. The CIs are neither too wide nor too narrow.

- *Dispersion of MLEs*: The shaded regions representing the 95% probability range of
  the MLEs get narrower as the sample size increases. This is an indicator of the
  increased precision in the estimates when provided with more data. This is consistent
  with the asymptotic properties of the MLE when the regularity conditions are satisfied,
  e.g., converges in probability to the true value of the parameter as $n$ goes to infinity.

- *IQR of Bootstrapped CIs*: The IQR (vertical blue bars) reduces with an increase in
  sample size. This suggests that the bootstrapped CIs are getting more consistent and
  focused around a narrower range with larger samples while maintaining a good coverage
  probability. As we get more data, the bootstrapped CIs are more likely to be closer
  to each other and the true value of the scale parameter.

  For small sample sizes, they are quite large, but to maintain well-calibrated CIs, this
  was necessary. The estimator is quite sensitive to the data, and so the bootstrapped
  CIs are quite wide to account for this sensitivity when the sample size is small and
  not necessarily representative of the true distribution.

- *Mean of MLEs for Scales*: The red dashed line indicating the mean of MLEs remains stable across
  different sample sizes and close to the true value, suggesting that the scale MLEs are 
  reasonably unbiased.

- *Mean of MLEs for Shapes*: The red dashed line is the mean of shape MLEs. Unlike the scale MLEs,
  we see that for small samples, particularly less than $200$, we observe a significant
  amount of positive bias for shape MLEs. The MLE for the shape parameters in this
  scenario appear to be more sensitive to the data than the scale parameters.

This scenario successfully illustrates the importance of sample size in estimating parameters.
The findings align with statistical theory and provide insights into the behavior of these estimators
for different sample sizes. In particular, it highlights the sensitivity of the shape parameter
estimates to right-censoring and masking for small sample sizes and the importance of having
sufficient data to overcome these effects.

## Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure {#sec:p-vs-mttf}

In this scenario, we use the well-designed series system described in
Table \ref{tab:series-sys}. We fix the sample size to $n = 90$ (reasonable sample size) and 
we fix the right-censoring quantile to $q = 0.825$, and we vary the masking probability 
from $p$ from $0.1$ (very slight masking the component cause of failure) to $0.85$
(extreme masking of the component cause of failure).

In Figure \ref{fig:masking-prob-vs-stats}, we show the effect of the masking probability
$p$ on the MLE for the shape and scale parameters. The top four plots only show the effect
on the MLEs for the the shape and scale parameters of components $1$ and $4$, since the
rest were essentially identical, and the bottom two plots show the coverage probabilities for
all parameters.

```{r masking-prob-vs-stats, fig.cap=c("Component Cause of Failure Masking ($p$) vs MLE", "Component Cause of Failure Masking Probability (p) vs. MLE"), fig.align="center", echo = F}
#knitr::include_graphics("image/p-vs-mle.pdf")
knitr::include_graphics("image/5_system_prob_fig.pdf")
```

#### Key Observations {-}

- *Coverage Probability (CP)*: For the scale parameters, the $95\%$ CI is well-calibrated
  for Bernulli masking probabilities up to $p = 0.725$, which is really quite significant,
  obtaining coverages over $90\%$, but drops precipitously after that point.  
  For the shape parameters, the $95\%$ CI is well-calibrated for masking probabilities only
  up to $p = 0.4$, which is still large, obtaining coverages generally over $90%$, but
  begins to drop slowly after that point.
  
  The BCa confidence intervals are well-calibrated for most realistic
  masking probabilities, constructing CIs that are neither too wide nor too narrow,
  but when the masking is severe and the sample size is small, one should
  take the CIs with a grain of salt.

- *Dispersion of MLEs*: The shaded regions representing the $95\%$ quantile of
  the MLEs become wider as the masking probability increases. This is an indicator of the
  decreased precision in the estimates when provided with more ambiguous data about the
  component cause of failure. However, even for fairly significant Bernoulli masking,
  $p \leq 0.55$, the $95\%$ quantiles are narrow and the CP is
  well-calibrated, indicating that the MLEs are still precise and accurate.

- *IQR of Bootstrapped CIs*: The IQR (vertical blue bars) show that the bootstrapped BCa
  CIs are becoming more spread out as the masking probability increases. They are also
  asymmetric, with the lower bound being more spread out than the upper bound, but this
  is consistent with the actual behavior of the dispersion of the MLEs, which exhibits
  the same pattern. The width of the CIs consistently increase as the masking probability
  increases, which we intuitively expected given the increased uncertanity about the
  component cause of failure.
  After a Bernoulli masking probability of $p \approx 0.5$, the width of the CIs rapidly increase,
  which is apparently necessary for the CPs to remain well-calibrated.

- *Mean of MLEs*: The red dashed line indicating the mean of the MLEs remains
  stable across different masking probabilities, only showing a significant positive bias
  when the masking probability $p$ becomes quite significant.

## Scenario: Assessing the Impact of Changing the Scale Parameter of Component 3 {#sec:scale-vs-mttf}

By Equation \eqref{eq:mttf-weibull}, we see that MTTF$_j$ is proportional to the scale parameter $\lambda_j$, which means
when we decrease the scale parameter of a component, we proportionally decrease the MTTF.
In this scenario, we start with the well-designed series system described in Table \ref{tab:series-sys},
and we will manipulate the MTTF of component 3, MTTF$_3$, by changing its
scale parameter, $\lambda_3$, and observing the effect this has on the MLE. Since the other components
had a similiar MTTF, we will arbitrarily choose component 1 to represent the other components.
The bottom plot shows the coverage probabilities for all parameters.

In Figure \ref{fig:mttf-vs-ci}, we show the effect of changing the scale parameter of component $3$, $lambda_3$,
but map $\lambda_3$ to MTTF$_3$ to make it more intuitive to reason about. We vary the MTTF of component 3
from $300$ to $1500$ and the other components have their MTTFs fixed at around $900$, as shown in
Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
the right-censoring quantile to $q = 0.825$ (moderate censoring), and the sample size to $n = 100$ (moderate sample size).

```{r mttf-vs-ci, fig.cap=c("MTTF$_3$ vs MLE By Varying Scale", "Mean-Time-To-Failure (MTTF) of Component 3 by Varying Its Scale Parameter"), fig.align="center", echo = F}
#knitr::include_graphics("image/plot-scale3-vs-stats.pdf")
#knitr::include_graphics("image/5_system_scale3_vary.pdf")
knitr::include_graphics("image/5_system_mttf3_by_scale3.pdf")
```

#### Key observations {-}

- *Coverage Probability (CP)*: When MTTF of component 3 is much smaller than other components,
the CP for $k_3$ is very well calibrated (approximately $95\%$) while the other shape parameters of
the other componentns are around $90\%$, which is still reasonable. (This is the case even though
the width of the CI for $k_3$ is extremely narrow compared to the others).
As MTTF$_3$ increases, the CP for $k_3$ decreases, while CP for the other components increase
slightly. The scale parameters are generally well-calibrated for all of the components.

- *Dispersion of MLEs*: For component 3, as its MTTF decreases, the dispersion of MLEs narrows,
indicating more precise estimates. Conversely, dispersion for other components widens. As MTTF
of component 3 increases, its dispersion widens while others narrow. This is consistent with
the fact that the smaller MTTF of component 3 means that, in this well-designed system at least,
it is more likely to be the component cause of failure, and so we have more information about
its parameters and are able to estimate them more accurately.

- *IQR of Bootstrapped CIs*: The dark blue vertical lines representing IQR show that the variability
of the Bootstrapped BCa CIs decreases for component 3 as its MTTF decreases, suggesting more
consistency in the estimates. For other components, IQR behaves conversely.

- *Mean of MLEs*: For component 3, the mean of MLEs of scale parameter increases with MTTF,
reflecting a small positive bias. The shape parameter also shows positive bias, which can be attributed
to the intricate relationship with the scale parameter and the presence of right-censoring and masking.

## Scenario: Analyzing the Impact of Changing the Shape Parameter of Component 3 {#sec:shape3-vary}

We assess the effects of varying component 3's shape parameter between $0.1$ and $3.5$ on the MLEs
for the shape and scale parameters of components 1 and 3.

#### Key observations {-}

- *Coverage Probability (CP)*: The CP for the scale parameter of component 3 increases with $\Pr\{K_i = 3\}$, indicating more accurate estimates due to more observed failures. For other components, it decreases. For the shape parameters, CPs are generally less calibrated than scale parameters, though reasonably good for $\Pr\{K_i = 3\} < 0.4$.

- *Dispersion of MLEs*: The mean MLE for the scale parameter $\lambda_3$ starts to increase significantly as $\Pr\{K_i = 3\} > 0.5$. For shape parameters, the bias for $k_1$ increases slowly, while for $k_3$ it decreases slowly to $0$ as $\Pr\{K_i = 3\}$ increases.

- *IQR of Bootstrapped CIs*: For the scale parameters, bootstrapped CIs widen with increasing $\Pr\{K_i = 3\}$, except when $k_3 < 1$. For the shape parameters, the CIs for $k_1$ widen as $\Pr\{K_i = 3\}$ increases, while CIs for $k_3$ narrow, becoming extremely small for $\Pr\{K_i = 3\} > 0.3$.

- *Mean of MLEs*: The mean MLE for the scale parameter is close to the true value with a slight positive bias. For shape parameters, the MLE is adjusting $k_1$ to be larger and $k_3$ to be smaller as $\Pr\{K_i = 3\}$ increases, affecting the likelihood of each component being the cause of failure.

## Implications and Recommendations

Here, we discuss and summarize the implications of our simulation study and provide recommendations
for practitioners.

#### Shape parameters {-}
The shape parameters are more sensitive to the data than the scale parameters. The coverage probabilities
were also consistently less well-calibrated for the shape parameters compared to the scale parameters,
but they were still reasonably well-calibrated for most scenarios, with a lower bound of $85\%$ for
the most extreme scenarios, e.g., Bernoulli masking probabilities of $p = 0.7$ with a small
sample ($n = 100$) and moderate right-censoring ($q = 0.825$).

The shape parameters define the failure characteristics of the components, but they have a complex
relationship to the MTTF and other reliability measures.
Ideally, the shape parameters will all be greater than $1$, indicating wear-out failures, but this is
not always the case, and sometimes it may be the case that some components have a shape parameter
greater than $1$ while others have a shape parametr less than $1$. Due to the sensitivity of the
shape parameters to the data, and the fact that the shape parameters are less intuitive than the
scale parameters, we recommend that the shape parameters be interpreted with caution, particularly
when the sample size is small.

Since a well-designed system has components with similar failure characteristics, the shape parameters
should be reasonably aligned. To mitigate the problems with the shape parameters, both from an analytic
and variability perspective, we may be motivated to use the reduced model that assumes homogeneity
in the shape parameters, as described in Section \ref{sec:reduced-weibull}. The reduced model offers
interpretability (the series system is itself Weibull) and reduced estimator
variability (only $m+1$ parameters instead of $2 m$), but it is must adequately describe the data.

#### Scale Parameters {-}
The scale parameters are less sensitive to the data than the shape parameters. The coverage probabilities
were also consistently well-calibrated for all scenarios, with a lower-bound of around $90\%$ for the
most extreme scenarios.
The scale parameters are linearly related to the MTTF, and have a simpler relationship to the
the other reliability measures too, which makes it easier to interpret and understand.

#### Sample Size {-}
The MLE is very sensitive to the data, and so the bootstrapped CIs are quite wide to account for this
sensitivity when the sample size is small and not necessarily representative of the true distribution.
We see that the MLE of the shape parameters is somewhat biased for small samples, but the scale
parameters have a small bias except for the most extreme cases of right-censoring, masking, or
deviations from a well-designed system. In either case, as the sample size increases, the bias
decreases and the CIs become more narrow and well-calibrated.
We observed that the confidence intervals were well-calibrated for even small sample sizes for the scale
parameters, but the shape parameters were less well-calibrated.

#### Right-Censoring {-}
For our well-designed system, the mean MLE for the shape parameters were mildly sensitive to right-censoring,
but even significant censoring around $50\%$ only introduced a small positive bias. The mean MLE for the
scale parameters were not very sensitive to right-censoring.

The sampling variance of the MLE for both the shape and scale parameters were somewhat sensitive to 
right-censoring, as evidenced by the decreasing width of the confidence intervals as a function of
increasing right-censoring times. However, the coverage probabilities of the confidence intervals
were generally well-calibrated for the shape and scale parameters except for the most extreme
cases of right-censoring.

#### Component Cause of Failure Masking {-}
For our well-designed system, the mean MLE for the shape parameters are insensitive to
masking, observing only a small positive bias for significant masking of $50\%$ or greater. However,
the calibration of the confidence intervals for the shape parameters were quite sensitive to masking,
reaching as low as $85\%$ for the most extreme cases of Bernoulli masking at $p \approx 0.7$.

#### Deviations from Well-Designed Systems {-}
When we deviate from a well-designed system, the MLE becomes much more sensitive to the data, exhibiting
more bias, more variability, and less well-calibrated confidence intervals. This is particularly true
for the shape parameters, which are more sensitive to the data than the scale parameters.

# Simulation Study: Full Weibull Model vs Reduced (Homogenous Shape) Model {#sec:full-vs-reduced}

In Section \ref{sec:weibull}, we developed the full model with Weibull
components and we conducted a sensitivity analysis of its MLE in
Section \ref{sec:sim_study}. Here, our focus shifts to a sensitivity
analysis aimed at understanding when it is appropriate to use the reduced model
that assumes homogeneity in the shape parameters of the components, as described
in Section \ref{sec:reduced-weibull}. The reduced model offers interpretability
(the series system is itself Weibull) and reduced estimator variability (only
$m+1$ parameters instead of $2m$), but it is must adequately describe the data.

We aim to assess the appropriateness of the reduced model under varying
sample sizes and shape parameters of the third component ($k_3$). We employ
a simulation study using the likelihood ratio test (LRT) for this purpose,
where the null hypothesis, $H_0$, assumes homogenous shape parameters.

We take the well-designed series system described in Table \ref{tab:series-sys},
and manipulate the shape parameter of the third
component ($k_3$) to cause the components to have different failure characteristics.
Recall that $k_3 = 1.1308$ corresponds to a *well-designed* series system, where
component shapes are reasoanbly aligned. We also vary the sample size $n$ to assess
the impact of sample size on the appropriateness of the reduced model.

Figure \ref{fig:lrt-contour} provides a contour plot with varying 
sample sizes along the $x$-axis, shape of component 3 along the $y$-axis,
and median $p$-value along the color scale.
The contour lines corresponding to $p$-values of $0.05$ and $0.1$ are
often used as a threshold for statistical significance. Points outside
of these contours in dark blue are indicative of significant evidence
against the null model and points inside of these contours in light blue
for $0.1$ and green for $0.05$ are indicative of insufficient evidence
against the null model.

Figure \ref{fig:lrt-samp-size} provides a plot of the median $p$-value
against the sample size for the well-designed system, where the shape
parameter of component 3 is fixed at $1.1308$. The $95$th percentile
of the $p$-values is also provided as a more stringent criterion for
statistical significance.

```{r samp-size-shape-lrt, results='asis', echo = F}
cat('
\\begin{figure}
    \\centering
    \\begin{minipage}{.5\\textwidth}
        \\centering
        \\includegraphics[width=1\\linewidth]{image/fig-lrt/contour_plot.pdf}
        \\captionof{figure}{$p$-Value vs Sample Size and Shape $k_3$}
        \\label{fig:lrt-contour}
    \\end{minipage}%
    \\begin{minipage}{.5\\textwidth}
        \\centering
        \\includegraphics[width=1\\linewidth]{image/fig-lrt/n-vs-p-value.pdf}
        \\captionof{figure}{$p$-Value vs Sample Size for Well-Designed System}
        \\label{fig:lrt-samp-size}
    \\end{minipage}
\\end{figure}
')
```

#### Sensitivity to Sample Size ($n$) {-}

- The sample size is an essential aspect of hypothesis testing, as it affects the
test's power, which is the probability of correctly rejecting the null hypothesis
when it is false. In the contour plot in Figure \ref{fig:lrt-contour}, as $n$
increases, the contours trend lower.
This indicates that larger samples result in smaller median $p$-values,
implying that the power of the LRT increases with the sample size.
However, its power is quite low for small samples, particularly for values of
$k_3$ somewhat close to the shape parameters of the other components in the system.

- Recall that in the well-designed series system, $k_3 = 1.1308$. In this case,
even very large sample sizes do not produce evidence
against the null model, indicating robust compatibility.

- In Figure \ref{fig:lrt-samp-size}, we fix $k_3$ at $1.1308$ and vary the sample size. 
The median $p$-value only manages to drop below the $0.05$ theshold with
sample sizes around $10000$. In the more stringent criterion given
by the $95$th percentile of the $p$-values, nearly $30000$ observations are
necessary to reject the null hypothesis in $95\%$ of the simulations.

#### Sensivity to Shape Parameter ($k_3$) {-}

- In Figure \ref{fig:lrt-contour}, for a given shape parameter, increasing the
sample size tends to decrease the median $p$-value. Larger samples provide
more information about the parameters, which increases the power of the LRT.

- The median $p$-values in the vicinity of the line $k_3 = 1.1308$ are
high across various sample sizes, indicating that the null model is a good fit.
As $k_3$ deviates from this line, the median $p$-value diminishes,
indicating increasing evidence against the null model.

## Implications and Recommendations

The power of the test for a well-designed series system is quite
low, requiring many thousands of observations before the test has
sufficient power to reject the null hypothesis. But, this is not
necessarily a bad thing. The reduced model is quite simple and
interpretable, and is by definition a good fit for a well-designed
series system. 

The findings suggest that the reduced model is particularly apt
when the system is well-designed, even for very large samples.
Practitioners should weigh the trade-offs between the simplicity
of the reduced model and the adequacy in describing the data,
with consideration of the available sample size and the characteristics
of the system being modeled.

For systems believed to be well-designed, employing the null model is
supported both statistically and practically due to its simplicity,
reduced estimator variability, and analytical tractability.
In the absence of prior information, or if the shape parameter
significantly diverges from the well-designed value, the choice between
models should be undertaken with caution. More complex models may be
favorable, especially with large sample sizes.

Future Work
===========

#### Relaxation of Conditions {-}

Investigate relaxations of Conditions 1, 2, and 3.
Condition 1 stipulates that the failed component is always in
the candidate set,
$$
    \Pr\{K_i \in \mathcal{C}_i\} = 1.
$$
Instead, we could model this as a probability, where the probability
of the failed component being in the candidate set is a function
of the failure time $T_i$ and the component cause of failure $K_i$,
$$
    \Pr\{K_i \in \mathcal{C}_i | K_i = j, T_i = t_i\} = g(j, t_i).
$$
Condition 2 stipulates that
$$
    \Pr\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\} = 
    \Pr\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j'\}
$$
for all $j, j' \in c_i$. We call this an *uninformed* candidate set,
since the conditional probability of the candidate set given the
failure time and component cause of failure is independent of the
component cause of failure. We could relax this condition to allow
for *informed* candidate sets, where the conditional probability
of the candidate set given the failure time and component cause of
failure is dependent on the component cause of failure.

In each of these violations or relaxations, we can either construct
a new likelihood model that takes this relaxation into account, or
we can use the existing likelihood model and assess the sensitivity
of the estimator to this violation. A potentially interesting way to
do the lattr is by using KL-divergence to measure the distance
between, for instance, the uninformed and informed candidate set models,
and then assess the sensitivity of the estimators to this distance.

#### Expanded Sensitivity Analyses {-}

- Assess the sensitivity of the reduced model to the presence of
masking and right-censoring. Since it has fewer parameters, it may
be more robust to these effects.

- Assess the sensitivity of the full and reduced models to the
number ($m$) of components in the series system.

#### Data Augmentation {-}

Assess the robustness of Data Augmentation (DA) as an implicit
prior, particularly for the reduced model, where we may adopt the
prior that the system is well-designed. By incorporating DA into
the procedure, we may be able to significantly improve the
performance of the estimator on extremely small samples.

Unlike a full Bayesian approach, where we would need to specify
a prior for the parameters, DA is an implicit prior that need not
be explicitly specified. It is a form of regularization that
improves the performance of the estimator by leveraging the
structure of the model and the data. It is particularly useful
for small samples, where the data is not sufficient to overcome
the masking and censoring effects.

#### Penalized Likelihood {-}

Assess the use of penalized likelihood methods instead of
DA as a form of regularization. For instance, we can add
a penalty term to the log-likelihood function that penalizes
the likelihood when the shape parameters are not close to
each other. Instead of simply using a reduced model, we can 
use a penalized likelihood approach to encourage the shape
parameters to be close to each other, but not necessarily
equal.

Conclusion
==========

This paper presented maximum likelihood methods for estimating
component reliability from masked failure data in series systems.
A likelihood model was derived accounting for right-censoring and
candidate sets masking the component cause of failure. Through
simulation studies, right-censoring and masking were found to bias
shape parameter estimates positively. The scale parameter estimates
were more robust. Overall, shape parameters were more sensitive to
the data and harder to estimate precisely than scale parameters.
Nonetheless, bootstrapping provided reasonably well-calibrated
confidence intervals even for small sample sizes.

A reduced model assuming homogeneous component shape parameters
was also assessed. This simplified model improves interpretability
as the system becomes Weibull, while reducing estimator variability.
The appropriateness of the reduced model was explored through an
LRT simulation study. For well-designed systems, the reduced model
showed excellent fit even for large samples. However, varying the
scale or shape of a single component quickly provided evidence
against the null model.

The reduced model thus appears favorable both statistically and
practically when systems are believed to be well-designed. However,
more complex models may be preferred given divergent component
properties or large samples. The choice between models should weigh
model simplicity against adequacy in describing available data.

Overall, this work provides a rigorous framework for quantifying
component reliability from limited observational data. The methods
demonstrated accurate and robust performance despite the significant
challenges introduced by masking and right-censoring. Future work
can focus on relaxing modeling assumptions and expanding the
sensitivity analysis. The techniques presented expand the range of
applications where component lifetime properties can be inferred
from system-level data.

# Appendix A: R Code For Log-likelihood Function {-}

The following code is the log-likelihood function for the Weibull series system
with a likelihood model that includes masked component cause of failure and right-censoring.
It is implemented in the R library `wei.series.md.c1.c2.c3` and is available on
[GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).
For clarity and brevity, we removed some of the functionality that is not relevant to the
analysis in this paper.

```{r, eval = FALSE}
#' Generates a log-likelihood function for a Weibull series system with respect
#' to parameter `theta` (shape, scale) for masked data with candidate sets
#' that satisfy conditions C1, C2, and C3 and right-censored data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Log-likelihood with respect to `theta` given `df`
loglik_wei_series_md_c1_c2_c3 <- function(df, theta) {
    n <- nrow(df)
    C <- md_decode_matrix(df, candset)
    m <- ncol(C)
    delta <- df[[right_censoring_indicator]]
    t <- df[[lifetime]]
    k <- length(theta)
    shapes <- theta[seq(1, k, 2)]
    scales <- theta[seq(2, k, 2)]

    s <- 0
    for (i in 1:n) {
        s <- s - sum((t[i] / scales)^shapes)
        if (delta[i]) {
            s <- s + log(sum(shapes[C[i, ]] / scales[C[i, ]] *
                (t[i] / scales[C[i, ]])^(shapes[C[i, ]] - 1)))
        }
    }
    s
}
```


# Appendix B: R Code For Score Function {-}

The following code is the score function (gradient of the log-likelihood function with respect to $\v\theta$) for the Weibull series system
with a likelihood model that includes masked component cause of failure and right-censoring.
It is implemented in the R library `wei.series.md.c1.c2.c3` and is available on [GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).

For clarity and brevity, we removed some of the functionality that is not relevant to the
analysis in this paper.

```{r score-code, eval = FALSE}
#' Computes the score function (gradient of the log-likelihood function) for a
#' Weibull series system with respect to parameter `theta` (shape, scale) for masked
#' data with candidate sets that satisfy conditions C1, C2, and C3 and right-censored
#' data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Score with respect to `theta` given `df`
score_wei_series_md_c1_c2_c3 <- function(df, theta) {
    n <- nrow(df)
    C <- md_decode_matrix(df, candset)
    m <- ncol(C)
    delta <- df[[right_censoring_indicator]]
    t <- df[[lifetime]]
    shapes <- theta[seq(1, length(theta), 2)]
    scales <- theta[seq(2, length(theta), 2)]
    shape_scores <- rep(0, m)
    scale_scores <- rep(0, m)

    for (i in 1:n) {
        rt.term.shapes <- -(t[i] / scales)^shapes * log(t[i] / scales)
        rt.term.scales <- (shapes / scales) * (t[i] / scales)^shapes

        # Initialize mask terms to 0
        mask.term.shapes <- rep(0, m)
        mask.term.scales <- rep(0, m)

        if (delta[i]) {
            cindex <- C[i, ]
            denom <- sum(shapes[cindex] / scales[cindex] * (t[i] / scales[cindex])^(shapes[cindex] - 1))

            numer.shapes <- 1 / t[i] * (t[i] / scales[cindex])^shapes[cindex] *
                (1 + shapes[cindex] * log(t[i] / scales[cindex]))
            mask.term.shapes[cindex] <- numer.shapes / denom

            numer.scales <- (shapes[cindex] / scales[cindex])^2 * (t[i] / scales[cindex])^(shapes[cindex] - 1)
            mask.term.scales[cindex] <- numer.scales / denom
        }

        shape_scores <- shape_scores + rt.term.shapes + mask.term.shapes
        scale_scores <- scale_scores + rt.term.scales - mask.term.scales
    }

    scr <- rep(0, length(theta))
    scr[seq(1, length(theta), 2)] <- shape_scores
    scr[seq(2, length(theta), 2)] <- scale_scores
    scr
}
```

# Appendix C: R Code For Simulation of Scenarios {-}

The following code is the Monte-carlo simulation code for running the scenarios
described in Section \ref{sec:sim_study}.

```{r bootstrap-sim-code, eval=FALSE}
#### Setup simulation parameters here ####
theta <- c(
    shape1 = 1.2576, scale1 = 994.3661,
    shape2 = 1.1635, scale2 = 908.9458,
    shape3 = NA, scale3 = 840.1141,
    shape4 = 1.1802, scale4 = 940.1342,
    shape5 = 1.2034, scale5 = 923.1631
)

shapes3 <- c(1.1308) # shape 3 true parameter values to simulate
scales3 <- c(840.1141) # scale 3 true parameter values to simulate
N <- c(30, 60, 100) # sample sizes to simulate
P <- c(.215) # masking probabilities to simulate
Q <- c(.825) # right censoring probabilities to simulate
R <- 1000L # number of simulations per scenario
B <- 1000L # number of bootstrap samples
max_iter <- 125L # max iterations for MLE
max_boot_iter <- 125L # max iterations for bootstrap MLE
n_cores <- detectCores() - 1 # number of cores to use for parallel processing
filename <- "data" # filename prefix for output files

#### Simulation code below here ####
library(tidyverse)
library(parallel)
library(boot)
library(algebraic.mle) # for `mle_boot`
library(wei.series.md.c1.c2.c3) # for `mle_lbfgsb_wei_series_md_c1_c2_c3` etc

file.meta <- paste0(filename, ".txt")
file.csv <- paste0(filename, ".csv")
if (file.exists(file.meta)) {
    stop("File already exists: ", file.meta)
}
if (file.exists(file.csv)) {
    stop("File already exists: ", file.csv)
}

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]
m <- length(shapes)

sink(file.meta)
cat("boostrap of confidence intervals:\n")
cat("   simulated on: ", Sys.time(), "\n")
cat("   type: ", ci_method, "\n")
cat("weibull series system:\n")
cat("   number of components: ", m, "\n")
cat("   scale parameters: ", scales, "\n")
cat("   shape parameters: ", shapes, "\n")
cat("simulation parameters:\n")
cat("   shapes3: ", shapes3, "\n")
cat("   scales3: ", scales3, "\n")
cat("   N: ", N, "\n")
cat("   P: ", P, "\n")
cat("   Q: ", Q, "\n")
cat("   R: ", R, "\n")
cat("   B: ", B, "\n")
cat("   max_iter: ", max_iter, "\n")
cat("   max_boot_iter: ", max_boot_iter, "\n")
cat("   n_cores: ", n_cores, "\n")
sink()

for (scale3 in scales3) {
    for (shape3 in shapes3) {
        for (n in N) {
            for (p in P) {
                for (q in Q) {
                    shapes[3] <- shape3
                    theta["shape3"] <- shape3

                    cat("[starting scenario: scale3 = ", scale3, ",
                        shape3 = ", shape3, ", n = ", n, ", p = ", p, ", q = ", q, "]\n")
                    tau <- qwei_series(p = q, scales = scales, shapes = shapes)

                    # we compute R MLEs for each scenario
                    shapes.mle <- matrix(NA, nrow = R, ncol = m)
                    scales.mle <- matrix(NA, nrow = R, ncol = m)
                    shapes.lower <- matrix(NA, nrow = R, ncol = m)
                    shapes.upper <- matrix(NA, nrow = R, ncol = m)
                    scales.lower <- matrix(NA, nrow = R, ncol = m)
                    scales.upper <- matrix(NA, nrow = R, ncol = m)

                    iter <- 0L
                    repeat {
                        retry <- FALSE
                        tryCatch(
                            {
                                repeat {
                                    df <- generate_guo_weibull_table_2_data(
                                        shapes = shapes, scales = scales, n = n, p = p, tau = tau
                                    )

                                    sol <- mle_lbfgsb_wei_series_md_c1_c2_c3(
                                        theta0 = theta, df = df, hessian = FALSE,
                                        control = list(maxit = max_iter, parscale = theta)
                                    )
                                    if (sol$convergence == 0) {
                                        break
                                    }
                                    cat("[", iter, "] MLE did not converge, retrying.\n")
                                }

                                mle_solver <- function(df, i) {
                                    mle_lbfgsb_wei_series_md_c1_c2_c3(
                                        theta0 = sol$par, df = df[i, ], hessian = FALSE,
                                        control = list(maxit = max_boot_iter, parscale = sol$par)
                                    )$par
                                }

                                # do the non-parametric bootstrap
                                sol.boot <- boot(df, mle_solver, R = B, parallel = "multicore", ncpus = n_cores)
                            },
                            error = function(e) {
                                cat("[error] ", conditionMessage(e), "\n")
                                cat("[retrying scenario: n = ", n, ", p = ", p, ", q = ", q, "\n")
                                retry <<- TRUE
                            }
                        )
                        if (retry) {
                            next
                        }
                        iter <- iter + 1L
                        shapes.mle[iter, ] <- sol$par[seq(1, length(theta), 2)]
                        scales.mle[iter, ] <- sol$par[seq(2, length(theta), 2)]

                        tryCatch(
                            {
                                ci <- confint(mle_boot(sol.boot), type = ci_method, level = ci_level)
                                shapes.ci <- ci[seq(1, length(theta), 2), ]
                                scales.ci <- ci[seq(2, length(theta), 2), ]
                                shapes.lower[iter, ] <- shapes.ci[, 1]
                                shapes.upper[iter, ] <- shapes.ci[, 2]
                                scales.lower[iter, ] <- scales.ci[, 1]
                                scales.upper[iter, ] <- scales.ci[, 2]
                            },
                            error = function(e) {
                                cat("[error] ", conditionMessage(e), "\n")
                            }
                        )
                        if (iter %% 5 == 0) {
                            cat("[iteration ", iter, "] shapes = ", shapes.mle[iter, ], "scales = ", scales.mle[iter, ], "\n")
                        }

                        if (iter == R) {
                            break
                        }
                    }

                    df <- data.frame(
                        n = rep(n, R), rep(scale3, R), rep(shape3, R),
                        p = rep(p, R), q = rep(q, R), tau = rep(tau, R), B = rep(B, R),
                        shapes = shapes.mle, scales = scales.mle,
                        shapes.lower = shapes.lower, shapes.upper = shapes.upper,
                        scales.lower = scales.lower, scales.upper = scales.upper
                    )

                    write.table(df,
                        file = file.csv, sep = ",", row.names = FALSE,
                        col.names = !file.exists(file.csv), append = TRUE
                    )
                }
            }
        }
    }
}
```


# Appendix D: Bernoulli Candidate Set Model {-#app:cand-model}

```{r, eval = FALSE}
#' Bernoulli candidate set model is a particular type of *uninformed* model.
#' This model satisfies conditions C1, C2, and C3.
#' The failed component will be in the corresponding candidate set with
#' probability 1, and the remaining components will be in the candidate set
#' with probability `p` (the same probability for each component). `p`
#' may be different for each system, but it is assumed to be the same for
#' each component within a system, so `p` can be a vector such that the
#' length of `p` is the number of systems in the data set (with recycling
#' if necessary).
#'
#' @param df masked data.
#' @param p a vector of probabilities (p[j] is the probability that the jth
#'          system will include a non-failed component in its candidate set,
#'          assuming the jth system is not right-censored).
md_bernoulli_cand_c1_c2_c3 <- function(df, p) {
    n <- nrow(df)
    p <- rep(p, length.out = n)
    Tm <- md_decode_matrix(df, comp)
    m <- ncol(Tm)
    Q <- matrix(p, nrow = n, ncol = m)
    Q[cbind(1:n, apply(Tm, 1, which.min))] <- 1
    Q[!df[[right_censoring_indicator]], ] <- 0
    df %>% bind_cols(md_encode_matrix(Q, prob))
}
```

# Appendix E: Series System Quantile Function {-#app:series-quantile}

```{r, eval=FALSE}
#' Quantile function (inverse of the cdf).
#' By definition, the quantile `p` * 100% for a strictly monotonically increasing
#' cdf `F` is the value `t` that satisfies `F(t) - p = 0`.
#' We solve for `t` using newton's method.
#'
#' @param p vector of probabilities.
#' @param shapes vector of weibull shape parameters for weibull lifetime
#'               components
#' @param scales vector of weibull scale parameters for weibull lifetime
#'               components
qwei_series <- function(p, shapes, scales) {
    t0 <- 1
    repeat {
        t1 <- t0 - sum((t0/scales)^shapes) + log(1-p) /
            sum(shapes*t0^(shapes-1)/scales^shapes)
        if (abs(t1 - t0) < tol) {
            break
        }
        t0 <- t1
    }
    return(t1)
}
```


# Appendix F: Likelihood Ratio Test {-#app:lrt}

In order to determine if a reduced model (e.g., Weibull series system in
which all of the shape parameters are homogeneous) is appropriate,
a hypothesis test test may be conducted to determine if there is
statistically significant evidence in support of the null hypothesis $H_0$,
e.g., that all of the shape parameters are homogeneous.

Given that we employ a well-defined likelihood model, the likelihood ratio test
(LRT) is a good choice. The LRT statistic is given by
$$
\Lambda = -2 (\ell_R - \ell_F)
$$
where $\ell_R$ is the log-likelihood of the null (reduced) model (the log-likelihood of
the reduced model evaluated at its MLE given a random sample) and $\ell_F$ is the log-likelihood
of the full model. Under the null model, the LRT statistic is asymptotically distributed chi-squared
with $m-1$ degrees of freedom, where $m$ is the number of components in the series system,
$$
\Lambda \sim \chi^2_{m-1}.
$$
If the LRT statistic is greater than the critical value of the chi-squared distribution with $m-1$
degrees of freedom, $\chi^2_{m-1, 1-\alpha}$, where $\alpha$ denotes the significance level, then
we find the data to be incompatible with the null hypothesis $H_0$.
