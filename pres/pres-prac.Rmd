---
title: "Reliability Estimation in Series Systems"
subtitle: "Maximum Likelihood Techniques for Right-Censored and Masked Failure Data"
author: Alex Towell
output:
  beamer_presentation:
    theme: Boadilla
    slide_level: 2
  powerpoint_presentation:
    slide_level: 2
header-includes:
   - \usepackage{tikz}
   - \usepackage{caption}
   - \usepackage{amsthm}
   - \usepackage{xcolor}
   - \renewcommand{\v}[1]{\boldsymbol{#1}}
   - \theoremstyle{definition}
   - \newtheorem{condition}{Condition}
   - \theoremstyle{plain}
   - \setbeameroption{show notes on second screen}
bibliography: ../refs.bib
link-citations: true
natbiboptions: "numbers"
csl: ../ieee-with-url.csl
---

## Context & Motivation

**Reliability** in **series systems** is like a chain's strength -- determined
by its weakest link.

- Essential for system design and maintenance.

**Main Goal**: Estimate individual component reliability from *failure data*.

**Challenges**:

- *Masked* component-level failure data.
- *Right-censoring* in system-level failure data.

**Our Response**:

- Derive techniques to interpret such ambiguous data.
- Aim for precise and accurate reliability estimates for individual components
using maximum likelihood estimation (MLE) and bootstrapped (BCa) confidence
intervals.

\note{
\begin{itemize}
\item \textbf{Chain Analogy}: Think of a series system as a chain. Its reliability,
just like a chain's strength, is determined by its weakest link or component.
When any component fails, the whole system does.
\item \textbf{Reliability Importance}: Understanding the reliability of each component is essential for the design and
maintenance of these systems. 
\item \textbf{Data Challenge}: The data we rely on can come with its own challenges. We
sometimes encounter ambiguous data like right-censored information or masked
component-level failures, where we don't know precisely which component failed.
\item \textbf{Aim}: Our goal is to interpret such ambiguous data and provide
accurate reliability estimates for each component, which includes providing
correctly specified 95% CIs, meaning they cover the true parameter value around
95% of the time. We do this using MLE and bootstrap the confidence intervals
using the BCa method.
\end{itemize}
}

## Core Contributions

**Likelihood Model** for **series systems**.

- Accounts for *right-censoring* and *masked component failure*.
- Can easily incorporate additional failure data.

**Specifications of Conditions**:

- Assumptions about the masking of component failures.
- Simplifies and makes the model more tractable.

**Simulation studies**:

- Components with *Weibull* lifetimes.

- Evaluate MLE and confidence intervals under different scenarios.

**R Library**: Methods available on GitHub.

- [github.com/queelius/wei.series.md.c1.c2.c3](https://github.com/queelius/wei.series.md.c1.c2.c3)

\note{
Our core contributions can be broken down into several parts:
\begin{itemize}
\item \textbf{Likelihood model}: We've derived a likelihood model for series systems that accounts for the
ambiguous data.
\item \textbf{Explain conditions}: We've clarified the conditions this model assumes about the masking of component failures. These conditions simplify the model and make it more tractable.
\item \textbf{Validated with simulation study}: We've validated our model with extensive simulations using Weibull distributions to gauge its performance under various scenarios.
\item \textbf{R Library}: For those interested, we made our methods available in an R Library hosted on GitHub.
\end{itemize}
}

# Series System

\begin{figure}
\centering
\resizebox{0.7\textwidth}{!}{\input{../image/series.tex}}
\end{figure}

**Critical Components**: Complex systems often comprise *critical* components.
If any component fails, the entire system fails.

- We call such systems *series systems*.
- **Example**: A car's engine and brakes.

**System Lifetime** is dictated by its shortest-lived component:
$$
T_i = \min(T_{i 1}, \ldots, T_{i 5})
$$

- Where: $T_i$ and $T_{i j}$ are the system and component lifetimes for the
$i$\textsuperscript{th} system and $j$\textsuperscript{th} component, respectively.

\note{
\begin{itemize}
\item \textbf{Critical Components}: Many complex systems have components that
are essential to their operation.
\item \textbf{Series System}: If any of these components fail, the entire system
fails. We call these series systems.
\item \textbf{Car}: Think of a car - if the engine or brakes fail, the car can't
be operated.
\item \textbf{Lifetime}: Its lifetime is the lifetime of its shortest-lived
component.
\item \textbf{Notation}: For reference, we show the math notation we'll use
throughout the talk.
\end{itemize}
}

## Reliability Function
**Reliability Function** represents the probability that a system or component functions
beyond a specified time.

- Essential for understanding longevity and dependability.

**Series System Reliability**: Product of the reliability of its components:
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$

- Here, $R_{T_i}(t;\v\theta)$ and $R_j(t;\v{\theta_j})$ are the reliability
  functions for the system $i$ and component $j$, respectively.

**Relevance**:

  - Forms the foundation for most reliability studies.
  - Integral to our likelihood model, e.g., right-censoring events.

\note{
\begin{itemize}
\item \textbf{Reliability Function} The reliability function tells us the chance
a component or system functions past a specific time. It's our key metric for longevity.
\item \textbf{Product of Component Reliability}: In a series system, the
overall reliability is the product of its component reliabilities. So, if even
one component has a low reliability, it can impact the whole system.
\item \textbf{Relevance}: Why does this matter to us? This concept is foundational to our studies,
especially when we're handling right-censored data.
\end{itemize}
}

## Hazard Function: Understanding Risks

**Hazard Function**: Measures the immediate risk of failure at a given time,
assuming survival up to that moment.

- Reveals how the risk of failure evolves over time.
- Guides maintenance schedules and interventions.

**Series System Hazard Function** is the sum of the hazard functions of its components:
$$
h_{T_i}(t;\v{\theta}) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$

  - Components' risks are additive.

\note{
\textbf{Hazard Function}: Let's shift focus to the hazard function. Essentially,
it's a way to gauge the immediate risk of failure, especially if it's been
functioning up until that point.

\textbf{Series Hazard Function} Lastly, the hazard function for a series system
is just the sum of the hazard functions of its components.

\textbf{Additive}: We see that the component risks are additive.
}

## Joint Distribution of Component Failure and System Lifetime
Our likelihood model depends on the **joint distribution** of the system
lifetime and the component that caused the failure.

- **Formula**: Product of the failing component's hazard function and the system
reliability function:
$$
f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta).
$$
- **Single Point of Failure**: A series system fails due to one component's malfunction.
- **Representation**: 
  - $K_i$: Component causing the $i$\textsuperscript{th} system's failure.
  - $h_j(t;\v{\theta_j})$: Hazard function for the $j$\textsuperscript{th} component.

\note{
\begin{itemize}
\item \textbf{Joint Distribution} In our likelihood model, understanding the
joint distribution of a system's lifetime and the component that led to its
failure is fundamental.
\item \textbf{Formula}: It is the product of the failing component's hazard
function and the system reliability function.
\item \textbf{Unique Cause}: Which emphasizes that in a series system, failure
can be attributed to a single component's malfunction.
\item \textbf{Notation}: Here, $K_i$ denotes the component responsible for the failure.
\end{itemize}
}


## Component Failure & Well-Designed Series Systems

The **marginal probability** of component failure helps predict the cause of
failure.

- **Derivation**: Marginalize the joint distribution over the system lifetime:
$$
\Pr\{K_i = j\} = E_{\v\theta} \biggl[ \frac{h_j(T_i;\v{\theta_j})} {h_{T_i}(T_i ; \v{\theta_l})} \biggr].
$$

**Well-Designed Series System**: Components exhibit comparable chances of
causing system failures.

- **Relevance**: Our simulation study employs a (reasonably) well-designed
series system.

\note{
\begin{itemize}
\item \textbf{Marginal}: We can use this joint distribution to calculate the marginal probability
of component failure.
\item \textbf{Expected Value}: When we do so, we find that it is the expected value of
the ratio of component and system hazard functions.
\item \textbf{Well-Designed}: We say that a series system is \emph{well-designed}
if each components has a comparable chance of failing.
\item \textbf{Relevance}: Our simulation study is based on a reasonably
well-designed series system.
\end{itemize}
}

# Likelihood Model

Our **likelihood model** handles the following data:

- **Right-Censored**: Experiment ends before failure ($\tau$): $S_i = \min(\tau, T_i)$.
- **Event Indicator**: $\delta_i = 1_{\{T_i < \tau\}}$
- **Masked Failure**: Candidate set $\mathcal{C}_i$ of components masking
the component cause of failure.

**Example**:

System | Right-Censored Lifetime | Event Indicator | Candidate Set  |
------ | ----------------------- | --------------- | -------------- |
   1   | $1.1$                   | 1               | $\{1,2\}$      |
   2   | $5$                     | 0               | $\emptyset$    |

  - **System 1**: Failure observed at $1.1$. Component 1 or 2 failed.
  - **System 2**: Right-censored at $\tau = 5$. No candidate set.

\note{
\begin{itemize}
\item \textbf{Data}: Our likelihood model deals with right-censoring and masked cause of failure.
\item \textbf{Right-censoring} occurs when the experiment ends before the system fails.
\item For \textbf{notation}, we let $\tau$ be the right-censoring time and $\delta_i$
be the event indicator. If $\delta_i = 0$, the system is right-censored.
\item \textbf{Masking} occurs when we observe a failure but we don't know the
precise component cause. Instead, we observe a candidate set of components that
could have failure.
\item Here's an example of observed data.
\item \textbf{System 1}: We see that the system failed at $1.1$. We don't know
which component failed, but we know it was either component 1 or 2.
\item \textbf{System 2}: The experiment ended before the system failed. Since no
component failed, the candidate set is empty.
\item Candidate sets may have a single component too, in which case there is
no masking.
\end{itemize}
}

## Data Generating Process
DGP is underlying process that generates observed data:

- **\textcolor{green}{Green}** elements are observed.
- **\textcolor{red}{Red}** elements are unobserved (latent).
- Candidate set ($\mathcal{C}_i$) related to components ($T_{i j})$
  and other unknowns.
  - Problematic. Seek a simple model valid under certain assumptions.

\begin{figure}
\centering
\resizebox{0.65\textwidth}{!}{\input{../image/dep_model.tex}}
\end{figure}

\note{
This is the data generating process for our observed data.

It is a graphical model, where green elements are observed and red elements are
unobserved.

The candidate sets are related to the component lifetimes and potentially many
other factors. The distribution of candidate sets could be quite complex
and difficult to model. We seek a simple model that is valid under certain
assumptions. We'll discuss these assumptions soon.

We see that the right-censoring time is independent of the component lifetimes
and parameters in our model. This is a reasonable assumption in many cases.
}

## Likelihood Contribution: Right-Censoring

**Total likelihood** is the product of the **likelihood contributions** of each
system:
$$
L(\v\theta) = \prod_{i=1}^n L_i(\v\theta).
$$

**Right-Censoring**: The likelihood contribution of the $i$\textsuperscript{th}
when it is right-censored ($\delta_i = 0$) is proportional to the system reliability:
$$
L_i(\v\theta) \propto \prod_{l=1}^m R_l(\tau;\v{\theta_l}).
$$

**Assumptions**: Right-censoring time $\tau$ independent of component lifetimes
and parameters.

\note{
\begin{itemize}
\item \textbf{Total likelihood} is the product of the likelihood contributions
of each system.
\item \textbf{Right-Censored} Let's look at the likelihood contribution for
right-censored data.
\item When a system is right-censored, the likelihood contribution is proportional to
the system reliability.
\item \textbf{Assumption} In our model, we assume that the right-censoring time
is independent of the component lifetimes and parameters.
\item This is a \textbf{reasonable} assumption in many cases.
\end{itemize}
}

## Likelihood Contribution: Masked Failures

Likelihood contribution ($\delta_i = 1$) proportional to the product of the
system reliability and the sum of the component hazards in the masking set:
$$
L_i(\v\theta) \propto R_{T_i}(s_i;\v\theta) \sum_{j\in c_i} h_j(s_i;\v{\theta_j}).
$$

**Candidate Set Contains Failed Component**: The candidate set, $\mathcal{C}_i$,
always includes the failed component: $\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1$.

**Equal Probabilities Across Candidate Sets**: The probability of of the
candidate set is constant across different components within it, i.e., for every
$j,j' \in c_i$:
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j, T_i = t_i\} = \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j', T_i = t_i\}.
$$

**Masking Probabilities Independent of Parameters**: The masking probabilities
when conditioned on $T_i$ and $K_i$ aren't functions of $\v{\theta}$.

\note{
Masking occurs when a system fails but the precise failed component is ambiguous. 

To make problem more tractable, we introduce certain conditions.

Reasonable for many realistic situations.
}


## Likelihood Contribution: Masked Failures (cont.)


**Joint distribution** of $T_i$, $K_i$, and $\mathcal{C}_i$:
$$
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta}) = f_{T_i,K_i}(t_i,j;\v{\theta})\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$
**Marginalize** over $K_i$ and apply Conditions 1, 2, and 3:
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l}) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
**Result**: We don't need to model the distribution of the candidate sets $\mathcal{C}_i$.

  - $L_i(\v\theta) \propto f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta})$.

## Methodology: Maximum Likelihood Estimation

**Maximum Likelihood Estimation (MLE)**: Maximize the likelihood function:
$$
\hat{\v\theta} = \arg\max_{\v\theta} L(\v\theta).
$$

**Solution**: Numerically solved system of equations for $\hat{\v\theta}$:
$$
\nabla_{\v\theta} \ell(\hat{\v\theta}) = \v{0}.
$$

\note{
MLE is all about finding the parameter values that make the observed data most probable. It's a way of fitting our model to our data.

But the computation can get tricky. We need to solve some equations numerically. 

To simplify, we often work with the log-likelihood, which is just the log of the likelihood.}

## Bootstrap Method: Confidence Intervals

**Sampling Distribution of MLE**: Asymptotic normality is useful for
constructing confidence intervals.

  - **Issue**: May need large samples before asymptotic normality holds.

**Bootstrapped CIs**: Resample data and find an MLE for each. Use the
distribution of the bootstrapped MLEs to construct CIs.

- **Percentile Method**: Simple and intuitive.

**Correctly Specified CIs**: A coverage probability close to the nominal level of $95\%$.

  - **Issue**: Coverage probability may be too low or too high.

- **Adjustments**: To improve coverage probability, we use the BCa method to
  adjust for bias (bias correction) and skewness (acceleration) in the estimate.
  Coverage probabilities above $90\%$ acceptable.

\note{
We need a way to measure the uncertainty in our estimates.

This is where confidence intervals come in. They help us pin down the likely range of our estimates.

We Bootstrap the confidence intervals since there is likely to be a lot of
bias and variability in our estimates due to the masking and censoring
in our small data sets.

So, to ensure our intervals are accurate, we use the BCa method to adjust the
intervals for bias and skewness in the estimate.
  
A coverage probability above $90\%$ is acceptable.
}

## Challenges with MLE on Masked Data

We discovered some challenges with the MLE on masked data.

**Convergence Issues**: Flat likelihood regions were observed due to the
  ambiguity in the masked data and small sample sizes.
  
**Bootstrap Issues**: Bootstrap relies on the Law of Large Numbers.

- Bootstrap might not represent the true variability, leading to inaccuracies.

- Due to right censoring and masking, the effective sample size is reduced.

**Mitigation**: We discard non-convergent samples for the MLE on original data,
but keep all resamples for the bootstrap.

- This ensures that the bootstrap for "good" data is representative of the
  variability in the original data.

- We report convergence rates in our simulation study.

\note{
Like any model, ours has its challenges. For instance, the ambiguity in masked data, combined with small sample sizes, can create flat regions in our likelihood, causing convergence issues. Also, bootstrapping may not always capture the true variability in our data. But we've taken steps to address these. We discard non-convergent samples for MLE on original data but retain all for bootstrapping, ensuring variability is captured.
}

# Simulation Study

## Series System: Weibull Components

The lifetime of the $j$\textsuperscript{th} component in the $i$\textsuperscript{th} system:
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j)
$$

- $\lambda_j$ is the *scale* parameter 
- $k_j$ is the *shape* parameter, where:
  - $k_j < 1$: Indicates infant mortality.
  - $k_j = 1$: Indicates random failures.
  - $k_j > 1$: Indicates wear-out failures.

Recall that for a series system:

- **Series Reliability** is the product of the component reliabilities.
- **Hazard** is the sum of the component hazard functions.
- **Likelihood**: $L(\v\theta) \propto \prod_{i=1}^n R_{T_i}(t_i;\v\theta) \left[\sum_{j \in c_i} h_j(t_i;\v{\theta_j})\right]^{\delta_i}$.

\note{
\begin{itemize}
\item \textbf{Weibull}: We model a series system with Weibull components.
\item \textbf{Component Functions}: Hazard and reliability functions are well-known for Weibull.
\item \textbf{Shape} parameter tells us a lot about the failure characteristics.
\item \textbf{Increasing}: When the function is increasing, think of it as
wearing-out over time.
\item \textbf{Decreasing}: If it's decreasing, it usually signals some
early-life challenges.
\item \textbf{Series System}: Recall that for a series system, the reliability
is the product of the component reliabilities and the hazard function is the
sum of the component hazard functions.
\item \textbf{Likelihood}: The likelihood is the same as before, we've just
reproduced it here.
\end{itemize}
}

## Well-Designed Series System

This study is centered around the following *well-designed series system*
with Weibull component lifetimes:

| Component | Shape | Scale  | $\Pr\{K_i\}$ |
|-----------|-------|--------|--------------|
| 1         | 1.26  | 994.37 | 0.17         |
| 2         | 1.16  | 908.95 | 0.21         |
| 3         | 1.13  | 840.11 | 0.23         |
| 4         | 1.18  | 940.13 | 0.20         |
| 5         | 1.20  | 923.16 | 0.20         |


\note{
\begin{itemize}
\item \textbf{Well-Designed}: This study is centered around a well-designed
series system with Weibull component lifetimes.
\item \textbf{Parameters}: We show the parameters for each component.
\item \textbf{Shape}: We see that the shape parameter is greater than 1 for
each component, indicating wear-out failures.
\item \textbf{Probability}: We also show the probability of each component
being the cause of failure.
\item \textbf{Uniform}: The probabilities are comparable, which is what we
want for a well-designed system.
\end{itemize}
}

## Performance Metrics

**Objective**: Evaluate the MLE and BCa confidence intervals' performance across
various scenarios.

- **MLE Evaluation**:
  - **Accuracy**: Proximity of the MLE's expected value to the actual value. 
  - **Precision**: Consistency of the MLE across samples.

- **BCa Confidence Intervals Evaluation**:
  - **Accuracy**: Confidence intervals (CIs) should cover true parameters
    around $95\%$ of the time.
      - Coverage probability (CP)
  - **Precision**: Assessed by the width of the CIs.
  
Both accuracy and precision are crucial for confidence in the analysis.

## Data Generation

We generate data for $n$ systems with $5$ components each.
We satisfy the assumptions of our likelihood model by generating data as follows:

- **Right-Censoring Model**: Right-censoring time set at a known value,
parameterized by the quantile $q$.

  - Satisfies the assumption that the right-censoring time is independent of
    component lifetimes and parameters.

- **Masking Model**: Using a *Bernoulli masking model* for component cause of failure,
parameterized by the probability $p$.

  - Satisfies masking Conditions 1, 2, and 3.

## Scenario: Impact of Right-Censoring

Vary the right-censoring quantile ($q$): $60\%$ to $100\%$.
Fixed the parameters: $p = 21.5\%$ and $n = 90$.

### Background
- **Right-Censoring**: No failure observed.
- **Impact**: Reduces the effective sample size.
- **MLE**: Bias and precision affected by censoring.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.

## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-tau-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-tau-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/q_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: CIs converge to $95\%$. Scale parameters better calibrated.
- **Convergence Rate**: Increases as right-censoring reduces.

## Conclusion
- MLE precision improves, bias drops with decreased right-censoring.
- BCa CIs perform well, particularly for scale parameters.
- MLE of most reliable component more affected by right-censoring.

## Impact of Masking Probability
Vary the masking probability $p$: $0.1$ to $0.7$.
Fixed the parameters: $q = 0.825$ and $n = 90$.

### Background
- **Masking** adds ambiguity in identifying the failed component.
- Impacts of masking on MLE:
  - **Ambiguity**: Higher $p$ increases uncertainty in parameter adjustment.
  - **Bias**: Similar to right-censoring, but affected by both $p$ and $q$.
  - **Precision**: Reduces as $p$ increases.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.45\textwidth}
```{r fig-p-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.45\textwidth}
```{r fig-p-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/p_vs_convergence.pdf")
```
\end{minipage}
\end{figure}


**Calibration**: Caution advised for severe masking with small samples.

- Scale parameters maintain coverage up to $p = 0.7$.

- Shape parameters drop below $90\%$ after $p = 0.4$.

**Convergence Rate**: Reduces after $p > 0.4$, consistent with CP behavior.


## Conclusion
- Masking influences MLE precision, coverage probability, and introduces bias.
- Despite significant masking, scale parameters have commendable CI coverage.

## Impact of Sample Size
Assess the impact of sample size on MLEs and BCa CIs.

- Vary sample size $n$: $50$ to $500$
- Parameters: $p = 0.215$, $q = 0.825$

### Background
- **Sample Size**: Number of systems observed.
- **Impact**: More data reduces uncertainty in parameter estimation.
- **MLE**: Mitigates biasing effects of right-censoring and masking.

## Both Scale and Shape Parameters

\begin{figure}
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.3-mle.pdf")
```
\end{minipage}
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

## Parameters

- **Dispersion**:
  - Dispersion reduces with $n$—indicating improved precision.
  - Disparity observed between components $k_1, \lambda_1$ and $k_3, \lambda_3$.
- **Bias**:
  - High positive bias initially, but diminishes around $n=250$.
  - Enough sample data can counteract right-censoring and masking effects.

- **Median-Aggregated CIs**:
  - CIs tighten as $n$ grows—showing more consistency.
  - Upper bound more dispersed than lower, reflecting the MLE bias direction.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.4\textwidth}
```{r fig-n-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.4\textwidth}
```{r fig-n-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/n_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: 
  - CIs are mostly above $90\%$ across sample sizes.
  - Converge to $95\%$ as $n$ grows.
  - Scale parameters have better coverage than shape.

- **Convergence Rate**:
  - Improves with $n$, surpassing $95\%$ for $n \geq 100$.
  - Caution for estimates with $n < 100$ in specific setups.

## Conclusion
- Sample size significantly mitigates challenges from right-censoring and masking.
- MLE precision and accuracy enhance notably with growing samples.
- BCa CIs become narrower and more reliable as sample size increases.

# Conclusion

## Part 1

### Key Findings
- Employed maximum likelihood techniques for component reliability estimation in series systems with masked failure data.
- Methods performed robustly despite masking and right-censoring challenges.

### Simulation Insights
- Right-censoring and masking introduce positive bias; more reliable components are most affected.
- Shape parameters harder to estimate than scale.
- Large samples can counteract these challenges.

## Part 2

### Confidence Intervals
- Bootstrapped BCa CIs demonstrated commendable coverage probabilities, even in smaller sample sizes.
  
### Takeaways
- Framework offers a rigorous method for latent component property estimation from limited observational data.
- Techniques validated to provide practical insights in diverse scenarios.
- Enhanced capability for learning from obscured system failure data.

# Discussion

