---
title: "Reliability Estimation in Series Systems"
subtitle: "Maximum Likelihood Techniques for Right-Censored and Masked Failure Data"
author: Alex Towell
output:
  beamer_presentation:
    theme: Boadilla
    slide_level: 2
  powerpoint_presentation:
    slide_level: 2
header-includes:
   - \usepackage{tikz}
   - \usepackage{caption}
   - \usepackage{amsthm}
   - \renewcommand{\v}[1]{\boldsymbol{#1}}
   - \theoremstyle{definition}
   - \newtheorem{condition}{Condition}
   - \theoremstyle{plain}
   - \setbeameroption{show notes on second screen}
bibliography: ../refs.bib
link-citations: true
natbiboptions: "numbers"
csl: ../ieee-with-url.csl
---

## Context & Motivation

**Reliability** is a critical metric in reliability analysis.

- Crucial for system design and maintenance.

**Challenge**: We often only have system-level failure data.

  - Masked and right-censored data obscure reliability estimates.

  - Makes it difficult to estimate component reliability.

- Need robust techniques to decipher this data and make accurate estimations.

\note{
Reliability is a fundamental concept in systems analysis.

Think of it as a measure of trustworthiness.

But the data we have to analyze reliability is often limited.

We might only see system-level failures, but not component-level details or
only ambiguous component-level details.

Imagine a car breaking down but not knowing exactly which part caused the failure.

This makes estimating individual component reliability challenging.

Our goal is to use this limited data and provide accurate reliability estimates
of components.
}

## Core Contributions
- Derivation of a likelihood model that accounts for right-censoring and masking.
  - Easy to add more failure data via likelihood contribution model.
  - R Library: [github.com/queelius/wei.series.md.c1.c2.c3](https://github.com/queelius/wei.series.md.c1.c2.c3)
- Clarification of the assumptions required for the likelihood model.
- Simulation studies with Weibull distributed component lifetimes.
  - Assess performance of MLE and BCa confidence intervals under various
    scenarios.

\note{
Our core contributions can be broken into three parts.

First we've derived a model to account for the complexities in failure data, like right-censoring and masking.

Imagine trying to solve a puzzle with missing pieces; our model helps fill in the gaps.

Second, we've clarified what assumptions we need to make this model work.

And third, we've simulated various scenarios using Weibull distributions to see how our model performs.

Also, for those interested in implementation, we've made our methods available in an R Library.
}

# Series System

\begin{figure}
\centering
\resizebox{0.7\textwidth}{!}{\input{../image/series.tex}}
\end{figure}

A lot of complex systems have *critical* components $(x_1, \ldots, x_5)$.

- One component fails, the system fails.
- We call these *series systems*.
- Let $T_i$ be the lifetime of system $i$ and $T_{i j}$ is the lifetime of component $j$ in system $i$.
- The system lifetime is the minimum of the component lifetimes:
$$
T_i = \min(T_{i 1}, \ldots, T_{i 5})
$$

\note{
Many complex systems have components that are critical for their functioning.

We depict this in the graph above, with five components.

Think of a car's engine or brakes. If one of these components fails, the entire system goes down.

We call these series systems.

For notation, we let $T_i$ be the lifetime of system $i$ and $T_{i j}$ be the lifetime of component $j$ in system $i$.

Then, the lifetime of the system is the minimum of the component lifetimes.
}

## Reliability Function: System Longevity
**Definition**: Represents the probability that a system or component functions
beyond a specified time $t$:
$$
R_{T_i}(t) = \Pr\{T_i > t\}.
$$

**Interpretation**:

- A high reliability value indicates a lower probability of failure.
- Essential for understanding the longevity and dependability of systems and components.

**Series System Reliability**: Product of the reliabilities of its components:
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$

**Relevance**: Forms the basis for most reliability analyses and helps in
  making informed decisions about system design and maintenance.

  - Directly used in our likelihood model for right-censoring events.

\note{
At its core, the reliability function tells us the probability a system will function beyond a certain time, $t$.

It tells us how dependable a system is.

In series systems, the reliability is the product of its component reliabilities.

This function lays the groundwork for our analysis and helps in making strategic decisions about design and upkeep.

It's also directly used in our likelihood model for right-censoring events.
}

## Hazard Function: Failures Characteristics

**Definition**: Instantaneous failure rate at a specific time, given survival up
to that point:
$$
h_{T_i}(x) = \frac{f_{T_i}(t)}{R_{T_i}(t)}.
$$

**Interpretation**

- A tool to understand how failure risk evolves over time.
- Guides maintenance schedules and interventions.
- Failure characteristics:
  - Rising: wear-out (aging).
  - Declining: infant mortality (defects).
  - Constant: random (accidents).

**Series System Hazard Function**:  Sum of the hazard functions of its components:
$$
h_{T_i}(t;\v{\theta}) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$

\note{
The hazard function, on the other hand, gives us insight into the risk of failure over time.

It's like a health monitor.

It helps guide maintenance schedules and predict when failures might occur.

The hazard function can take on different failure characteristics.

It can rise, indicating wear-out failures, or decline, indicating infant mortality, or stay constant, indicating random failures.

In a series system, the hazard function is simply the sum of its components' hazards.
}

## Component Cause of Failure

We must understand the cause of failure in order to estimate component
reliability.

Let $K_i$ denote component cause of failure of $i$\textsuperscript{th} system.

**Joint Distribution of System Lifetime and Component Cause of Failure**: Understanding why something fails is crucial for estimating its reliability.

- To predict component failure causes, we need the joint distribution of the system lifetime and the component cause of failure:
$$
f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta).
$$
- **Relevance**: Important in our likelihood model for contributions from
masked failures.

**Probability of Component Failure:** Component $j$ causes failure:
$$
\Pr\{K_i = j\} = E_{\v\theta} \biggl[ \frac{h_j(T_i;\v{\theta_j})} {h_{T_i}(T_i ; \v{\theta_l})} \biggr].
$$
- Marginalize the joint distribution over the system lifetime.

**Conditional Probability of Component Failure**: Component $j$ causes failure given a system failure:
$$
\Pr\{K_i = j | T_i = t\} = \frac{h_j(t;\v{\theta_j})} {h_{T_i}(t ; \v{\theta_l})}.
$$
- By conditioning on the system lifetime.

\note{
Understanding why something fails is crucial for improving and estimating its reliability.

For notation, let $K_i$ denotes which component caused the failure in a system. 

If a failure occurs, it is unique since only one component can cause failure.
This is not the case in non-series systems.

The joint distribution between the system lifetime and cause of failure is
needed for estimating component reliabilities in our likelihood model.

We can also use this joint distribution to calculate the probability that a
component caused failure.

Both the marginal and conditional probabilities of component failure can be
derived from the joint distribution.

The marginal probability is the expected value of the hazard function of the
component divided by the hazard function of the system.

The conditional probability is the hazard function of the component divided by
the hazard function of the system, conditioned on the system lifetime.
}

## Reliability of Well-Designed Series Systems
- MTTF is a summary measure of reliability:
  - Equivalent to integrating its reliability function over its support.
  - MTTF can be misleading. We can't assume components with longer MTTFs are more reliable.
- A series system is only as strong as its weakest component.
- In a *well-designed series system*, components have similar failure
  characteristics:
  - Similar MTTFs and probabilities of being the cause of failure.
- **Relevance**: Our simulation study is based on a (reasonably) well-designed
  series system.

\note{
Components with longer MTTFs aren't necessarily more reliable.

Recall earlier that the hazard function can take on different failure
characteristics. A component that has a high chance of failing early on might
last a very long time if it survives the initial period, and so have a high
MTTF, and vice versa.

Since a series system is only as strong as its weakest component, in a
well-designed series system, components have similar failure characteristics.

Similar MTTFs and probabilities of being the cause of failure.

Our simulation study is based on a reasonably well-designed series system.

We'll see how the MLE and BCa confidence intervals perform under various
scenarios. We did a preliminary study with a poorly designed system, and we
did find that the MLE and BCa confidence intervals were less accurate and
precise. We don't report those results here, but they are interesting
for future work.
}

# Likelihood Model
We have our system model, but we don't observe component lifetimes. We observe
data related to component lifetimes.

### Observed Data
- Right censoring: No failure observed.
  - The experiment ended before the system failed.
    - $\tau$ is the right-censoring time.
    - $\delta_i = 0$ indicates right-censoring for system $i$.
- Masked causes
  - The system failed, but we don't know the component cause.
    - $S_i$ is the observed time of system failure.
    - $\delta_i = 1$ indicates system failure for system $i$.
    - $\mathcal{C}_i$ are a subset of components that could have caused failure.

\note{
Our likelihood model is based on the observed data.

We observe right-censoring and masked causes of failure.

Right-censoring occurs when the experiment ends before the system fails.

For notation, we let $\tau$ be the right-censoring time and $\delta_i = 0$
indicates right-censoring for system $i$.

Masking occurs when we observe a system failure lifetime, but we don't know the precise component cause but instead a candidate set of components that
could have caused failure.
}

## Observed Data Example
Observed data with a right-censoring time $\tau = 5$ for a series system with
$3$ components.

System | Right-censored lifetime | Event indicator | Candidate set  |
------ | ----------------------- | --------------- | -------------- |
   1   | $1.1$                   | 1               | $\{1,2\}$      |
   2   | $1.3$                   | 1               | $\{2\}$        |
   3   | $2.6$                   | 1               | $\{2,3\}$      |
   4   | $3.7$                   | 1               | $\{1,2,3\}$    |
   5   | $5$                     | 0               | $\emptyset$    |
   6   | $5$                     | 0               | $\emptyset$    |

\note{
Here's an example of observed data for a series system with three components.

We have a right-censoring time of $\tau = 5$.

We see the first four systems failed. System 2 is a special case, since it
only includes component 2. In our model, this means we do in fact know that component 2 caused the failure.

In the other failed systems, we don't know the precise component cause, but
we do have subsets of the components that could have caused failure.

System 4 is also a special case, since all components could have caused failure.
If all the observations were like this, we'd have a lot of ambiguity in our
data.

The last two systems are right-censored. The experiment ended before these
systems failed.
}

## Data Generating Process
DGP is underlying process that generates observed data:

- Green elements are observed.
- Red elements are unobserved (latent).
- Candidate sets ($\mathcal{C}_i$) related to component lifetimes ($T_{i j})$
  and other (unknown) covariates.
  - Distribution of candidate sets complex. Seek a simple model valid under certain assumptions.

\begin{figure}
\centering
\resizebox{0.7\textwidth}{!}{\input{../image/dep_model.tex}}
\end{figure}

\note{
This is the data generating process for our observed data.

It is a graphical model, where green elements are observed and red elements are
unobserved.

The candidate sets are related to the component lifetimes and potentially many
other factors. The distribution of candidate sets could be quite complex
and difficult to model. We seek a simple model that is valid under certain
assumptions. We'll discuss these assumptions soon.

We see that the right-censoring time is independent of the component lifetimes
and parameters in our model. This is a reasonable assumption in many cases.
}

## Likelihood Function

### Assumptions
- Right-censoring time $\tau$ independent of component lifetimes and parameters:
\begin{align*}
S_i       &= \min(\tau, T_i),\\
\delta_i  &= 1_{\{T_i < \tau\}}.
\end{align*}
- Observed failure time with candidate sets. Candidate sets satisfy some
conditions (discussed later).

### Likelihood Contributions
  $$
  L_i(\v\theta) \propto
  \begin{cases}
      \prod_{l=1}^m R_l(s_i;\v{\theta_l})         &\text{ if } \delta_i = 0\\
      \prod_{l=1}^m R_l(s_i;\v{\theta_l})
          \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
  \end{cases}
  $$

## Likelihood Contribution: Masked Failures

**Masking**: When a system fails, but the precise failed component is ambiguous. 

**Simplification**: To make problem more tractable, we introduce certain conditions.

  - Reasonable for many realistic situations.

\note{
Masking occurs when a system fails but the precise failed component is ambiguous. 

To make problem more tractable, we introduce certain conditions.

These conditions are reasonable for many real-world systems.
}

## Masking Conditions

**Candidate Set Contains Failed Component**: The candidate set, $\mathcal{C}_i$,
always includes the failed component:

$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$

**Equal Probabilities Across Candidate Sets**: For an observed system failure
time $T_i=t_i$ and a candidate set $\mathcal{C}_i = c_i$, the probability of
of the set is constant across different component failures within the set:

$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j, T_i = t_i\} = \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j', T_i = t_i\}
$$
for every $j,j' \in c_i$.

**Masking Probabilities Independent of Parameters**: The masking probabilities
when conditioned on $T_i$ and failed component $K_i$ aren't functions of $\v{\theta}$.

## Likelihood Contribution: Masked Component Cause of Failure

**Joint distribution** of $T_i$, $K_i$, and $\mathcal{C}_i$:
$$
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta}) = f_{T_i,K_i}(t_i,j;\v{\theta})\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$
**Marginalize** over $K_i$ and apply Conditions 1, 2, and 3:
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l}) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
**Result**: We don't need to model the distribution of the candidate sets $\mathcal{C}_i$.

  - $L_i(\v\theta) \propto f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta})$.

## Methodology: Maximum Likelihood Estimation

**Maximum Likelihood Estimation (MLE)**: Maximize the likelihood function:
$$
\hat{\v\theta} = \arg\max_{\v\theta} L(\v\theta).
$$

**Solution**: Numerically solved system of equations for $\hat{\v\theta}$:
$$
\nabla_{\v\theta} \ell(\hat{\v\theta}) = \v{0}.
$$

\note{
MLE is all about finding the parameter values that make the observed data most probable. It's a way of fitting our model to our data.

But the computation can get tricky. We need to solve some equations numerically. 

To simplify, we often work with the log-likelihood, which is just the log of the likelihood.}

## Bootstrap Method: Confidence Intervals

**Sampling Distribution of MLE**: Asymptotic normality is useful for
constructing confidence intervals.

  - **Issue**: May need large samples before asymptotic normality holds.

**Bootstrapped CIs**: Resample data and find an MLE for each. Use the
distribution of the bootstrapped MLEs to construct CIs.

- **Percentile Method**: Simple and intuitive.

**Correctly Specified CIs**: A coverage probability close to the nominal level of $95\%$.

  - **Issue**: Coverage probability may be too low or too high.

- **Adjustments**: To improve coverage probability, we use the BCa method to
  adjust for bias (bias correction) and skewness (acceleration) in the estimate.
  Coverage probabilities above $90\%$ acceptable.

\note{
We need a way to measure the uncertainty in our estimates.

This is where confidence intervals come in. They help us pin down the likely range of our estimates.

We Bootstrap the confidence intervals since there is likely to be a lot of
bias and variability in our estimates due to the masking and censoring
in our small data sets.

So, to ensure our intervals are accurate, we use the BCa method to adjust the
intervals for bias and skewness in the estimate.
  
A coverage probability above $90\%$ is acceptable.
}

## Challenges with MLE on Masked Data

We discovered some challenges with the MLE on masked data.

**Convergence Issues**: Flat likelihood regions were observed due to the
  ambiguity in the masked data and small sample sizes.
  
**Bootstrap Issues**: Bootstrap relies on the Law of Large Numbers.

- Bootstrap might not represent the true variability, leading to inaccuracies.

- Due to right censoring and masking, the effective sample size is reduced.

**Mitigation**: We discard non-convergent samples for the MLE on original data,
but keep all resamples for the bootstrap.

- This ensures that the bootstrap for "good" data is representative of the
  variability in the original data.

- We report convergence rates in our simulation study.

\note{
Like any model, ours has its challenges. For instance, the ambiguity in masked data, combined with small sample sizes, can create flat regions in our likelihood, causing convergence issues. Also, bootstrapping may not always capture the true variability in our data. But we've taken steps to address these. We discard non-convergent samples for MLE on original data but retain all for bootstrapping, ensuring variability is captured.
}

# Series System with Weibull Component Lifetimes

**Weibull Distribution**: We model a system's components using Weibull
distributed lifetimes.

The lifetime distribution for the $j$\textsuperscript{th} component of the
$i$\textsuperscript{th} system is:
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j)
$$

Where:

- $\lambda_j > 0$ is the scale parameter.
- $k_j > 0$ is the shape parameter.

#### Significance of the Shape Parameter:

- $k_j < 1$: Indicates infant mortality. E.g., defective components weeded out early.
- $k_j = 1$: Indicates random failures. E.g., result of random shocks.
- $k_j > 1$: Indicates wear-out failures. E.g., components wearing out with age.

\note{Weibull Distribution: Crucial and versatile distribution in reliability
analysis.}

## Theoretical Results

Reliability and hazard functions of a series system with Weibull components:
\begin{align*}
R_{T_i}(t;\v\theta) &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
h_{T_i}(t;\v\theta) &= \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{align*}
where $\v\theta = (k_1, \lambda_1, \ldots, k_m, \lambda_m)$ is the parameter vector of the series system.

### Likelihood Model

**Right Censoring and Masked Failures**: The likelihood contribution of
$i$\textsuperscript{th} system:
$$
L_i(\v\theta) \propto
\begin{cases}
    R_{T_i}(t_i;\v\theta) & \text{if } \delta_i = 0,\\
    R_{T_i}(t_i;\v\theta) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}) & \text{if } \delta_i = 1.
\end{cases}
$$

# Simulation Study Overview

This study is centered around the following *well-designed series system*
with Weibull component lifetimes:

| Component   | Shape | Scale | MTTF | $\Pr\{K_i\}$ |
|-------------|------------|----------|---------|-------|
| 1      | 1.26 | 994.37 | 924.87 | 0.17 | 0.74 |
| 2      | 1.16 | 908.95 | 862.16 | 0.21 | 0.70 |
| 3      | 1.13 | 840.11 | 803.56 | 0.23 | 0.67 |
| 4      | 1.18 | 940.13 | 888.24 | 0.20 | 0.71 |
| 5      | 1.20 | 923.16 | 867.75 | 0.20 | 0.71 |
| System | NA   | NA     | 222.88 | NA   | 0.18 |

## Performance Metrics

**Objective**: Evaluate the MLE and BCa confidence intervals' performance across
various scenarios.

- **MLE Evaluation**:
  - **Accuracy**: Proximity of the MLE's expected value to the actual value. 
  - **Precision**: Consistency of the MLE across samples.

- **BCa Confidence Intervals Evaluation**:
  - **Accuracy**: Confidence intervals (CIs) should cover true parameters
    around $95\%$ of the time.
      - Coverage probability (CP)
  - **Precision**: Assessed by the width of the CIs.
  
Both accuracy and precision are crucial for confidence in the analysis.

## Data Generation

We generate data for $n$ systems with $5$ components each.
We satisfy the assumptions of our likelihood model by generating data as follows:

- **Right-Censoring Model**: Right-censoring time set at a known value,
parameterized by the quantile $q$.

  - Satisfies the assumption that the right-censoring time is independent of
    component lifetimes and parameters.

- **Masking Model**: Using a *Bernoulli masking model* for component cause of failure,
parameterized by the probability $p$.

  - Satisfies masking Conditions 1, 2, and 3.

## Scenario: Impact of Right-Censoring

Vary the right-censoring quantile ($q$): $60\%$ to $100\%$.
Fixed the parameters: $p = 21.5\%$ and $n = 90$.

### Background
- **Right-Censoring**: No failure observed.
- **Impact**: Reduces the effective sample size.
- **MLE**: Bias and precision affected by censoring.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.

## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-tau-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-tau-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/q_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: CIs converge to $95\%$. Scale parameters better calibrated.
- **Convergence Rate**: Increases as right-censoring reduces.

## Conclusion
- MLE precision improves, bias drops with decreased right-censoring.
- BCa CIs perform well, particularly for scale parameters.
- MLE of most reliable component more affected by right-censoring.

## Impact of Masking Probability
Vary the masking probability $p$: $0.1$ to $0.7$.
Fixed the parameters: $q = 0.825$ and $n = 90$.

### Background
- **Masking** adds ambiguity in identifying the failed component.
- Impacts of masking on MLE:
  - **Ambiguity**: Higher $p$ increases uncertainty in parameter adjustment.
  - **Bias**: Similar to right-censoring, but affected by both $p$ and $q$.
  - **Precision**: Reduces as $p$ increases.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.45\textwidth}
```{r fig-p-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.45\textwidth}
```{r fig-p-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/p_vs_convergence.pdf")
```
\end{minipage}
\end{figure}


**Calibration**: Caution advised for severe masking with small samples.

- Scale parameters maintain coverage up to $p = 0.7$.

- Shape parameters drop below $90\%$ after $p = 0.4$.

**Convergence Rate**: Reduces after $p > 0.4$, consistent with CP behavior.


## Conclusion
- Masking influences MLE precision, coverage probability, and introduces bias.
- Despite significant masking, scale parameters have commendable CI coverage.

## Impact of Sample Size
Assess the impact of sample size on MLEs and BCa CIs.

- Vary sample size $n$: $50$ to $500$
- Parameters: $p = 0.215$, $q = 0.825$

### Background
- **Sample Size**: Number of systems observed.
- **Impact**: More data reduces uncertainty in parameter estimation.
- **MLE**: Mitigates biasing effects of right-censoring and masking.

## Both Scale and Shape Parameters

\begin{figure}
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.3-mle.pdf")
```
\end{minipage}
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

## Parameters

- **Dispersion**:
  - Dispersion reduces with $n$—indicating improved precision.
  - Disparity observed between components $k_1, \lambda_1$ and $k_3, \lambda_3$.
- **Bias**:
  - High positive bias initially, but diminishes around $n=250$.
  - Enough sample data can counteract right-censoring and masking effects.

- **Median-Aggregated CIs**:
  - CIs tighten as $n$ grows—showing more consistency.
  - Upper bound more dispersed than lower, reflecting the MLE bias direction.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.4\textwidth}
```{r fig-n-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.4\textwidth}
```{r fig-n-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/n_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: 
  - CIs are mostly above $90\%$ across sample sizes.
  - Converge to $95\%$ as $n$ grows.
  - Scale parameters have better coverage than shape.

- **Convergence Rate**:
  - Improves with $n$, surpassing $95\%$ for $n \geq 100$.
  - Caution for estimates with $n < 100$ in specific setups.

## Conclusion
- Sample size significantly mitigates challenges from right-censoring and masking.
- MLE precision and accuracy enhance notably with growing samples.
- BCa CIs become narrower and more reliable as sample size increases.

# Conclusion

## Part 1

### Key Findings
- Employed maximum likelihood techniques for component reliability estimation in series systems with masked failure data.
- Methods performed robustly despite masking and right-censoring challenges.

### Simulation Insights
- Right-censoring and masking introduce positive bias; more reliable components are most affected.
- Shape parameters harder to estimate than scale.
- Large samples can counteract these challenges.

## Part 2

### Confidence Intervals
- Bootstrapped BCa CIs demonstrated commendable coverage probabilities, even in smaller sample sizes.
  
### Takeaways
- Framework offers a rigorous method for latent component property estimation from limited observational data.
- Techniques validated to provide practical insights in diverse scenarios.
- Enhanced capability for learning from obscured system failure data.

# Discussion

