---
title: "Reliability Estimation in Series Systems"
subtitle: "Maximum Likelihood Techniques for Right-Censored and Masked Failure Data"
author: Alex Towell
output:
  beamer_presentation:
    theme: Boadilla
    slide_level: 2
  powerpoint_presentation:
    slide_level: 2
header-includes:
- \usepackage{tikz}
- \usepackage{caption}
- \usepackage{amsthm}
- \usepackage{xcolor}
- \renewcommand{\v}[1]{\boldsymbol{#1}}
- \theoremstyle{definition}
- \newtheorem{condition}{Condition}
- \theoremstyle{plain}
- \setbeameroption{show notes on second screen}
bibliography: ../refs.bib
#link-citations: true
#csl: ../ieee-with-url.csl
---

## Context & Motivation

**Reliability** in **series systems** is like a chain's strength -- determined
by its weakest link.

- Essential for system design and maintenance.

**Main Goal**: Estimate individual component reliability from *failure data*.

**Challenges**:

- *Masked* component-level failure data.
- *Right-censoring* in system-level failure data.

**Our Response**:

- Derive techniques to interpret such ambiguous data.
- Aim for precise and accurate reliability estimates for individual components
using maximum likelihood estimation (MLE) and bootstrapped (BCa) confidence
intervals.

\note{
\begin{itemize}
\item \textbf{Chain Analogy}: Think of a series system as a chain. Its reliability,
just like a chain's strength, is determined by its weakest link or component.
When any component fails, the whole system does.
\item \textbf{Reliability Importance}: Understanding the reliability of each component is essential for the design and
maintenance of these systems. 
\item \textbf{Data Challenge}: The data we rely on can come with its own challenges. We
sometimes encounter ambiguous data like right-censored information or masked
component-level failures, where we don't know precisely which component failed.
\item \textbf{Aim}: Our goal is to interpret such ambiguous data and provide
accurate reliability estimates for each component, which includes providing
correctly specified 95% CIs, meaning they cover the true parameter value around
95% of the time. We do this using MLE and bootstrap the confidence intervals
using the BCa method.
\end{itemize}
}

## Core Contributions

**Likelihood Model** for **series systems**.

- Accounts for *right-censoring* and *masked component failure*.
- Can easily incorporate additional failure data.

**Specifications of Conditions**:

- Assumptions about the masking of component failures.
- Simplifies and makes the model more tractable.

**Simulation studies**:

- Components with *Weibull* lifetimes.

- Evaluate MLE and confidence intervals under different scenarios.

**R Library**: Methods available on GitHub.

- [github.com/queelius/wei.series.md.c1.c2.c3](https://github.com/queelius/wei.series.md.c1.c2.c3)

\note{
Our core contributions can be broken down into several parts:
\begin{itemize}
\item \textbf{Likelihood model}: We've derived a likelihood model for series systems that accounts for the
ambiguous data.
\item \textbf{Explain conditions}: We've clarified the conditions this model assumes about the masking of component failures. These conditions simplify the model and make it more tractable.
\item \textbf{Validated with simulation study}: We've validated our model with extensive simulations using Weibull distributions to gauge its performance under various scenarios.
\item \textbf{R Library}: For those interested, we made our methods available in an R Library hosted on GitHub.
\end{itemize}
}

# Series System

\begin{figure}
\centering
\resizebox{0.7\textwidth}{!}{\input{../image/series.tex}}
\end{figure}

**Critical Components**: Complex systems often comprise *critical* components.
If any component fails, the entire system fails.

- We call such systems *series systems*.
- **Example**: A car's engine and brakes.

**System Lifetime** is dictated by its shortest-lived component:
$$
T_i = \min(T_{i 1}, \ldots, T_{i 5})
$$

- Where: $T_i$ and $T_{i j}$ are the system and component lifetimes for the
$i$\textsuperscript{th} system and $j$\textsuperscript{th} component, respectively.

\note{
\begin{itemize}
\item \textbf{Critical Components}: Many complex systems have components that
are essential to their operation.
\item \textbf{Series System}: If any of these components fail, the entire system
fails. We call these series systems.
\item \textbf{Car}: Think of a car - if the engine or brakes fail, the car can't
be operated.
\item \textbf{Lifetime}: Its lifetime is the lifetime of its shortest-lived
component.
\item \textbf{Notation}: For reference, we show the math notation we'll use
throughout the talk.
\end{itemize}
}

## Reliability Function
**Reliability Function** represents the probability that a system or component functions
beyond a specified time.

- Essential for understanding longevity and dependability.

**Series System Reliability**: Product of the reliability of its components:
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$

- Here, $R_{T_i}(t;\v\theta)$ and $R_j(t;\v{\theta_j})$ are the reliability
  functions for the system $i$ and component $j$, respectively.

**Relevance**:

  - Forms the foundation for most reliability studies.
  - Integral to our likelihood model, e.g., right-censoring events.

\note{
\begin{itemize}
\item \textbf{Reliability Function} The reliability function tells us the chance
a component or system functions past a specific time. It's our key metric for longevity.
\item \textbf{Product of Component Reliability}: In a series system, the
overall reliability is the product of its component reliabilities. So, if even
one component has a low reliability, it can impact the whole system.
\item \textbf{Relevance}: Why does this matter to us? This concept is foundational to our studies,
especially when we're handling right-censored data.
\end{itemize}
}

## Hazard Function: Understanding Risks

**Hazard Function**: Measures the immediate risk of failure at a given time,
assuming survival up to that moment.

- Reveals how the risk of failure evolves over time.
- Guides maintenance schedules and interventions.

**Series System Hazard Function** is the sum of the hazard functions of its components:
$$
h_{T_i}(t;\v{\theta}) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$

  - Components' risks are additive.

\note{
\textbf{Hazard Function}: Let's shift focus to the hazard function. Essentially,
it's a way to gauge the immediate risk of failure, especially if it's been
functioning up until that point.

\textbf{Series Hazard Function} Lastly, the hazard function for a series system
is just the sum of the hazard functions of its components.

\textbf{Additive}: We see that the component risks are additive.
}

## Joint Distribution of Component Failure and System Lifetime
Our likelihood model depends on the **joint distribution** of the system
lifetime and the component that caused the failure.

- **Formula**: Product of the failing component's hazard function and the system
reliability function:
$$
f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta).
$$
- **Single Point of Failure**: A series system fails due to one component's malfunction.
- **Representation**: 
  - $K_i$: Component causing the $i$\textsuperscript{th} system's failure.
  - $h_j(t;\v{\theta_j})$: Hazard function for the $j$\textsuperscript{th} component.

\note{
\begin{itemize}
\item \textbf{Joint Distribution} In our likelihood model, understanding the
joint distribution of a system's lifetime and the component that led to its
failure is fundamental.
\item \textbf{Formula}: It is the product of the failing component's hazard
function and the system reliability function.
\item \textbf{Unique Cause}: Which emphasizes that in a series system, failure
can be attributed to a single component's malfunction.
\item \textbf{Notation}: Here, $K_i$ denotes the component responsible for the failure.
\end{itemize}
}


## Component Failure & Well-Designed Series Systems

The **marginal probability** of component failure helps predict the cause of
failure.

- **Derivation**: Marginalize the joint distribution over the system lifetime:
$$
\Pr\{K_i = j\} = E_{\v\theta} \biggl[ \frac{h_j(T_i;\v{\theta_j})} {h_{T_i}(T_i ; \v{\theta_l})} \biggr].
$$

**Well-Designed Series System**: Components exhibit comparable chances of
causing system failures.

- **Relevance**: Our simulation study employs a (reasonably) well-designed
series system.

\note{
\begin{itemize}
\item \textbf{Marginal}: We can use this joint distribution to calculate the marginal probability
of component failure.
\item \textbf{Expected Value}: When we do so, we find that it is the expected value of
the ratio of component and system hazard functions.
\item \textbf{Well-Designed}: We say that a series system is \emph{well-designed}
if each components has a comparable chance of failing.
\item \textbf{Relevance}: Our simulation study is based on a reasonably
well-designed series system.
\end{itemize}
}

# Likelihood Model

Likelihood measures how well our model parameters ($\v\theta$) explain the data.
Each system contributes to the **total likelihood** via its *likelihood contribution*:
$$
L(\v\theta|\text{data}) = \prod_{i=1}^n L_i(\v\theta|\text{data}_i).
$$
where **data$_i$** is the data for the $i$\textsuperscript{th} system and $L_i$ is its contribution.

Our model handles the following data:
**Right-Censored**: Experiment ends before failure (Event Indicator: $\delta_i = 0$).
  - Contribution is system reliability: $L_i(\v\theta) = R_{T_i}(\tau;\v\theta)$.
**Masked Failure**: Failure observed, but the failed component is masked by a
*candidate set*. More on its contribution later.

System | Right-Censored Lifetime | Event Indicator | Candidate Set  |
------ | ----------------------- | --------------- | -------------- |
   1   | $1.1$                   | 1               | $\{1,2\}$      |
   2   | $5$                     | 0               | $\emptyset$    |

\note{
Let's talk about the likelihood model, which is a way of measuring how well our
model explains the data.

\begin{itemize}
\item \textbf{Total likelihood} is the product of the likelihood contributions of each system.
\item \textbf{Contributions}: Our likelihood model deals with right-censoring and masked cause of failure.
\item \textbf{Right-Censoring} occurs when the experiment ends before the system fails. Its contribution is just the system reliability,
since the event indicator tells us if the system was right-censored.
\item \textbf{Masking} occurs when we observe a failure but we don't know the
precise component cause. Instead, we observe a candidate set of components that
could have failure. More on this later.
\item Here's an example of observed data.
\item \textbf{System 1}: We see that the system failed at $1.1$. We don't know
which component failed, but we know it was either component 1 or 2.
\item \textbf{System 2}: The experiment ended before the system failed. Since no
component failed, the candidate set is empty.
\item Candidate sets may have a single component too, in which case there is
no masking.
\end{itemize}
}

## Data Generating Process
DGP is underlying process that generates the data:

- *\textcolor{green}{Green}* elements are observed, *\textcolor{red}{Red}* elements are latent.
- **Right-Censored** lifetime: $S_i = \min(T_i, \tau)$.
- **Event Indicator**: $\delta_i = 1_{\{T_i < \tau\}}$.
- **Candidate Set**: $\mathcal{C}_i$ related to components ($T_{i j})$
  and other unknowns.

\begin{figure}
\centering
\resizebox{0.67\textwidth}{!}{\input{../image/dep_model.tex}}
\end{figure}

\note{
\begin{itemize}
\item \textbf{DGP}: Let's discuss the data generating process to motivate our model.
\item \textbf{Graph}: Here's the graph: green is observed and red is latent.
\item \textbf{Infer}: We don't get to see the red elements, but we can infer most of them
from the green elements.
\item \textbf{Green}: So, let's focus on the green elements.
\item The right-censoring time is the minimum of the system lifetime and the
right-censoring time.
\item The event indicator is 1 if the system fails before the right-censoring time, 0 otherwise.
\item The candidate sets are related to the component lifetimes and many other factors.
\item This can be very difficult to model. We seek a simple model that is valid under certain
assumptions, which we'll discuss a bit later.
\end{itemize}
}


## Likelihood Contribution: Masked Failures

A masked failure ($\delta_i = 1$) has a likelihood contribution proportional to
the product of the system reliability and the sum of the component hazards in the masking set:
$$
L_i(\v\theta) \propto R_{T_i}(s_i;\v\theta) \sum_{j\in c_i} h_j(s_i;\v{\theta_j}).
$$

**Candidate Set Contains Failed Component**: The candidate set, $\mathcal{C}_i$,
always includes the failed component: $\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1$.

**Equal Probabilities Across Candidate Sets**: The probability of of the
candidate set is constant across different components within it, i.e., for every
$j,j' \in c_i$:
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j, T_i = t_i\} = \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | K_i = j', T_i = t_i\}.
$$

**Masking Probabilities Independent of Parameters**: The masking probabilities
when conditioned on $T_i$ and $K_i$ aren't functions of $\v{\theta}$.

\note{
\begin{itemize}
\item The right-censoring contribution is straightforward. But the masked failure
contribution is a bit more complicated.
\item Masking occurs when a system fails but the precise failed component is ambiguous. 
\item To make problem more tractable, we introduce certain conditions.
\item Reasonable for many realistic situations.
\end{itemize}
}


## Likelihood Contribution: Derivation for Masked Failures (cont.)

**Joint distribution** of $T_i$, $K_i$, and $\mathcal{C}_i$:
$$
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta}) = f_{T_i,K_i}(t_i,j;\v{\theta})\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$
**Marginalize** over $K_i$ and apply Conditions 1, 2, and 3:
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l}) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
**Result**: We don't need to model the distribution of the candidate sets $\mathcal{C}_i$.

  - $L_i(\v\theta) \propto f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta})$.

\note{
\begin{itemize}
\item We can marginalize over $K_i$ and apply the conditions to get the
likelihood contribution for masked failures.
\item The result is that we don't need to model the distribution of the candidate sets.
\item This is a huge simplification.
\end{itemize}
}

## Methodology: Maximum Likelihood Estimation

**Maximum Likelihood Estimation (MLE)**: Maximize the likelihood function:
$$
\hat{\v\theta} = \arg\max_{\v\theta} L(\v\theta).
$$

**Solution**: Numerically solved system of equations for $\hat{\v\theta}$:
$$
\nabla_{\v\theta} \log L(\hat{\v\theta}) = \v{0}.
$$

\note{
\begin{itemize}
\item \textbf{MLE}: We use the standard MLE approach.
\item \textbf{ArgMax}: We find the parameter values that maximize the
log-likelihood function.
\item \textbf{Solution}: Since there is no closed-form solution, we numerically solve it.
\end{itemize}
}

## Bootstrap Confidence Intervals (CIs)

Confidence Intervals (CI) quantify the uncertainty in our estimate.

**Asymptotic Sampling Distribution of MLE** is a popular choice for constructing
CIs.

- **Challenge**: Asymptotic distribution may not be accurate for small sample sizes.
  - Particularly since we're dealing with right-censoring and masking.

**Bootstrapped CIs**: Resample data and obtain MLE for each.

- Use **percentiles** of bootstrapped MLEs for CIs.

**Correctly Specified CIs**:

- Desired: Coverage probability near $95\%$. ($>90\%$ acceptable.)
- **Challenge**: Actual coverage may deviate.

**BCa adjustments** counteracts bias and skewness in estimates.

\note{
\begin{itemize}
\item \textbf{Goal}: Need a way to measure the uncertainty in our estimate.
\item \textbf{CIs} are a popular; they help us pin down the likely range of values for our parameters.
\item \textbf{Bootstrap} the CIs, since there is a lot of bias and variability in our estimate due to the masking and censoring
in our small data sets and the asymptotic distribution is not likely to be accurate.
\item \textbf{Specified}: We want our CIs to be correctly specified, meaning they cover the true parameter value around 95% of the time.
\item \textbf{BCa}: But they may be too low or too high; we use the BCa method to adjust for bias and skewness in the estimate.
A coverage probability above $90\%$ is acceptable.
\end{itemize}
}

## Challenges with MLE on Masked Data

We discovered some challenges with the MLE.

**Convergence Issues**: Flat likelihood regions were observed due to the
  ambiguity in the masked data and small sample sizes.
  
**Bootstrap Issues**: Bootstrap relies on the Law of Large Numbers.

- It might not represent the true variability for small samples.

- Due to censoring and masking, the effective sample size is reduced.

**Mitigation**: In simulation study, we discard non-convergent samples for the
MLE on original data, but keep all resamples for the bootstrap.

- Helps ensure robustness of the results, while acknowledging the inherent
  complexities introduced by masking and censoring.

- We report convergence rates in our simulation study.

\note{
Like any model, ours has its challenges:
\begin{itemize}
\item \textbf{Masking}: Masking and censoring, combined with small sample sizes,
can cause flat likelihood regions, which can lead to convergence issues.
\item \textbf{Small}: For small samples, bootstrapping may not always capture
the true variability in the data
\textbf{Approach}: We take the following approach in our simulation study.
\textbf{Discard}: We discard non-convergent samples for the MLE on original data
but retain all MLEs for the resampled data.
\textbf{Robustness}: This helps ensure the robustness of our results while
acknowledging the inherent complexities introduced by masking and censoring.
\textbf{Convergence Rate}: We report the convergence rate in our simulation study.
\end{itemize}
}

# Simulation Study

## Series System: Weibull Components

The lifetime of the $j$\textsuperscript{th} component in the $i$\textsuperscript{th} system:
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j)
$$

- $\lambda_j$ is the **scale** parameter 
- $k_j$ is the **shape** parameter:
  - $k_j < 1$: Indicates infant mortality.
  - $k_j = 1$: Indicates random failures.
  - $k_j > 1$: Indicates wear-out failures.

Recall that for a series system:

- **Series Reliability** is the product of the component reliabilities.
- **Hazard** is the sum of the component hazard functions.
- **Likelihood**: $L(\v\theta) \propto \prod_{i=1}^n R_{T_i}(t_i;\v\theta) \left[\sum_{j \in c_i} h_j(t_i;\v{\theta_j})\right]^{\delta_i}$.

\note{
\begin{itemize}
\item \textbf{Weibull}: We model a series system with Weibull components.
\item \textbf{Component Functions}: Hazard and reliability functions are well-known for Weibull.
\item \textbf{Shape} parameter tells us a lot about the failure characteristics.
\item \textbf{Increasing}: When the function is increasing, think of it as
wearing-out over time.
\item \textbf{Decreasing}: If it's decreasing, it usually signals some
early-life challenges.
\item \textbf{Series System}: Recall that for a series system, the reliability
is the product of the component reliabilities and the hazard function is the
sum of the component hazard functions.
\item \textbf{Likelihood}: The likelihood is the same as before, we've just
reproduced it here.
\end{itemize}
}

## Well-Designed Series System

**Simulation study** centered around series system with Weibull components:

| Component | Shape | Scale  | $\Pr\{K_i\}$ |
|-----------|-------|--------|--------------|
| 1         | 1.26  | 994.37 | 0.17         |
| 2         | 1.16  | 908.95 | 0.21         |
| 3         | 1.13  | 840.11 | 0.23         |
| 4         | 1.18  | 940.13 | 0.20         |
| 5         | 1.20  | 923.16 | 0.20         |

- Based on (Guo, Niu, and Szidarovszky 2013) which studies a 3-component series system.
  - We add components 4 and 5 to make the system more complex.
- **Probabilities** are comparable: it is *reasonably well-designed*.
  - Component 1 is most reliable, component 3 is least reliable.
- **Shape** parameters are greater than 1, indicating wear-out failures.

\note{
\begin{itemize}
\item \textbf{Centered}: This study is centered around a series system with Weibull components.
\item \textbf{Based}: It's based on a paper that studies a 3-component series system.
\item \textbf{Added}: We added components 4 and 5 to make it more complex.
\item \textbf{Probability}: We show the probability of each component being the cause of failure.
\item \textbf{Well-Designed}: The probabilities are comparable, so no weak links.
It's reasonably well-designed. Component 1 is most reliable, component 3 is least.
\item \textbf{Parameters}: We show the shape and scale parameters for each component.
\item \textbf{Wear-Out}: The shape parameters are greater than 1, indicating
components are likely to fail due to wear-out.
\end{itemize}
}

## Data Generation

**Latent Component Lifetimes** are generated for each system in the study.

**Right-censoring**: In our simulation study, we independently control the
probability $q$ (quantile) of right-censoring by finding the value $\tau$ that satisfies
$\Pr\{T_i < \tau\} = q$.

  - $S_i = \min(T_i, \tau)$ and $\delta_i = 1_{\{T_i < \tau\}}$.

**Masking Component Failures**: The *Bernoulli Masking Model* is used to mask
component cause of failure, parameterized by masking probability $p$.

- $p$ chosen independently: at the extremes, if $p = 0$ there is no masking, and if $p = 1$, there is total masking.
- We describe the process and how it satisfies the masking conditions next.


\note{
\begin{itemize}
\item \textbf{Data Generation}: We generate the latent component lifetimes for the series system we just discussed.
\item \textbf{Observed Data}: Then, we generate the data we actually see, which is based on the component data.
\item \textbf{Right-Censoring}: We control the probability of right-censoring by
finding the value of $\tau$ that satisfies the quantile $q$. Then, we set the
right-censoring time to be the minimum of the system lifetime and $\tau$. The
event indicator is 1 if the system fails before $\tau$, 0 otherwise.
\item \textbf{Masking}: We use a Bernoulli masking model to mask the component
cause of failure. We parameterize the level of masking by the masking probability, $p$.
\item We parameterize the level of masking by the masking probability, $p$, which
specifies that each non-failed component has a $p$ probability of masking the failed component
by including it in the candidate set.
\end{itemize}
}


## Data Generation: Satisfying Masking Conditions

We generate the candidate sets for each system in the study.

**Satisfying Masking Conditions**:

- **Condition 1**: The failed component deterministically placed in candidate set. 
- **Condition 2**: By using a Bernoulli distribution with a constant probability
  $p$ for all components, probability of a candidate set is constant as we
  vary which component failed within set.
- **Condition 3**: Masking only depends on the fixed parameter $p$ and
  doesn't interact with the system parameter $\theta$.

\note{
\begin{itemize}
\item \textbf{Masking}: We use a Bernoulli masking model for masking the failed
component.
\item This satisifes the masking failure conditions in the following ways:
\item \textbf{Condition 1}: The failed component is deterministically placed in the candidate set.
\item \textbf{Condition 2}: The probability of masking is the same for all components,
so the probability of the candidate set is constant across components.
\item \textbf{Condition 3}: The masking probability is independent of the parameters.
\end{itemize}
}

## Performance Metrics

**Objective**: Evaluate the MLE and BCa confidence intervals' performance across
various scenarios.

- **MLE Evaluation**:
  - **Accuracy**: Proximity of the MLE's expected value to the actual value. 
  - **Precision**: Consistency of the MLE across samples.

- **BCa Confidence Intervals Evaluation**:
  - **Accuracy**: Confidence intervals (CIs) should cover true parameters
    around $95\%$ of the time.
      - Coverage probability (CP)
  - **Precision**: Assessed by the width of the CIs.
  
Both accuracy and precision are crucial for confidence in the analysis.

\note{
\begin{itemize}
\item \textbf{Objective}: We want to evaluate the performance of the MLE and
BCa confidence intervals across various scenarios.
\item \textbf{MLE Evaluation}: We evaluate the MLE in terms of accuracy and precision.
\item \textbf{Accuracy}: Accuracy is the proximity of the MLE's expected value
to the actual value.
\item \textbf{Precision}: Precision is the consistency of the MLE across samples.
\item \textbf{BCa Confidence Intervals Evaluation}: We evaluate the BCa confidence
intervals in terms of accuracy and precision.
\item \textbf{Accuracy}: Accuracy is measured by the coverage probability, which
is the proportion of times the confidence interval covers the true parameter.
\item \textbf{Precision}: Precision is assessed by the width of the confidence
interval.
\item Both accuracy and precision are crucial for confidence in the analysis.
\end{itemize}
}

## Scenario: Impact of Right-Censoring

Vary the right-censoring quantile ($q$): $60\%$ to $100\%$.
Fixed the parameters: $p = 21.5\%$ and $n = 90$.

### Background
- **Right-Censoring**: No failure observed.
- **Impact**: Reduces the effective sample size.
- **MLE**: Bias and precision affected by censoring.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-tau-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

- **Dispersion**: Less censoring improves MLE precision.
- **Bias**: Both parameters are biased. Bias decreases with less censoring.
- **Median-Aggregated CIs**: Bootstrapped CIs become consistent with more data.

## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-tau-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-q-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-tau-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/q_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: CIs converge to $95\%$. Scale parameters better calibrated.
- **Convergence Rate**: Increases as right-censoring reduces.

## Conclusion
- MLE precision improves, bias drops with decreased right-censoring.
- BCa CIs perform well, particularly for scale parameters.
- MLE of most reliable component more affected by right-censoring.

## Impact of Masking Probability
Vary the masking probability $p$: $0.1$ to $0.7$.
Fixed the parameters: $q = 0.825$ and $n = 90$.

### Background
- **Masking** adds ambiguity in identifying the failed component.
- Impacts of masking on MLE:
  - **Ambiguity**: Higher $p$ increases uncertainty in parameter adjustment.
  - **Bias**: Similar to right-censoring, but affected by both $p$ and $q$.
  - **Precision**: Reduces as $p$ increases.

## Scale Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-scale-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-scale.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Shape Parameters

\begin{figure}
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.5\textwidth}
```{r fig-shape-p-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}


- **Dispersion**: Increases with $p$, indicating reduced precision.
- **Bias**: Positive bias rises with $p$.
- **Median-Aggregated CIs**: Widen and show asymmetry as $p$ grows.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.45\textwidth}
```{r fig-p-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-p-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.45\textwidth}
```{r fig-p-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/p_vs_convergence.pdf")
```
\end{minipage}
\end{figure}


**Calibration**: Caution advised for severe masking with small samples.

- Scale parameters maintain coverage up to $p = 0.7$.

- Shape parameters drop below $90\%$ after $p = 0.4$.

**Convergence Rate**: Reduces after $p > 0.4$, consistent with CP behavior.


## Conclusion
- Masking influences MLE precision, coverage probability, and introduces bias.
- Despite significant masking, scale parameters have commendable CI coverage.

## Impact of Sample Size
Assess the impact of sample size on MLEs and BCa CIs.

- Vary sample size $n$: $50$ to $500$
- Parameters: $p = 0.215$, $q = 0.825$

### Background
- **Sample Size**: Number of systems observed.
- **Impact**: More data reduces uncertainty in parameter estimation.
- **MLE**: Mitigates biasing effects of right-censoring and masking.

## Both Scale and Shape Parameters

\begin{figure}
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-scale-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-scale.3-mle.pdf")
```
\end{minipage}
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-1, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.1-mle.pdf")
```
\end{minipage}%
\begin{minipage}{.375\textwidth}
```{r fig-shape-n-3, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-shape.3-mle.pdf")
```
\end{minipage}
\end{figure}

## Parameters

- **Dispersion**:
  - Dispersion reduces with $n$—indicating improved precision.
  - Disparity observed between components $k_1, \lambda_1$ and $k_3, \lambda_3$.
- **Bias**:
  - High positive bias initially, but diminishes around $n=250$.
  - Enough sample data can counteract right-censoring and masking effects.

- **Median-Aggregated CIs**:
  - CIs tighten as $n$ grows—showing more consistency.
  - Upper bound more dispersed than lower, reflecting the MLE bias direction.


## Coverage Probability and Convergence Rate

\begin{figure}
\begin{minipage}{.4\textwidth}
```{r fig-n-cp, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/plot-n-vs-cp.pdf")
```
\end{minipage}%
\begin{minipage}{.4\textwidth}
```{r fig-n-conv, echo=F, out.width="0.95\\linewidth", fig.align="center"}
knitr::include_graphics("../image/n_vs_convergence.pdf")
```
\end{minipage}
\end{figure}

- **Calibration**: 
  - CIs are mostly above $90\%$ across sample sizes.
  - Converge to $95\%$ as $n$ grows.
  - Scale parameters have better coverage than shape.

- **Convergence Rate**:
  - Improves with $n$, surpassing $95\%$ for $n \geq 100$.
  - Caution for estimates with $n < 100$ in specific setups.

## Conclusion
- Sample size significantly mitigates challenges from right-censoring and masking.
- MLE precision and accuracy enhance notably with growing samples.
- BCa CIs become narrower and more reliable as sample size increases.

# Conclusion

### Key Findings
- Employed maximum likelihood techniques for component reliability estimation in series systems with masked failure data.
- Methods performed robustly despite masking and right-censoring challenges.

### Simulation Insights
- Right-censoring and masking introduce positive bias; more reliable components are most affected.
- Shape parameters harder to estimate than scale.
- Large samples can counteract these challenges.

## Conclusion (cont.)

### Confidence Intervals
- Bootstrapped BCa CIs demonstrated commendable coverage probabilities, even in smaller sample sizes.
  
### Takeaways
- Framework offers a rigorous method for latent component property estimation from limited observational data.
- Techniques validated to provide practical insights in diverse scenarios.
- Enhanced capability for learning from obscured system failure data.

## Future Work and Discussion

Directions to enhance learning from masked data:

- **Relax Masking Conditions**: Assess sensitivity to violations and and explore alternative likelihood models.
- **System Design Deviations**: Assess estimator sensitivity to deviations.
- **Homogenous Shape Parameter**: Analyze trade-offs with the full model.
- **Bootstrap Techniques**: Semi-parametric approaches and prediction intervals.
- **Regularization**: Data augmentation and penalized likelihood methods.
- **Additional Likelihood Contributions**: Predictors, etc.

