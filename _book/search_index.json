[["index.html", "Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data 1 Introduction", " Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data Alex Towell Abstract Accurately estimating reliability of individual components in multi-component systems is challenging when only system-level failure data is observable. This paper develops maximum likelihood techniques to estimate component reliability from right-censored lifetimes and candidate sets indicative of masked failure causes in series systems. A likelihood model accounts for right-censoring and candidate sets. Extensive simulation studies demonstrate accurate and robust performance of the maximum likelihood estimator despite small samples and significant masking and censoring. The bias-corrected accelerated bootstrap provides well-calibrated confidence intervals. The methods expand the capability to quantify latent component properties from limited system reliability data. Key contributions include derivations of likelihood models and validation of estimation techniques via simulations. Together, these advance rigorous component reliability assessment from masked failure data. 1 Introduction Accurately estimating the reliability of individual components in multi-component systems is an important challenge, as component lifetimes and failure causes are often not directly observable. In a series system (Agustin 2011), only system-level failure times may be recorded along with limited information about the failed component. Such masked data poses difficulties for assessing component reliability. Deriving a likelihood model incorporating right-censoring and candidate sets to enable masked data to be used for parameter estimation. Demonstrating through simulation studies that the maximum likelihood estimator performs well despite small samples and significant masking and right-censoring. Estimation of scale parameters is more robust than shape parameters in the Weibull model. Showing that bootstrapping provides reasonably well-calibrated confidence intervals for the maximum likelihood estimates, even with small sample sizes. The remainder of the paper details the series system and likelihood models, maximum likelihood estimation methodology, bootstrap confidence interval estimation, and extensive simulation studies exploring estimator behavior under various sample sizes, masking levels, and model assumptions. Together, these contributions provide a statistically rigorous framework for learning about latent component properties from limited observational data on system reliability. The proposed methods expand the capability to quantify component lifetimes in situations where failure data is significantly masked. References "],["statmod.html", "2 Series System Model 2.1 Component Cause of Failure 2.2 System and Component Reliabilities", " 2 Series System Model Consider a system composed of \\(m\\) components arranged in a series configuration. Each component and system has two possible states, functioning or failed. We have \\(n\\) systems whose lifetimes are independent and identically distributed (i.i.d.). The lifetime of the \\(i\\) system denoted by the random variable \\(T_{i}\\). The lifetime of the \\(j\\) component in the \\(i\\) system is denoted by the random variable \\(T_{i j}\\). We assume the component lifetimes in a single system are statistically independent and non-identically distributed. Here, lifetime is defined as the elapsed time from when the new, functioning component (or system) is put into operation until it fails for the first time. A series system fails when any component fails, thus the lifetime of the \\(i\\) system is given by the component with the shortest lifetime, \\[ T_i = \\min\\bigr\\{T_{i 1},T_{i 2}, \\ldots, T_{i m} \\bigr\\}. \\] There are three particularly important distribution functions in reliability analysis: the reliability function, the probability density function, and the hazard function. The reliability function, \\(R_{T_i}(t)\\), is the probability that the \\(i\\) system has a lifespan larger than a duration \\(t\\), \\[\\begin{equation} R_{T_i}(t) = \\Pr\\{T_i &gt; t\\}\\\\ \\end{equation}\\] The probability density function (pdf) of \\(T_i\\) is denoted by \\(f_{T_i}(t)\\) and may be defined as \\[ f_{T_i}(t) = -\\frac{d}{dt} R_{T_i}(t). \\] Next, we introduce the hazard function. The probability that a failure occurs between \\(t\\) and \\(\\Delta t\\) given that no failure occurs before time \\(t\\) is given by \\[ \\Pr\\{T_i \\leq t+\\Delta t|T_i &gt; t\\} = \\frac{\\Pr\\{t &lt; T_i &lt; t+\\Delta t\\}}{\\Pr\\{T_i &gt; t\\}}. \\] The failure rate is given by the dividing this equation by the length of the time interval, \\(\\Delta t\\): \\[ \\frac{\\Pr\\{t &lt; T_i &lt; t+\\Delta t\\}}{\\Delta t} \\frac{1}{\\Pr\\{T_i &gt; t\\}} = \\frac{R_{T_i}(t) - R_{T_i}(t+\\Delta t)}{R_{T_i}(t)}. \\] The hazard function \\(h_{T_i}(t)\\) for \\(T_i\\) is the instantaneous failure rate at time \\(t\\), which is given by \\[\\begin{equation} \\label{eq:failure_rate} \\begin{split} h_{T_i}(t) &amp;= \\lim_{\\Delta t \\to 0} \\frac{\\Pr\\{t &lt; T_i &lt; t+\\Delta t\\}}{\\Delta t} \\frac{1}{\\Pr\\{T_i &gt; t\\}}\\\\ &amp;= \\frac{f_{T_i}(t)}{R_{T_i}(t)}. \\end{split} \\end{equation}\\] The lifetime of the \\(j\\) component is assumed to follow a parametric distribution indexed by a parameter vector \\(\\boldsymbol{\\theta_j}\\). The parameter vector of the overall system is defined as \\[ \\boldsymbol{\\theta }= (\\boldsymbol{\\theta_1},\\ldots,\\boldsymbol{\\theta_m}). \\] When a random variable \\(X\\) is parameterized by a particular \\(\\boldsymbol{\\theta}\\), we denote the reliability function by \\(R_X(t;\\boldsymbol{\\theta})\\), and the same for the other distribution functions. As a special case, for the components in the series system, we subscript by their labels, e.g, the \\(j\\) component’s pdf is denoted by \\(f_j(t;\\boldsymbol{\\theta_j})\\). Two continuous random variables \\(X\\) and \\(Y\\) have a joint pdf \\(f_{X,Y}(x,y)\\). Given the joint pdf \\(f(x,y)\\), the marginal pdf of \\(X\\) is given by \\[ f_X(x) = \\int_{\\mathcal{Y}} f_{X,Y}(x,y) dy, \\] where \\(\\mathcal{Y}\\) is the support of \\(Y\\). (If \\(Y\\) is discrete, replace the integration with a summation over \\(\\mathcal{Y}\\).) The conditional pdf of \\(Y\\) given \\(X=x\\), \\(f_{Y|X}(y|x)\\), is defined as \\[ f_{X|Y}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}. \\] We may generalize all of the above to more than two random variables, e.g., the joint pdf of \\(X_1,\\ldots,X_m\\) is denoted by \\(f(x_1,\\ldots,x_m)\\). Next, we dive deeper into these concepts and provide mathematical derivations for the reliability function, pdf, and hazard function of the series system. We begin with the reliability function of the series system, as given by the following theorem. Theorem 2.1 The series system has a reliability function given by \\[\\begin{equation} \\label{eq:sys_reliability_function} R_{T_i}(t;\\boldsymbol{\\theta}) = \\prod_{j=1}^m R_j(t;\\boldsymbol{\\theta_j}). \\end{equation}\\] Proof. The reliability function is defined as \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\Pr\\{T_i &gt; t\\} \\] which may be rewritten as \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\Pr\\{\\min\\{T_{i 1},\\ldots,T_{i m}\\} &gt; t\\}. \\] For the minimum to be larger than \\(t\\), every component must be larger than \\(t\\), \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\Pr\\{T_{i 1} &gt; t,\\ldots,T_{i m} &gt; t\\}. \\] Since the component lifetimes are independent, by the product rule the above may be rewritten as \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\Pr\\{T_{i 1} &gt; t\\} \\times \\cdots \\times \\Pr\\{T_{i m} &gt; t\\}. \\] By definition, \\(R_j(t;\\boldsymbol{\\theta}) = \\Pr\\{T_{i j} &gt; t\\}\\). Performing this substitution obtains the result \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\prod_{j=1}^m R_j(t;\\boldsymbol{\\theta_j}). \\] Theorem 2.1 shows that the system’s overall reliability is the product of the reliabilities of its individual components. This property is inherent to series systems and will be used in the subsequent derivations. Next, we turn our attention to the pdf of the system lifetime, described in the following theorem. Theorem 2.2 The series system has a pdf given by \\[\\begin{equation} \\label{eq:sys_pdf} f_{T_i}(t;\\boldsymbol{\\theta}) = \\sum_{j=1}^m f_j(t;\\boldsymbol{\\theta_j}) \\prod_{\\substack{k=1\\\\k\\neq j}}^m R_k(t;\\boldsymbol{\\theta_j}). \\end{equation}\\] Proof. By definition, the pdf may be written as \\[ f_{T_i}(t;\\boldsymbol{\\theta}) = -\\frac{d}{dt} \\prod_{j=1}^m R_j(t;\\boldsymbol{\\theta_j}). \\] By the product rule, this may be rewritten as \\[\\begin{align*} f_{T_i}(t;\\boldsymbol{\\theta}) &amp;= -\\frac{d}{dt} R_1(t;\\boldsymbol{\\theta_1})\\prod_{j=2}^m R_j(t;\\boldsymbol{\\theta_j}) - R_1(t;\\boldsymbol{\\theta_1}) \\frac{d}{dt} \\prod_{j=2}^m R_j(t;\\boldsymbol{\\theta_j})\\\\ &amp;= f_1(t;\\boldsymbol{\\theta}) \\prod_{j=2}^m R_j(t;\\boldsymbol{\\theta_j}) - R_1(t;\\boldsymbol{\\theta_1}) \\frac{d}{dt} \\prod_{j=2}^m R_j(t;\\boldsymbol{\\theta_j}). \\end{align*}\\] Recursively applying the product rule \\(m-1\\) times results in \\[ f_{T_i}(t;\\boldsymbol{\\theta}) = \\sum_{j=1}^{m-1} f_j(t;\\boldsymbol{\\theta_j}) \\prod_{\\substack{k=1\\\\k \\neq j}}^m R_k(t;\\boldsymbol{\\theta_k}) - \\prod_{j=1}^{m-1} R_j(t;\\boldsymbol{\\theta_j}) \\frac{d}{dt} R_m(t;\\boldsymbol{\\theta_m}), \\] which simplifies to \\[ f_{T_i}(t;\\boldsymbol{\\theta})= \\sum_{j=1}^m f_j(t;\\boldsymbol{\\theta_j}) \\prod_{\\substack{k=1\\\\k \\neq j}}^m R_k(t;\\boldsymbol{\\theta_k}). \\] Theorem 2.2 shows the pdf of the system lifetime as a function of the pdfs and reliabilities of its components. We continue with the hazard function of the system lifetime, defined in the next theorem. Theorem 2.3 The series system has a hazard function given by \\[\\begin{equation} \\label{eq:sys-failure-rate} h_{T_i}(t;\\boldsymbol{\\theta}) = \\sum_{j=1}^m h_j(t;\\boldsymbol{\\theta_j}). \\end{equation}\\] Proof. By Equation , the \\(i\\) series system lifetime has a hazard function defined as \\[ h_{T_i}(t;\\boldsymbol{\\theta}) = \\frac{f_{T_i}(t;\\boldsymbol{\\theta})}{R_{T_i}(t;\\boldsymbol{\\theta})}. \\] Plugging in expressions for these functions results in \\[ h_{T_i}(t;\\boldsymbol{\\theta}) = \\frac{\\sum_{j=1}^m f_j(t;\\boldsymbol{\\theta_j}) \\prod_{\\substack{k=1\\\\k \\neq j}}^m R_k(t;\\boldsymbol{\\theta_k})} {\\prod_{j=1}^m R_j(t;\\boldsymbol{\\theta_j})}, \\] which can be simplified to \\[ h_{T_i}(t;\\boldsymbol{\\theta}) = \\sum_{j=1}^m \\frac{f_j(t;\\boldsymbol{\\theta_j})}{R_j(t;\\boldsymbol{\\theta_j})} = \\sum_{j=1}^m h_j(t;\\boldsymbol{\\theta_j}). \\] Theorem 2.3 reveals that the system’s hazard function is the sum of the hazard functions of its components. By definition, the hazard function is the ratio of the pdf to the reliability function, \\[ h_{T_i}(t;\\boldsymbol{\\theta}) = \\frac{f_{T_i}(t;\\boldsymbol{\\theta})}{R_{T_i}(t;\\boldsymbol{\\theta})}, \\] and we can rearrange this to get \\[\\begin{equation} \\label{eq:sys_pdf_2} \\begin{split} f_{T_i}(t;\\boldsymbol{\\theta}) &amp;= h_{T_i}(t;\\boldsymbol{\\theta}) R_{T_i}(t;\\boldsymbol{\\theta})\\\\ &amp;= \\biggl\\{\\sum_{j=1}^m h_j(t;\\boldsymbol{\\theta_j})\\biggr\\} \\biggl\\{ \\prod_{j=1}^m R_j(t;\\boldsymbol{\\theta_j}) \\biggr\\}, \\end{split} \\end{equation}\\] which we sometimes find to be a more convenient form than Equation . In this section, we derived the mathematical forms for the system’s reliability, probability density, and hazard functions. Next, we build upon these concepts to derive distributions related to the component cause of failure. 2.1 Component Cause of Failure Whenever a series system fails, precisely one of the components is the cause. We model the component cause of the series system failure as a random variable. Definition 2.1 The component cause of failure of a series system is denoted by the random variable \\(K_i\\) whose support is given by \\(\\{1,\\ldots,m\\}\\). For example, \\(K_i=j\\) indicates that the component indexed by \\(j\\) failed first, i.e., \\[ T_{i j} &lt; T_{i j&#39;} \\] for every \\(j&#39;\\) in the support of \\(K_i\\) except for \\(j\\). Since we have series systems, \\(K_i\\) is unique. The system lifetime and the component cause of failure has a joint distribution given by the following theorem. Theorem 2.4 The joint pdf of the component cause of failure \\(K_i\\) and series system lifetime \\(T_i\\) is given by \\[\\begin{equation} \\label{eq:f_k_and_t} f_{K_i,T_i}(j,t;\\boldsymbol{\\theta}) = h_j(t;\\boldsymbol{\\theta_j}) \\prod_{l=1}^m R_l(t;\\boldsymbol{\\theta}), \\end{equation}\\] where \\(h_j(t;\\boldsymbol{\\theta_j})\\) is the hazard function of the \\(j\\) component and \\(R_l(t;\\boldsymbol{\\theta_l})\\) is the reliability function of the series system. Proof. Consider a series system with \\(3\\) components. By the assumption that component lifetimes are mutually independent, the joint pdf of \\(T_{i 1},T_{i 2},T_{i 3}\\) is given by \\[ f(t_1,t_2,t_3;\\boldsymbol{\\theta}) = \\prod_{j=1}^{3} f_j(t;\\boldsymbol{\\theta_j}). \\] The first component is the cause of failure at time \\(t\\) if \\(K_i = 1\\) and \\(T_i = t\\), which may be rephrased as the likelihood that \\(T_{i 1} = t\\), \\(T_{i 2} &gt; t\\), and \\(T_{i 3} &gt; t\\). Thus, \\[\\begin{align*} f_{K_i,T_i}(j;\\boldsymbol{\\theta}) &amp;= \\int_t^{\\infty} \\int_t^{\\infty} f_1(t;\\boldsymbol{\\theta_1}) f_2(t_2;\\boldsymbol{\\theta_2}) f_3(t_3;\\boldsymbol{\\theta_3}) dt_3 dt_2\\\\ &amp;= \\int_t^{\\infty} f_1(t;\\boldsymbol{\\theta_1}) f_2(t_2;\\boldsymbol{\\theta_2}) R_3(t;\\boldsymbol{\\theta_3}) dt_2\\\\ &amp;= f_1(t;\\boldsymbol{\\theta_1}) R_2(t;\\boldsymbol{\\theta_2}) R_3(t_1;\\boldsymbol{\\theta_3}). \\end{align*}\\] Since \\(h_1(t;\\boldsymbol{\\theta_1}) = f_1(t;\\boldsymbol{\\theta_1}) / R_1(t;\\boldsymbol{\\theta_1})\\), \\[ f_1(t;\\boldsymbol{\\theta_1}) = h_1(t;\\boldsymbol{\\theta_1}) R_1(t;\\boldsymbol{\\theta_1}). \\] Making this substitution into the above expression for \\(f_{K_i,T_i}(j,t;\\boldsymbol{\\theta})\\) yields \\[ f_{K_i,T_i}(j,t;\\boldsymbol{\\theta}) = h_1(t;\\boldsymbol{\\theta_1}) \\prod_{l=1}^m R_l(t;\\boldsymbol{\\theta_l})\\\\ \\] Generalizing from this completes the proof. Theorem 2.4 shows that the joint pdf of the component cause of failure and system lifetime is a function of the hazard functions and reliability functions of the components. This result will be used in the Section 3 to derive the likelihood function for the masked data. The probability that the \\(j\\) component is the cause of failure is given by the following theorem. Theorem 2.5 The probability that the \\(j\\) component is the cause of failure is given by \\[\\begin{equation} \\label{eq:prob_k} \\Pr\\{K_i = j\\} = E_{\\boldsymbol{\\theta}} \\biggl[ \\frac{h_j(T_i;\\boldsymbol{\\theta_j})} {\\sum_{l=1}^m h_l(T_i ; \\boldsymbol{\\theta_l})} \\biggr] \\end{equation}\\] where \\(K_i\\) is the random variable denoting the component cause of failure of the \\(i\\) system and \\(T_i\\) is the random variable denoting the lifetime of the \\(i\\) system. Proof. The probability the \\(j\\) component is the cause of failure is given by marginalizing the joint pdf of \\(K_i\\) and \\(T_i\\) over \\(T_i\\), \\[ \\Pr\\{K_i = j\\} = \\int_0^{\\infty} f_{K_i,T_i}(j,t;\\boldsymbol{\\theta}) dt. \\] By Theorem 2.4, this is equivalent to \\[\\begin{align*} \\Pr\\{K_i = j\\} &amp;= \\int_0^{\\infty} h_j(t;\\boldsymbol{\\theta_j}) R_{T_i}(t;\\boldsymbol{\\theta}) dt\\\\ &amp;= \\int_0^{\\infty} \\biggl(\\frac{h_j(t;\\boldsymbol{\\theta_j})}{h_{T_i}(t ; \\boldsymbol{\\theta})}\\biggr) f_{T_i}(t ; \\boldsymbol{\\theta}) dt\\\\ &amp;= E_{\\boldsymbol{\\theta}}\\biggl[\\frac{h_j(T_i;\\boldsymbol{\\theta_j})}{\\sum_{l=1}^m h_l(T_i ; \\boldsymbol{\\theta_l})}\\biggr]. \\end{align*}\\] Theorem 2.6 The probability that the \\(j\\) component is the cause of system failure given only that we know a system failure occured at time \\(t_i\\) is given by \\[ \\Pr\\{K_i = j|T_i=t_i\\} = \\frac{h_j(t_i;\\boldsymbol{\\theta_j})}{\\sum_{l=1}^m h_l(t_i;\\boldsymbol{\\theta_l})}. \\] Proof. By the definition of conditional probability, \\[\\begin{align*} \\Pr\\{K_i = j|T_i = t_i\\} &amp;= \\frac{f_{K_i,T_i}(j, t_i;\\boldsymbol{\\theta})}{f_{T_i}(t_i;\\boldsymbol{\\theta})}\\\\ &amp;= \\frac{h_j(t_i;\\boldsymbol{\\theta_j})}{R_{T_i}(t_i;\\boldsymbol{\\theta})} / f_{T_i}(t_i;\\boldsymbol{\\theta}) \\end{align*}\\] By the property that \\(h_{T_i}(t_i;\\boldsymbol{\\theta}) = f_{T_i}(t_i;\\boldsymbol{\\theta}) / R_{T_i}(t_i;\\boldsymbol{\\theta})\\), we may do this substitution and simplify to obtain \\[ \\Pr\\{K_i = j|T_i = t_i\\} = \\frac{h_j(t_i;\\boldsymbol{\\theta_j})}{\\sum_{l=1}^m h_l(t_i;\\boldsymbol{\\theta_l})}. \\] 2.2 System and Component Reliabilities A common measure of reliability is mean time to failure (MTTF). The MTTF is defined as the expectation of the lifetime, \\[\\begin{equation} \\label{mttf-sys} \\text{MTTF} = E_{\\boldsymbol{\\theta}}\\{T_i\\}, \\end{equation}\\] which if certain assumptions are satisfied1 is equivalent to the integration of the reliability function over its support. While the MTTF provides a summary measure of reliability, it is not a complete description. Depending on the failure characteristics, MTTF can be misleading. For example, a system that has a high likelihood of failing early in its life may still have a large MTTF if it is fat-tailed.2 The reliability of the components in the series system determines the reliability of the system. We denote the MTTF of the \\(j\\) component by \\(\\text{MTTF}_j\\) and, according to Equation , the probability that the \\(j\\) component is the cause of failure is given by \\(\\Pr\\{K_i = j\\}\\). In a well-designed series system, there is no component that is the “weakest link” that either has a much shorter MTTF or a much higher probability of being the component cause of failure than any of the other components, e.g., \\(\\Pr\\{K_i = j\\} \\approx \\Pr\\{K_i = k\\}\\) and and MTTF\\(_j \\approx\\) MTTF\\(_k\\) for all \\(j\\) and \\(k\\). This just means that the components should have similar reliabilities and failure characteristics. We use these results in the simulation study in Section 7, where we assess the sensitivity of the MLE with respect to varying the reliability of one of the Weibull components. We vary its reliability in two different ways: We vary its shape parameter (keeping its scale parameter constant), which determines the failure characteristics of the component and also affects its MTTF. We vary its scale parameter (keeping its shape parameter constant), which scales its MTTF while retaining the same failure characteristics. References "],["like-model.html", "3 Likelihood Model for Masked Data 3.1 Masked Component Cause of Failure 3.2 Right-Censored Data 3.3 Identifiability and Convergence Issues", " 3 Likelihood Model for Masked Data The object of interest is the (unknown) parameter value \\(\\boldsymbol{\\theta}\\). To estimate this \\(\\boldsymbol{\\theta}\\), we need data. In our case, we call it masked data because we do not necessarily observe the event of interest, say a system failure, directly. We consider two types of masking: masking the system failure lifetime and masking the component cause of failure. We generally encounter three types of system failure lifetime masking: A system failure is observed at a particular point in time. A system failure is observed to occur within a particular interval of time. A system failure is not observed, but we know that the system survived at least until a particular point in time. This is known as right-censoring and can occur if, for instance, an experiment is terminated while the system is still functioning. We generally encounter two types of component cause of failure masking: The component cause of failure is observed. The component cause of failure is not observed, but we know that the failed component is in some set of components. This is known as masking the component cause of failure. Thus, the component cause of failure masking will take the form of candidate sets. A candidate set consists of some subset of component labels that plausibly contains the label of the failed component. The sample space of candidate sets are all subsets of \\(\\{1,\\ldots,m\\}\\), thus there are \\(2^m\\) possible outcomes in the sample space. In this paper, we limit our focus to observing right censored lifetimes and exact lifetimes but with masked component cause of failures. We consider a sample of \\(n\\) i.i.d. series systems, each of which is put into operation at some time and and observed until either it fails or is right-censored. We denote the right-censoring time of the \\(i\\) system by \\(\\tau_i\\). We do not directly observe the system lifetime, \\(T_i\\), but rather, we observe the right-censored lifetime, \\(S_i\\), which is given by \\[\\begin{equation} S_i = \\min\\{\\tau_i, T_i\\}, \\end{equation}\\] We also observe a right-censoring indicator, \\(\\delta_i\\), which is given by \\[\\begin{equation} \\delta_i = 1_{T_i &lt; \\tau_i} \\end{equation}\\] where \\(1_{\\text{condition}}\\) is an indicator function that outputs \\(1\\) if condition is true and \\(0\\) otherwise. Here, \\(\\delta_i = 1\\) indicates the event of interest, a system failure, was observed. If a system failure lifetime is observed, then we also observe a candidate set that contains the component cause of failure. We denote the candidate set for the \\(i\\) system by \\(\\mathcal{C}_i\\), which is a subset of \\(\\{1,\\ldots,m\\}\\). Since the data generating process for candidate sets may be subject to chance variations, it as a random set. Consider we have an independent and identically distributed (i.i.d.) random sample of masked data, \\(D = \\{D_1, \\ldots, D_n\\}\\), where each \\(D_i\\) contanis the following: \\(S_i\\), the system lifetime of the \\(i\\) system. \\(\\delta_i\\), the right-censoring indicator of the \\(i\\) system. \\(\\mathcal{C}_i\\), the set of candidate component causes of failure for the \\(i\\) system. The masked data generation process is illustrated by Figure . An example of masked data \\(D\\) for exact, right-censored system failure times with candidate sets that mask the component cause of failure can be seen in Table 1 for a series system with \\(m=3\\) components. Right-censored lifetime data with masked component cause of failure. System Right-censoring time (\\(S_i\\)) Right censoring indicator (\\(\\delta_i\\)) Candidate set (\\(\\mathcal{C}_i\\)) 1 \\(4.3\\) 1 \\(\\{1,2\\}\\) 2 \\(1.3\\) 1 \\(\\{2\\}\\) 3 \\(5.4\\) 0 \\(\\emptyset\\) 4 \\(2.6\\) 1 \\(\\{2,3\\}\\) 5 \\(3.7\\) 1 \\(\\{1,2,3\\}\\) 6 \\(10\\) 0 \\(\\emptyset\\) In our model, we assume the data is governed by a pdf, which is determined by a specific parameter, represented as \\(\\boldsymbol{\\theta}\\) within the parameter space \\(\\boldsymbol{\\Omega}\\). The joint pdf of the data \\(D\\) can be represented as follows: \\[ f(D ; \\boldsymbol{\\theta}) = \\prod_{i=1}^n f(s_i,\\delta_i,c_i;\\boldsymbol{\\theta}), \\] where \\(s_i\\) is the observed system lifetime of the \\(i\\) system, \\(\\delta_i\\) is the observed right-censoring indicator of the \\(i\\) system, and \\(c_i\\) is the observed candidate set of the \\(i\\) system. This joint pdf tells us how likely we are to observe the particular data, \\(D\\), given the parameter \\(\\boldsymbol{\\theta}\\). When we keep the data constant and allow the parameter \\(\\boldsymbol{\\theta}\\) to vary, we obtain what is called the likelihood function \\(L\\), defined as \\[ L(\\boldsymbol{\\theta}) = \\prod_{i=1}^n L_i(\\boldsymbol{\\theta}) \\] where \\[ L_i(\\boldsymbol{\\theta}) = f(s_i,\\delta_i,c_i;\\boldsymbol{\\theta}) \\] is the likelihood contribution of the \\(i\\) system. In other words, the likelihood function quantifies how likely different parameter values \\(\\boldsymbol{\\theta}\\) are, given the observed data. For each type of data, right-censored data and masked component cause of failure data, we will derive the likelihood contribution \\(L_i\\), which refers to the part of the likelihood function that this particular piece of data contributes to. We present the following theorem for the likelihood contribution model. In the follow subsections, we prove this result for each type of masked data, right-censored system lifetime data \\((\\delta_i = 0)\\) and masking of the component cause of failure \\((\\delta_i = 1)\\). 3.1 Masked Component Cause of Failure Suppose a diagnostician is unable to identify the precise component cause of the failure, e.g., due to cost considerations he or she replaced multiple components at once, successfully repairing the system but failing to precisely identity the failed component. In this case, the cause of failure is said to be masked. The unobserved component lifetimes may have many covariates, like ambient operating temperature, but the only covariate we observe in our masked data model are the system’s lifetime and additional masked data in the form of a candidate set that is somehow correlated with the unobserved component lifetimes. The key goal of our analysis is to estimate the parameter \\(\\boldsymbol{\\theta}\\), which maximize the likelihood of the observed data, and to estimate the precision and accuracy of this estimate using the Bootstrap method. To achieve this, we first need to assess the joint distribution of the system’s continuous lifetime, \\(T_i\\), and the discrete candidate set, \\(\\mathcal{C}_i\\), which can be written as \\[ f_{T_i,\\mathcal{C}_i}(t_i,c_i;\\boldsymbol{\\theta}) = f_{T_i}(t_i;\\boldsymbol{\\theta}) \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i = c_i | T_i = t_i\\}, \\] where \\(f_{T_i}(t_i;\\boldsymbol{\\theta})\\) is the pdf of \\(T_i\\) and \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i = c_i | T_i = t_i\\}\\) is the conditional pmf of \\(\\mathcal{C}_i\\) given \\(T_i = t_i\\). We assume the pdf \\(f_{T_i}(t_i;\\boldsymbol{\\theta})\\) is known, but we do not have knowledge of \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i = c_i | T_i = t_i\\}\\), i.e., the data generating process for candidate sets is unknown. However, it is critical that the masked data, \\(\\mathcal{C}_i\\), is correlated with the \\(i\\) system. This way, the conditional distribution of \\(\\mathcal{C}_i\\) given \\(T_i = t_i\\) may provide information about \\(\\boldsymbol{\\theta}\\), despite our Statistical interest being primarily in the series system rather than the candidate sets. To make this problem tractable, we assume a set of conditions that make it unnecessary to estimate the generative processes for candidate sets. The most important way in which \\(\\mathcal{C}_i\\) is correlated with the \\(i\\) system is given by assuming the following condition. Assuming Condition , \\(\\mathcal{C}_i\\) must contain the index of the failed component, but we can say little else about what other component indices may appear in \\(\\mathcal{C}_i\\). In order to derive the joint distribution of \\(\\mathcal{C}_i\\) and \\(T_i\\) assuming Condition , we take the following approach. We notice that \\(\\mathcal{C}_i\\) and \\(K_i\\) are statistically dependent. We denote the conditional pmf of \\(\\mathcal{C}_i\\) given \\(T_i = t_i\\) and \\(K_i = j\\) as \\[ \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i = c_i | T_i = t_i, K_i = j\\}. \\] Even though \\(K_i\\) is not observable in our masked data model, we can still consider the joint distribution of \\(T_i\\), \\(K_i\\), and \\(\\mathcal{C}_i\\). By Theorem 2.4, the joint pdf of \\(T_i\\) and \\(K_i\\) is given by \\[ f_{T_i,K_i}(t_i,j;\\boldsymbol{\\theta}) = h_j(t_i;\\boldsymbol{\\theta_j}) R_{T_i}(t_i;\\boldsymbol{\\theta}), \\] where \\(h_j(t_i;\\boldsymbol{\\theta_j})\\) is the hazard function for the \\(j\\) component and \\(R_{T_i}(t_i;\\boldsymbol{\\theta})\\) is the reliability function of the system. Thus, the joint pdf of \\(T_i\\), \\(K_i\\), and \\(\\mathcal{C}_i\\) may be written as \\[\\begin{equation} \\label{eq:joint_pdf_t_k_c} \\begin{split} f_{T_i,K_i,\\mathcal{C}_i}(t_i,j,c_i;\\boldsymbol{\\theta}) &amp;= f_{T_i,K_i}(t_i,k;\\boldsymbol{\\theta}) \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\}\\\\ &amp;= h_j(t_i;\\boldsymbol{\\theta_j}) R_{T_i}(t_i;\\boldsymbol{\\theta}) \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\}. \\end{split} \\end{equation}\\] We are going to need the joint pdf of \\(T_i\\) and \\(\\mathcal{C}_i\\), which may be obtained by summing over the support \\(\\{1,\\ldots,m\\}\\) of \\(K_i\\) in Equation , \\[ f_{T_i,\\mathcal{C}_i}(t_i,c_i;\\boldsymbol{\\theta}) = R_{T_i}(t_i;\\boldsymbol{\\theta}) \\sum_{j=1}^m \\biggl\\{ h_j(t_i;\\boldsymbol{\\theta_j}) \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\} \\biggr\\}. \\] By Condition , \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\} = 0\\) when \\(K_i = j\\) and \\(j \\notin c_i\\), and so we may rewrite the joint pdf of \\(T_i\\) and \\(\\mathcal{C}_i\\) as \\[\\begin{equation} \\label{eq:part1} f_{T_i,\\mathcal{C}_i}(t_i,c_i;\\boldsymbol{\\theta}) = R_{T_i}(t_i;\\boldsymbol{\\theta}) \\sum_{j \\in c_i} \\biggl\\{ h_j(t_i;\\boldsymbol{\\theta_j}) \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\} \\biggr\\}. \\end{equation}\\] When we try to find an MLE of \\(\\boldsymbol{\\theta}\\) (see Section 4), we solve the simultaneous equations of the MLE and choose a solution \\(\\hat{\\boldsymbol{\\theta}}\\) that is a maximum for the likelihood function. When we do this, we find that \\(\\hat{\\boldsymbol{\\theta}}\\) depends on the unknown conditional pmf \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\}\\). So, we are motivated to seek out more conditions (that approximately hold in realistic situations) whose MLEs are independent of the pmf \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\}\\). In many industrial problems, masking generally occurred due to time constraints and the expense of failure analysis (Guess, Hodgson, and Usher 1991). In this setting, Condition generally holds. Assuming Conditions and , \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j\\}\\) may be factored out of the summation in Equation , and thus the joint pdf of \\(T_i\\) and \\(\\mathcal{C}_i\\) may be rewritten as \\[ f_{T_i,\\mathcal{C}_i}(t_i,c_i;\\boldsymbol{\\theta}) = \\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\\} R_{T_i}(t_i;\\boldsymbol{\\theta}) \\sum_{j \\in c_i} h_j(t_i;\\boldsymbol{\\theta_j}) \\] where \\(j&#39; \\in c_i\\). If \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\\}\\) is a function of \\(\\boldsymbol{\\theta}\\), the MLEs are still dependent on the unknown \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\\}\\). This is a more tractable problem, but we are primarily interested in the situation where we do not need to know (nor estimate) \\(\\Pr{}_{\\!\\boldsymbol{\\theta}}\\{\\mathcal{C}_i=c_i|T_i=t_i,K_i=j&#39;\\}\\) to find an MLE of \\(\\boldsymbol{\\theta}\\). The last condition we assume achieves this result. When Conditions , , and are satisfied, the joint pdf of \\(T_i\\) and \\(\\mathcal{C}_i\\) is given by \\[ f_{T_i,\\mathcal{C}_i}(t_i,c_i;\\boldsymbol{\\theta}) = \\beta_i R_{T_i}(t_i;\\boldsymbol{\\theta}) \\sum_{j \\in c_i} h_j(t_i;\\boldsymbol{\\theta_j}). \\] When we fix the sample and allow \\(\\boldsymbol{\\theta}\\) to vary, we obtain the contribution to the likelihood \\(L\\) from the \\(i\\) observation when the system lifetime is exactly known (i.e., \\(\\delta_i = 1\\)) but the component cause of failure is masked by a candidate set \\(c_i\\): \\[\\begin{equation} \\label{eq:likelihood_contribution_masked} L_i(\\boldsymbol{\\theta}) = R_{T_i}(t_i;\\boldsymbol{\\theta}) \\sum_{j \\in c_i} h_j(t_i;\\boldsymbol{\\theta_j}). \\end{equation}\\] To summarize this result, assuming Conditions , , and , if we observe an exact system failure time for the \\(i\\)-th system (\\(\\delta_i = 1\\)), but the component that failed is masked by a candidate set \\(c_i\\), then its likelihood contribution is given by Equation . 3.2 Right-Censored Data As described in Section 3, we observe realizations of \\((S_i,\\delta_i,\\mathcal{C}_i)\\) where \\(S_i = \\min\\{T_i,\\tau_i\\}\\) is the right-censored system lifetime, \\(\\delta_i = 1_{\\{T_i &lt; \\tau_i\\}}\\) is the right-censoring indicator, and \\(\\mathcal{C}_i\\) is the candidate set. In the previous section, we discussed the likelihood contribution from an observation of a masked component cause of failure, i.e., \\(\\delta_i = 1\\). We now derive the likelihood contribution of a right-censored observation \\((\\delta_i = 0\\)) in our masked data model. When we combine the two likelihood contributions, we obtain the likelihood contribution for the \\(i\\) system shown in Theorem , \\[ L_i(\\boldsymbol{\\theta}) = \\begin{cases} R_{T_i}(s_i;\\boldsymbol{\\theta}) &amp;\\text{ if } \\delta_i = 0\\\\ \\beta_i R_{T_i}(s_i;\\boldsymbol{\\theta}) \\sum_{j\\in c_i} h_j(s_i;\\boldsymbol{\\theta_j}) &amp;\\text{ if } \\delta_i = 1. \\end{cases} \\] We use this result in Section 4 to derive the maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\theta}\\). 3.3 Identifiability and Convergence Issues In our likelihood model, masking and right-censoring can lead to issues related to identifiability and flat likelihood regions. Identifiability refers to the unique mapping of the model parameters to the likelihood function, and lack of identifiability can lead to multiple sets of parameters that explain the data equally well, making inference about the true parameters challenging (Lehmann and Casella 1998), while flat likelihood regions can complicate convergence (Wu 1983). In our simulation study, we address these challenges in a pragmatic way. Specifically, failure to converge to a solution within a maximum of 125 iterations is interpreted as evidence of the aforementioned issues, leading to the discarding of the sample, with the process then repeated with a new synthetic sample. Note, however, that in Section 5 where we discuss the bias-corrected and accelerated (BCa) bootstrap method for constructing confidence intervals, we do not discard any resamples. This strategy helps ensure the robustness of the results, while acknowledging the inherent complexities of likelihood-based estimation in models characterized by masking and right-censoring. References "],["mle.html", "4 Maximum Likelihood Estimation", " 4 Maximum Likelihood Estimation In our analysis, we use maximum likelihood estimation (MLE) to estimate the series system parameter \\(\\boldsymbol{\\theta}\\) from the masked data (Bain and Engelhardt 1992; Casella and Berger 2002). The MLE finds parameter values that maximize the likelihood of the observed data under the assumed model. A maximum likelihood estimate, \\(\\hat{\\boldsymbol{\\theta}}\\), is a solution of \\[\\begin{equation} \\label{eq:mle} L(\\hat{\\boldsymbol{\\theta}}) = \\max_{\\boldsymbol{\\theta }\\in \\boldsymbol{\\Omega}} L(\\boldsymbol{\\theta}), \\end{equation}\\] where \\(L(\\boldsymbol{\\theta})\\) is the likelihood function of the observed data. For computational efficiency and analytical simplicity, we work with the log-likelihood function, denoted as \\(\\ell(\\boldsymbol{\\theta})\\), instead of the likelihood function (Casella and Berger 2002). The MLE, \\(\\hat{\\boldsymbol{\\theta}}\\), is often found by solving a system of equations derived from setting the derivative of the log-likelihood function to zero, i.e., \\[\\begin{equation} \\label{eq:mle_eq} \\frac{\\partial}{\\partial \\theta_j} \\ell(\\boldsymbol{\\theta}) = 0, \\end{equation}\\] for each component \\(\\theta_j\\) of the parameter \\(\\boldsymbol{\\theta}\\) (Bain and Engelhardt 1992). When there’s no closed-form solution, we resort to numerical methods like the Newton-Raphson method. Assuming some regularity conditions, such as the likelihood function being identifiable, the MLE has many desirable asymptotic properties that underpin statistical inference, namely that it is an asymptotically unbiased estimator of the parameter \\(\\boldsymbol{\\theta}\\) and it is normally distributed with a variance given by the inverse of the Fisher Information Matrix (FIM) (Casella and Berger 2002). However, for smaller samples, these asymptotic properties may not yield accurate approximations. We propose to use the bootstrap method to offer an empirical approach for estimating the sampling distribution of the MLE, in particular for computing confidence intervals. References "],["boot.html", "5 Bias-Corrected and Accelerated Bootstrap Confidence Intervals 5.1 Issues with Resampling from the Observed Data", " 5 Bias-Corrected and Accelerated Bootstrap Confidence Intervals We utilize the non-parametric bootstrap to approximate the sampling distribution of the MLE. In the non-parametric bootstrap, we resample from the observed data with replacement to generate a bootstrap sample. The MLE is then computed for the bootstrap sample. This process is repeated \\(B\\) times, giving us \\(B\\) bootstrap replicates of the MLE. The sampling distribution of the MLE is then approximated by the empirical distribution of the bootstrap replicates of the MLE. The method we use to generate confidence intervals is known as Bias-Corrected and Accelerated Bootstrap Confidence Intervals (BCa), which applies two corrections to the standard bootstrap method: Bias correction: This adjusts for bias in the bootstrap distribution itself. This bias is measured as the difference between the mean of the bootstrap distribution and the observed statistic. It works by transforming the percentiles of the bootstrap distribution to correct for these issues. This may be a useful transformation in our case since we are dealing with small samples and we have two potential sources of bias: right-censoring and masking component cause of failure. They seem to have opposing effects on the MLE, but the relationship is difficult to quantify. Acceleration: This adjusts for the rate of change of the statistic as a function of the true, unknown parameter. This correction is important when the shape of the statistic’s distribution changes with the true parameter. Since we have a number of different shape parameters, \\(k_1,\\ldots,k_m\\), we may expect the shape of the distribution of the MLE to change as a function of the true parameter, making this correction potentially useful. Since we are primarly interested in generating confidence intervals for small samples for a potentially biased MLE, the BCa method may be a good choice for our analysis. For more details on BCa, see (Efron 1987). In our simulation study, we will assess the performance of the bootstrapped BCa confidence intervals by computing the coverage probability of the confidence intervals. A well-calibrated 95% confidence interval contains the true value around 95% of the time. If the confidence interval is too narrow, it will have a coverage probability less than 95%, which conveys a sort of false confidence in the precision of the MLE. If the confidence interval is too wide, it will have a coverage probability greater than 95%, which conveys a lack of confidence in the precision of the MLE. We want confidence intervals to be as narrow as possible while still having a coverage probability close to the nominal level, 95%. 5.1 Issues with Resampling from the Observed Data While the bootstrap method provides a robust and flexible tool for statistical estimation, its effectiveness can be influenced by several factors (Efron and Tibshirani 1994). Firstly, instances of non-convergence in our bootstrap samples were observed. Such cases can occur when the estimation method, like the MLE used in our analysis, fails to converge due to the specifics of the resampled data (Casella and Berger 2002). This issue can potentially introduce bias or reduce the effective sample size of our bootstrap distribution. Secondly, the bootstrap’s accuracy can be compromised with small sample sizes, as the method relies on the law of large numbers to approximate the true sampling distribution. For small datasets, the bootstrap samples might not adequately represent the true variability in the data, leading to inaccurate results (Efron and Tibshirani 1994). Thirdly, our data involves right censoring and a masking of the component cause of failure when a system failure is observed. These aspects can cause certain data points or trends to be underrepresented or not represented at all in our data, introducing bias in the bootstrap distribution (Klein and Moeschberger 2005). Despite these challenges, we found the bootstrap method useful in approximating the sampling distribution of the MLE, taking care in interpreting the results, particularly as it relates to coverage probabilities. References "],["weibull.html", "6 Series System with Weibull Components 6.1 Reliability 6.2 Likelihood Model 6.3 Weibull Series System: Homogeneous Shape Parameters", " 6 Series System with Weibull Components The Weibull distribution, introduced by Waloddi Weibull in 1937, has been instrumental in reliability analysis due to its ability to model a wide range of failure behaviors. Reflecting on its utility, Weibull modestly noted that it “[…] may sometimes render good service.” (Abernethy 2006). In the context of our study, we utilize the Weibull to model a system as originating from Weibull components in a series configuration, producing a specific form of the likelihood model described in Section 3, which deals with challenges such as right censoring and masked component cause of failure. The \\(j\\) component of the \\(i\\) has a lifetime distribution given by \\[ T_{i j} \\sim \\operatorname{Weibull}(k_j,\\lambda_j) \\qquad \\text{for } i = 1,\\ldots,n \\text{ and } j = 1,\\ldots,m, \\] where \\(\\lambda_j &gt; 0\\) is the scale parameter and \\(k_j &gt; 0\\) is the shape parameter. The \\(j\\) component has a reliability function, pdf, and hazard function given respectively by \\[\\begin{align} R_j(t;\\lambda_j,k_j) &amp;= \\exp\\biggl\\{-\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j}\\biggr\\},\\\\ f_j(t;\\lambda_j,k_j) &amp;= \\frac{k_j}{\\lambda_j}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j-1} \\exp\\biggl\\{-\\left(\\frac{t}{\\lambda_j}\\right)^{k_j} \\biggr\\},\\\\ h_j(t;\\lambda_j,k_j) \\label{eq:weibull_haz} &amp;= \\frac{k_j}{\\lambda_j}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j-1}. \\end{align}\\] The shape parameter of the Weibull distribtion is of particular importance: \\(k_j &lt; 1\\) indicates infant mortality. An example of how this might arise is a result of defective components being weeded out early, and the remaining components surviving for a much longer time. \\(k_j = 1\\) indicates random failures (independent of age). An example of how this might arise is a result of random shocks to the system, but otherwise the system is age-independent.3 \\(k_j &gt; 1\\) indicates wear-out failures. An example of how this might arise is a result of components wearing as they age We show that the lifetime of the series system composed of \\(m\\) Weibull components has a reliability, hazard, and probability density functions given by the following theorem. ::: {.theorem #sys_weibull} The lifetime of a series system composed of \\(m\\) Weibull components has a reliability function, hazard function, and pdf respectively given by \\[\\begin{align} \\label{eq:sys_weibull_reliability_function} R_{T_i}(t;\\boldsymbol{\\theta}) &amp;= \\exp\\biggl\\{-\\sum_{j=1}^{m}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j}\\biggr\\},\\\\ \\label{eq:sys_weibull_failure_rate_function} h_{T_i}(t;\\boldsymbol{\\theta}) &amp;= \\sum_{j=1}^{m} \\frac{k_j}{\\lambda_j}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j-1},\\\\ \\label{eq:sys_weibull_pdf} f_{T_i}(t;\\boldsymbol{\\theta}) &amp;= \\biggl\\{ \\sum_{j=1}^m \\frac{k_j}{\\lambda_j}\\left(\\frac{t}{\\lambda_j}\\right)^{k_j-1} \\biggr\\} \\exp \\biggl\\{ -\\sum_{j=1}^m \\bigl(\\frac{t}{\\lambda_j}\\bigr)^{k_j} \\biggr\\}. \\end{align}\\] ::: Proof. The proof for the reliability function follows from Theorem 2.1, \\[ R_{T_i}(t;\\boldsymbol{\\theta}) = \\prod_{j=1}^{m} R_j(t;\\lambda_j,k_j). \\] Plugging in the Weibull component reliability functions obtains the result \\[\\begin{align*} R_{T_i}(t;\\boldsymbol{\\theta}) = \\prod_{j=1}^{m} \\exp\\biggl\\{-\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j}\\biggr\\} = \\exp\\biggl\\{-\\sum_{j=1}^{m}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j}\\biggr\\}. \\end{align*}\\] The proof for the hazard function follows from Theorem 2.3, \\[\\begin{align*} h_{T_i}(t;\\boldsymbol{\\theta}) = \\sum_{j=1}^{m} h_j(t;\\boldsymbol{\\theta_j}) = \\sum_{j=1}^{m} \\frac{k_j}{\\lambda_j}\\biggl(\\frac{t}{\\lambda_j}\\biggr)^{k_j-1} \\end{align*}\\] The proof for the pdf follows from Theorem 2.2. By definition, \\[ f_{T_i}(t;\\boldsymbol{\\theta}) = h_{T_i}(t;\\boldsymbol{\\theta}) R_{T_i}(t;\\boldsymbol{\\theta}). \\] Plugging in the failure rate and reliability functions given respectively by Equations and completes the proof. 6.1 Reliability In Section 2.2, we discussed the concept of reliability. In the case of Weibull components, the MTTF of the \\(j\\) component is given by \\[\\begin{equation} \\label{eq:mttf-weibull} \\text{MTTF}_j = \\lambda_j \\Gamma\\biggl(1 + \\frac{1}{k_j}\\biggr), \\end{equation}\\] where \\(\\Gamma\\) is the gamma function. We mentioned that the MTTF can sometimes be a poor measure of reliability, e.g., the MTTF and the probability of failing early can be large. The Weibull is a good example of this phenomenon. If \\(k &gt; 1\\), the Weibull is a fat-tailed distribution, and it can exhibit both a large MTTF and a high probability of failing early. Components may have similar MTTFs, but some components may be more likely to fail early and others may be more likely to fail late, depending upon their failure characterstics (shape parameters), and so the probability of component failure given by Equation is a useful measure of component reliability compared to the other components in the system. In a well-designed series system, the component failure characteristics are similar: they have a similar MTTF and a similar probability of being the component cause of failure, i.e., they have similar shapes and scales, so that system failures are not dominated by some subset of components. 6.2 Likelihood Model In Section 3, we discussed two separate kinds of likelihood contributions, masked component cause of failure data (with exact system failure times) and right-censored data. The likelihood contribution of the \\(i\\) system is given by the following theorem. Taking the log of the likelihood contribution function obtains the following result. See Appendix A for the R code that implements the log-likelihood function for the series system with Weibull components. We find an MLE by solving , i.e., a point \\(\\boldsymbol{\\hat\\theta} = (\\hat{k}_1,\\hat{\\lambda}_1,\\ldots,\\hat{k}_m,\\hat{\\lambda}_m)\\) satisfying \\(\\nabla_{\\theta} \\ell(\\boldsymbol{\\hat\\theta}) = \\boldsymbol{0}\\), where \\(\\nabla_{\\boldsymbol{\\theta}}\\) is the gradient of the log-likelihood function (score) with respect to \\(\\boldsymbol{\\theta}\\). To solve this system of equations, we use the Newton-Raphson method, which requires the score and the Hessian of the log-likelihood function. We analytically derive the score since it is useful to have for the Newton-Raphson method, but we do not do the same for the Hessian of the log-likelihood for the following reasons: The gradient is easy to derive, and it is useful to have for computing gradients efficiently and accurately, which will be useful for numerically approximating the Hessian. The Hessian is tedious and error prone to derive, and Newton-like methods often do not require the Hessian to be explicitly computed. The following theorem derives the score function. The result follows from taking the partial derivatives of the log-likelihood contribution of the \\(i\\)-th system given by Equation . It is a tedious calculation so the proof has been omitted, but the result has been verified by using a very precise numerical approximation of the gradient. By the linearity of differentiation, the gradient of a sum of functions is the sum of their gradients, and so the score function conditioned on the entire sample is given by \\[\\begin{equation} \\label{eq:weibull_series_score} \\nabla \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\nabla \\ell_i(\\boldsymbol{\\theta}). \\end{equation}\\] 6.3 Weibull Series System: Homogeneous Shape Parameters A series system composed of Weibull components is not generally Weibull unless the shape parameters of the components are homogeneous. where \\(\\lambda_j\\) is the scale parameter of the \\(j\\) component. Theorem 6.1 If a series system has Weibull components with homogeneous shape parameters, the component cause of failure is conditionally independent of the system failure time: \\[ \\Pr\\{K_i = j | T_i = t_i \\} = \\Pr\\{K_i = j\\} = \\frac{\\lambda_j^{-k}}{\\sum_{l=1}^{m} \\lambda_l^{-k}}. \\] Proof. By Theorem 2.6, the conditional probability of the \\(j\\) component being the cause of failure given the system failure time is given by \\[\\begin{align*} \\Pr\\{K_i = j | T_i = t\\} &amp;= \\frac{f_{K_i, T_i}(j, t;\\boldsymbol{\\theta})}{f_{T_i}(t;\\boldsymbol{\\theta})} = \\frac{h_j(t;k,\\lambda_j) R_{T_i}(t;\\boldsymbol{\\theta})} {h_{T_i}(t;\\boldsymbol{\\theta_j}) R_{T_i}(t;\\boldsymbol{\\theta})}\\\\ &amp;= \\frac{h_j(t;k,\\lambda_j)}{\\sum_{l=1}^m h_l(t;k,\\lambda_l)} = \\frac{\\frac{k}{\\lambda_j}\\bigl(\\frac{t}{\\lambda_j}\\bigr)^{k-1}} {\\sum_{l=1}^m \\frac{k}{\\lambda_l}\\bigl(\\frac{t}{\\lambda_l}\\bigr)^{k-1}} = \\frac{\\bigl(\\frac{1}{\\lambda_j}\\bigr)^k} {\\sum_{l=1}^m \\bigl(\\frac{1}{\\lambda_l}\\bigr)^k}. \\end{align*}\\] If we have prior knowledge that the shape parameters are sufficiently homogenous, it may be useful to simplify the likelihood model by assuming the shape parameters are identical, simplifying the series system to Weibull, facilitating analysis and interpretation. According to the bias-variance trade-off, we expect the MLE to be more biased but have lower sampling variance. We denote the full model log-likelihood function by \\(\\ell_F\\) and the reduced model log-likelihood by \\(\\ell_R\\). The reduced model is obtained by setting the shape parameter of each component to be the same, i.e., \\(k_1 = \\cdots = k_m = k\\). Thus, the reduced model log-likelihood function is given by \\[ \\ell_R(k, \\lambda_1, \\lambda_2, \\cdots, \\lambda_m) = \\ell_F(k, \\lambda_1, k, \\lambda_2, \\ldots, k, \\lambda_m), \\] The same may be done for the score and hessian of the log-likelihood functions. References "],["sim-study.html", "7 Simulation Study: Full Model 7.1 Data Generating Process 7.2 Simulation Scenarios 7.3 Scenario: Assessing the Impact of Right-Censoring 7.4 Scenario: Assessing the Impact of Sample Size 7.5 Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure 7.6 Scenario: Assessing the Impact of Changing the Scale Parameter of Component 3 7.7 Scenario: Assessing the Impact of Changing the Shape Parameter of Component 3", " 7 Simulation Study: Full Model In this section, we conduct a simulation study to assess the performance of the MLE for the full likelihood model defined in Section 6. In this simulation study, we assess the sensitivity of the MLE to various simulation scenarios. In particular, we assess two important properties of the MLE with respect to a scenario: Accuracy (Bias): How close is the expected value of the MLE to the true parameter values? If the expected value of the MLE is close to the true parameter values, the accuracy is high. Precision: How much does the MLE vary from sample to sample? We measure this by assessing the 95% confidence intervals (BCa, Bias-Corrected and accelerated). If the confidence intervals are both small and have good coverage probability (the proportion of confidence intervals that contain the true parameter values), then the MLE is precise. We begin by specifying the parameters of the series system that will be the central object of our simulation study. We consider the data in (Guo, Niu, and Szidarovszky 2013), in which they study the reliability of a series system with three components. They fit Weibull components in a series configuration to the data, resulting in an MLE with shape and scale estimates given by the first three components in Table . To make the model slightly more complex, we add two more components to this series system, with shape and scale parameters given by the last two components in Table . We will refer to this system as the base system. In Section 2.2, we defined a well-designed series system as one that consists of components with similar reliabilities, where we define reliability in two ways, the mean time to failure (MTTF) and the probability that a specific component will be the cause of failure. All things else being equal, components with long MTTFs and with near uniform probability of being the component cause of failure is preferrable, otherwise we have a weak link in the system. The base system defined in Table satisfies this definition of being a well-designed system. We see that there are no components that are significantly less reliable than any of the others, component 1 being the most reliable and component 3 being the least reliable. This is a result of the scales and shapes being similar for each component. In addition, the shapes are larger than \\(1\\), which means components are unlikely to fail early. Table 7.1: Weibull Components in Series Configuration Shape (\\(k_j\\)) Scale (\\(\\lambda_j\\)) MTTF\\(_j\\) \\(\\Pr\\{K_i = j\\}\\) \\(R_j(\\tau;k_j,\\lambda_j)\\) Component 1 1.2576 994.3661 924.869 0.169 0.744 Component 2 1.1635 908.9458 862.157 0.207 0.698 Component 3 1.1308 840.1141 803.564 0.234 0.667 Component 4 1.1802 940.1342 888.237 0.196 0.711 Component 5 1.2034 923.1631 867.748 0.195 0.711 Series System NA NA 222.884 NA 0.175 7.1 Data Generating Process In this section, we describe the data generating process for our simulation studies. It consists of three parts: the series system, the candidate set model, and the right-censoring model. Series System Lifetime We generate data from a Weibull series system with \\(m\\) components. As described in Section 6, the \\(j\\) component of the \\(i\\) system has a lifetime distribution given by \\[ T_{i j} \\sim \\operatorname{Weibull}(k_j, \\lambda_j) \\] and the lifetime of the series system composed of \\(m\\) Weibull components is defined as \\[ T_i = \\min\\{T_{i 1}, \\ldots, T_{i m}\\}. \\] To generate a data set, we first generate the \\(m\\) component failure times, by efficiently sampling from their respective distributions, and we then set the failure time \\(t_i\\) of the system to the minimum of the component failure times. Right-Censoring Model We employ a simple right-censoring model, where the right-censoring time \\(\\tau\\) is fixed at some known value, e.g., an experiment is run for a fixed amount of time \\(\\tau\\), and all systems that have not failed by the end of the experiment are right-censored. The censoring time \\(S_i\\) of the \\(i\\) system is thus given by \\[ S_i = \\min\\{T_i, \\tau\\}. \\] So, after we generate the system failure time \\(T_i\\), we generate the censoring time \\(S_i\\) by taking the minimum of \\(T_i\\) and \\(\\tau\\). In our simulation study, we paramaterize the right-censoring time \\(\\tau\\) by the quantile \\(q = 0.825\\) of the series system, \\[ \\tau = F_{T_i}^{-1}(q). \\] This means that \\(82.5\\%\\) of the series systems are expected to fail before time \\(\\tau\\) and \\(17.5\\%\\) of the series are expected to be right-censored. To solve for the \\(82.5\\%\\) quantile of the series system, we define the function \\(g\\) as \\[ g(\\tau) = F_{T_i}(\\tau;\\boldsymbol{\\theta}) - q \\] and find its root using the Newton-Raphson method. See Appendix E for the R code that implements this procedure. Masking Model for Component Cause of Failure We must generate data that satisfies the masking conditions described in Section 3.1. There are many ways to satisfying the masking conditions. We choose the simplest method, which we call the Bernoulli candidate set model. In this model, each non-failed component is included in the candidate set with a fixed probability \\(p\\), independently of all other components and independently of \\(\\boldsymbol{\\theta}\\), and the failed component is always included in the candidate set. See D} for the R code that implements this model. 7.2 Simulation Scenarios We define a simulation scenario to be some combination of \\(n\\) (sample size), \\(p\\) (masking probability in our Bernoulli candidate set model), \\(k_3\\) (shape parameter of the third component), \\(\\lambda_3\\) (scale parameter of the third component), and \\(q\\) (right-censoring quantile). We are interested in choosing a small number of scenarios that are representative of real-world scenarios and that are interesting to analyze. Here is an outline of the simulation study for a particular scenario: Fix a combination of simulation parameters to some value, and vary the remaining parameters. For example, if we want to assess how the sampling distribution of the MLE changes with respect to sample size, we might choose some particular values for \\(p\\), \\(k_3\\), \\(\\lambda_3\\), and \\(q\\), and vary the sample size \\(n\\) over the desired range. Simulate \\(R \\geq 300\\) datasets from the Data Generating Process (DGP) described in Section 7.1 and compute an MLE for each dataset. We choose \\(R\\) to be large enough so that the sampling distribution of the MLE is well approximated by the empirical distribution of the \\(R\\) MLEs. For each of these \\(R\\) MLEs, compute some function of the MLE, like the BCa confidence intervals or the likelihood ratio test statistic. This will give us \\(R\\) statistics as a Monte-carlo estimate of the sampling distribution of the statistic. Use the \\(R\\) statistics to estimate some property of the sampling distribution of the statistic, e.g., the mean of the MLE or the coverage probability of the BCa confidence intervals, with respect to the parameter(s) we are varying in the scenario, e.g., assess how the coverage probability of the BCa confidence intervals changes with respect to sample size. Visualize the results and assess the behavior of estimator under the chosen scenario. For how we run a simulation scenario, see Appendix C. 7.3 Scenario: Assessing the Impact of Right-Censoring In this scenario, we use the well-designed series system described in Table , and we vary the right-censoring quantile (\\(q\\)) from \\(60\\%\\) to \\(100\\%\\) (no right-censoring), with a component cause of failure masking probability of \\(21.5\\%\\) and sample size \\(n = 100\\). Figure 7.1: Right-Censoring Quantile vs MLE (\\(p = 0.215, n = 100\\)) When a right-censoring event occurs, in order to increase the likelihood of the data, the MLE is nudged in a direction that increases the probability of a right-censoring event at time \\(\\tau\\), which is given by \\(R_{T_i}(t;\\boldsymbol{\\theta})\\), representing a source of bias in the estimate. To increase \\(R_{T_i}(\\tau)\\), we move in the direction (gradient) of these partial derivatives. The partial derivatives of \\(R_{T_i}(\\tau)\\) are given by \\[\\begin{align*} \\frac{\\partial R_{T_i}(\\tau)}{\\partial \\lambda_j} &amp;= R_{T_i}(\\tau;\\boldsymbol{\\theta}) \\left(\\frac{\\tau}{\\lambda_j}\\right)^{k_j} \\frac{k_j}{\\lambda_j},\\\\ \\frac{\\partial R_{T_i}(\\tau)}{\\partial k_j} &amp;= R_{T_i}(\\tau;\\boldsymbol{\\theta}) \\left(\\frac{\\tau}{\\lambda_j}\\right)^{k_j} \\left(\\log \\lambda_j - \\log \\tau\\right), \\end{align*}\\] for \\(j = 1, \\ldots, m\\). We see that these partial derivatives are related to the score of a right-censored likelihood contribution in Theorem . Let us analyze these partial derivatives: As \\(\\tau\\) increases, \\(R_{T_i}(\\tau;\\boldsymbol{\\theta})\\) decreases, and so the effect right-censoring has on the MLE decreases. This is what we see in Figure . The partial derivatives with respect to the scale parameters are always positive, so right-censoring positively bias the scale parameter estimates to make right-censoring events more likely. The more right-censoring, the more the positive bias. We see this in Figure , where the bias of the MLE for the scale parameter decreases as we decrease the probability (\\(1-q\\)) of a right-censoring event. The partial derivative with respect to the shape parameter of the \\(j\\) component, \\(k_j\\), is non-negative if \\(\\lambda_j \\geq \\tau\\) and otherwise negative. In our well-designed series system, the scale parameters are large compared to most of the right-censoring times for \\(\\tau(q)\\), so the MLE nudges the shape parameter estimates in a positive direction to increase the probability of a right-censoring event \\(R_{T_i}(\\tau)\\) at time \\(\\tau\\). We see this in Figure , where the shape parameter estimates are positively biased for most of the quantiles \\(q\\). 7.3.1 Key Observations Coverage Probability (CP) The CP is well-calibrated, obtaining a value near the nominal 95% level across different right-censoring quantiles. This suggests that the bootstrapped CIs will contain the true value of the parameters with the specified confidence level. The CIs are neither too wide nor too narrow. Dispersion of MLEs The shaded regions representing the 95% probability range of the MLEs get narrower as the right-censoring quantile increases. This is an indicator of the increased precision in the estimates as more data is available due to decreased censoring. IQR of Bootstrapped CIs The IQR (vertical blue bars) reduces with an increase in sample size. This suggests that the bootstrapped CIs are getting more consistent and focused around a narrower range with larger samples while maintaining a good coverage probability. As we get more data, the bootstrapped CIs are more likely to be closer to each other and the true value of the parameters. For small right-censoring quantiles (small right-censoring times), they are quite large, but to maintain well-calibrated CIs, this was necessary. The estimator is quite sensitive to the data, and so the bootstrapped CIs are quite wide to account for this sensitivity when the sample contains insufficient information due to censoring. Bias of MLEs The red dashed line indicating the mean of MLEs initially is quite biased, but quickly diminshes to neglible levels for scale parameters. The bias for the shape parameters never reach zero, but this is potentially due to masking. At a larger sample size, we anticipate the bias in the shape estimates would also decrease to zero. 7.4 Scenario: Assessing the Impact of Sample Size In this scenario, we use the well-designed series system described in Table . We fix the masking probability to \\(p = 0.215\\) (moderate masking), we fix the right-censoring quantile to \\(q = 0.825\\) (moderate censoring), and we vary the sample size \\(n\\) from \\(50\\) (small sample size) to \\(1000\\) (very large sample size). Figure 7.2: Sample Size vs MLEs (\\(p = 0.215, q = 0.825\\)) In Figure , we show the effect of the sample size \\(n\\) on the MLEs for the shape and scale parameters. The top four plots only show the effect on the MLEs for the shape and scale parameters of components \\(1\\) and \\(4\\), since the rest were essentially identical, and the bottom two plots show the coverage probabilities for all parameters. 7.4.1 Key Observations Coverage Probability (CP) The CP is well-calibrated, obtaining a value near the nominal \\(95\\%\\) level across different sample sizes. This suggests that the bootstrapped CIs will contain the true value of the shape parameter with the specified confidence level. The CIs are neither too wide nor too narrow. Dispersion of MLEs The shaded regions representing the \\(95\\%\\) probability range of the MLEs get narrower as the sample size increases. This is an indicator of the increased precision in the estimates when provided with more data. This is consistent with the asymptotic properties of the MLE when the regularity conditions are satisfied, e.g., converges in probability to the true value of the parameter as \\(n\\) goes to infinity. IQR of Bootstrapped CIs The IQR (vertical blue bars) reduces with an increase in sample size. This suggests that the bootstrapped CIs are getting more consistent and focused around a narrower range with larger samples while maintaining a good coverage probability. As we get more data, the bootstrapped CIs are more likely to be closer to each other and the true value of the scale parameter. For small sample sizes, they are quite large, but to maintain well-calibrated CIs, this was necessary. The estimator is quite sensitive to the data, and so the bootstrapped CIs are quite wide to account for this sensitivity when the sample size is small and not necessarily representative of the true distribution. Bias of MLEs for Scales The red dashed line indicating the mean of MLEs remains stable across different sample sizes and close to the true value, suggesting that the scale MLEs are reasonably unbiased. Bias of MLEs for Shapes The red dashed line is the mean of shape MLEs. Unlike the scale MLEs, we see that for small samples, particularly less than \\(200\\), we observe a significant amount of positive bias for shape MLEs. The MLE for the shape parameters in this scenario appear to be more sensitive to the data than the scale parameters. This scenario successfully illustrates the importance of sample size in estimating parameters. The findings align with statistical theory and provide insights into the behavior of these estimators for different sample sizes. In particular, it highlights the sensitivity of the shape parameter estimates to right-censoring and masking for small sample sizes and the importance of having sufficient data to overcome these effects. 7.5 Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure In this scenario, we use the well-designed series system described in Table . We fix the sample size to \\(n = 90\\) (reasonable sample size) and we fix the right-censoring quantile to \\(q = 0.825\\), and we vary the masking probability from \\(p\\) from \\(0.1\\) (very slight masking the component cause of failure) to \\(0.85\\) (extreme masking of the component cause of failure). In Figure , we show the effect of the masking probability \\(p\\) on the MLE for the shape and scale parameters. The top four plots only show the effect on the MLEs for the the shape and scale parameters of components \\(1\\) and \\(4\\), since the rest were essentially identical, and the bottom two plots show the coverage probabilities for all parameters. Figure 7.3: Component Cause of Failure Masking (\\(p\\)) vs MLE 7.5.1 Key Observations Coverage Probability (CP) For the scale parameters, the \\(95\\%\\) CI is well-calibrated for Bernulli masking probabilities up to \\(p = 0.725\\), which is really quite significant, obtaining coverages over \\(90\\%\\), but drops precipitously after that point. For the shape parameters, the \\(95\\%\\) CI is well-calibrated for masking probabilities only up to \\(p = 0.4\\), which is still large, obtaining coverages generally over \\(90%\\), but begins to drop slowly after that point. The BCa confidence intervals are well-calibrated for most realistic masking probabilities, constructing CIs that are neither too wide nor too narrow, but when the masking is severe and the sample size is small, one should take the CIs with a grain of salt. Dispersion of MLEs The shaded regions representing the \\(95\\%\\) quantile of the MLEs become wider as the masking probability increases. This is an indicator of the decreased precision in the estimates when provided with more ambiguous data about the component cause of failure. However, even for fairly significant Bernoulli masking, \\(p \\leq 0.55\\), the \\(95\\%\\) quantiles are narrow and the CP is well-calibrated, indicating that the MLEs are still precise and accurate. IQR of Bootstrapped CIs The IQR (vertical blue bars) show that the bootstrapped BCa CIs are becoming more spread out as the masking probability increases. They are also asymmetric, with the lower bound being more spread out than the upper bound, but this is consistent with the actual behavior of the dispersion of the MLEs, which exhibits the same pattern. The width of the CIs consistently increase as the masking probability increases, which we intuitively expected given the increased uncertanity about the component cause of failure. After a Bernoulli masking probability of \\(p \\approx 0.5\\), the width of the CIs rapidly increase, which is apparently necessary for the CPs to remain well-calibrated. Bias of MLEs The red dashed line indicating the mean of the MLEs remains stable across different masking probabilities, only showing a significant positive bias when the masking probability \\(p\\) becomes quite significant. 7.6 Scenario: Assessing the Impact of Changing the Scale Parameter of Component 3 By Equation , we see that MTTF\\(_j\\) is proportional to the scale parameter \\(\\lambda_j\\), which means when we decrease the scale parameter of a component, we proportionally decrease the MTTF. In this scenario, we start with the well-designed series system described in Table , and we will manipulate the MTTF of component 3, MTTF\\(_3\\), by changing its scale parameter, \\(\\lambda_3\\), and observing the effect this has on the MLE. Since the other components had a similiar MTTF, we will arbitrarily choose component 1 to represent the other components. The bottom plot shows the coverage probabilities for all parameters. In Figure , we show the effect of changing the scale parameter of component \\(3\\), \\(lambda_3\\), but map \\(\\lambda_3\\) to MTTF\\(_3\\) to make it more intuitive to reason about. We vary the MTTF of component 3 from \\(300\\) to \\(1500\\) and the other components have their MTTFs fixed at around \\(900\\), as shown in Table . We fix the masking probability to \\(p = 0.215\\) (moderate masking), the right-censoring quantile to \\(q = 0.825\\) (moderate censoring), and the sample size to \\(n = 100\\) (moderate sample size). Figure 7.4: MTTF\\(_3\\) vs MLE By Varying Scale 7.6.1 Key Observations Coverage Probability (CP) When MTTF of component 3 is much smaller than other components, the CP for \\(k_3\\) is very well calibrated (approximately obtaining the nominal level \\(95\\%\\)) while the CP for other componentns are around \\(90\\%\\), which is still reasonable. (This is the case even though the width of the CI for \\(k_3\\) is extremely narrow compared to the others). As MTTF\\(_3\\) increases, the CP for \\(k_3\\) decreases, while the CP for the other components increase slightly. The scale parameters are generally well-calibrated for all of the components, except for component 3 when its MTTF is large and it dips down to \\(90\\%\\). Despite the individual differences, the mean of the CPs for shape and scale parameters hardly change. Dispersion of MLEs For component 3, as its MTTF decreases, the dispersion of MLEs narrows, indicating more precise estimates. Conversely, dispersion for other components widens. As MTTF of component 3 increases, its dispersion widens while others narrow. This is consistent with the fact that the smaller MTTF of component 3 means that, in this well-designed system at least, it is more likely to be the component cause of failure, and so we have more information about its parameters and are able to estimate them more accurately. IQR of Bootstrapped CIs The dark blue vertical lines representing IQR are consistent with the dispersion of MLEs, which is the ideal behavior, and suggests that the BCa confidence intervals are performing well. Bias of MLEs For component 3, the bias of MLE for the scale parameter becomes slightly more negatively biased as MTTF\\(_3\\) increases, and the bias of the MLE for the shape parameter becomes slightly more positively biased. The MLE for the shape and scale parameters for component 1 have a very small bias, if any, and are not affected by the MTTF\\(_3\\). The scale parameters are easier to estimate than the shape parameters, and so they are less sensitive to changes in scale than the shape parameters, as we will show in the next scenario. 7.7 Scenario: Assessing the Impact of Changing the Shape Parameter of Component 3 The shape parameter determines the failure characteristics. We vary the shape paramenter of component 3 from \\(0.1\\) to \\(3.5\\) and observe the effect it has on the MLE. When \\(k_3 &lt; 1\\), this indicates infant mortality, and when \\(k_3 &gt; 1\\), this indicates wear-out failures. We analyze the effect of component 3’s shape parameter on the MLE and the bootstrapped confidence intervals for the shape and scale parameters of components 1 and 3 (the component we are varying). First, we look at the effect on the scale parameter. Figure 7.5: Probability of Component 3 Failure vs MLE 7.7.1 Key Observations Coverage Probability (CP) The CP for the scale parameters are well-calibrated and close to the nominal level of \\(0.95\\) for all values of \\(\\Pr\\{K_i = 3\\}\\). For the the shape parameter of component 3 (\\(k_3\\)) in bold orange colors, we see that it is well-calibrated for all values of \\(\\Pr\\{K_i = 3\\}\\), but actually may become too large for extreme values of \\(\\Pr\\{K_i = 3\\}\\). The CP for the shape parameters of the other components decreases with $\\(\\Pr\\{K_i = 3\\}\\), dipping below \\(90\\%\\) for \\(\\Pr\\{K_i = 3\\} &gt; 0.4\\). At a sample size of \\(n = 100\\), the CP for the shape parameters of the other components is generally not well-calibrated for \\(\\Pr\\{K_i = 3\\} &gt; 0.4\\). Dispersion of MLEs The dispersion of the MLE for the shape and scale parameters of component 1, \\(k_1\\) and \\(\\lambda_1\\), is fairly steady but begins to increase rapdily at the extreme values of \\(\\Pr\\{K_i = 3\\}\\). This is indicative of having less information about the failure characteristcs of component \\(1\\) as component \\(3\\) begins to dominate the component cause of failure. The dispersion of the shape parameter \\(k_3\\) is initially quite large, indicative of having very little information about the failure characteristcs of component 3 since it is unlikely to be the component cause of failure, but its dispersion rapidly decreases as \\(\\Pr\\{K_i = 3\\}\\) increases and more information is available about component 3’s failure characteristics. In fact, it nearly becomes a point at \\(\\Pr\\{K_i = 3\\} = 0.6\\). The dispersion of the the scale parameter of component \\(3\\), \\(\\lambda_1\\), is quite steady and is less spread out than the MLE for \\(\\lambda_1\\), but at extreme values of \\(\\Pr\\{K_i = 3\\}\\), it also begins to rapidly increase, suggesting some complex interactions between the shape and scale parameters of component 3. IQR of Bootstrapped CIs The CIs precisely track the dispersion of the MLEs, which is the ideal behavior, and suggests that the BCa confidence intervals are performing well. Bias of MLEs The MLE for the scale parameters are nearly unbiased and generally seem unaffected by changes in \\(\\Pr\\{K_i = 3\\}\\). As \\(\\Pr\\{K_i = 3\\}\\) increases the MLE is adjusting \\(k_1\\) to be more positively biased, decreasing its infant morality rate to make it less likely to be the component cause of failure, and adjusting \\(k_3\\) to be less positively biased, increasing its infant mortality rate, to make it more likely to be the component cause of failure. References "],["future-work.html", "8 Future Work", " 8 Future Work Relaxation of Masking Conditions Investigate relaxations of Conditions 1, 2, and 3. Condition 1 stipulates that the failed component is always in the candidate set, \\[ \\Pr\\{K_i \\in \\mathcal{C}_i\\} = 1. \\] Instead, we could model this as a probability, where the probability of the failed component being in the candidate set is a function of the failure time \\(T_i\\) and the component cause of failure \\(K_i\\), \\[ \\Pr\\{K_i \\in \\mathcal{C}_i | K_i = j, T_i = t_i\\} = g(j, t_i). \\] Condition 2 stipulates that \\[ \\Pr\\{\\mathcal{C}_i = c_i | T_i = t_i, K_i = j\\} = \\Pr\\{\\mathcal{C}_i = c_i | T_i = t_i, K_i = j&#39;\\} \\] for all \\(j, j&#39; \\in c_i\\). We call this an uninformed candidate set, since the conditional probability of the candidate set given the failure time and component cause of failure is independent of the component cause of failure. We could relax this condition to allow for informed candidate sets, where the conditional probability of the candidate set given the failure time and component cause of failure is dependent on the component cause of failure. In each of these violations or relaxations, we can either construct a new likelihood model that takes this relaxation into account, or we can use the existing likelihood model and assess the sensitivity of the estimator to this violation. A potentially interesting way to do the lattr is by using KL-divergence to measure the distance between, for instance, the uninformed and informed candidate set models, and then assess the sensitivity of the estimators to this distance. Expanded Sensitivity Analyses Explore the use of a reduced model that assumes homogeneity in shape parameters. This approach would simplify the system to \\(m+1\\) parameters, as opposed to the \\(2m\\) parameters in the current model, potentially increasing interpretability and reducing estimator variability. This direction warrants detailed investigation to ensure the reduced model retains sufficient accuracy and adequately describes the data. Semi-Parametric Bootstrap We used the non-parameteric bootstrap to construct confidence intervals, but we could also use the semi-parametric bootstrap. In the semi-parametric bootstrap, instead of resampling from the original data, we sample component lifetimes from the parametric distribution fitted to the original data and sample candidate sets from the empirical distribution of the conditional candidate sets in the original data. This is a compromise between the non-parametric bootstrap and the fully parametric bootstrap.[^30] [^30]: The fully parametric bootstrap is not appropriate for our likelihood model because we do not assume a parametric form for the distribution of the candidate sets. Data Augmentation Assess the robustness of Data Augmentation (DA) as an implicit prior. For example, we may adopt the prior that the system is well-designed and augment particularly small samples with synthetic data from a reduced model (with homogenous shape parameters) fitted to the original data. Unlike a full Bayesian approach, where we would need to specify a prior for the parameters, DA is an implicit prior that need not be explicitly specified. It is a form of regularization that reduces the variance of the estimator by leveraging the structure of the model and the data. Penalized Likelihood For Homogenous Shape Parameters Assess the use of penalized likelihood methods instead of DA as a form of regularization. For instance, we can add a penalty term to the log-likelihood function that penalizes the likelihood when the shape parameters are not close to each other. Instead of using a reduced model, we can use a penalized likelihood approach to encourage the shape parameters to be close to each other, but not necessarily equal. General Likelihood Model with Predictors In this paper, we focused on a likelihood model that assumed Weibull components in a series configuration. We can extend this model by generalizing the hazard functions in two ways: Let the hazard model be a function of predictors \\(\\boldsymbol{w_1}, \\ldots, \\boldsymbol{w_n}\\), where \\(\\boldsymbol{w_i}\\) is a vector of predictors for the \\(i\\)th observation. Then, the hazard function for the \\(j\\)th component is \\[ h_j(t_i|\\boldsymbol{w_i};\\boldsymbol{\\beta_j}), \\] for instance we might make the shape and scale parameters of the Weibull component model be a function of the predictors \\(\\boldsymbol{w_i}\\). Replace the Weibull hazard function with a more general hazard function. For instance, in the Cox proportional hazards model (Cox 1972), the hazard function for the \\(j\\)th component is given by \\[ h_j(t_i|\\boldsymbol{w_i};\\boldsymbol{\\beta_j}) = h_0(t_i) \\exp(\\boldsymbol{\\beta_j}^T \\boldsymbol{w_i}), \\] where \\(h_{0}(t_i)\\) is a baseline hazard function shared by all components and \\(\\boldsymbol{\\beta_j}\\) is the parameter vector for the \\(j\\)th component. A more general model would allow the component hazard functions to take any valid form, namely non-negative and integrable. In either case, by the relation \\[ R_j(t_i|\\boldsymbol{w_i};\\boldsymbol{\\beta_j}) = e^{-H_j(t)}, \\] where \\[ H_j(t_i) = \\int_0^{t_i} h_j(u|\\boldsymbol{w_i};\\boldsymbol{\\beta_j}) du \\] is the cumulative hazard function for the \\(j\\) component, we can plug these component hazard and reliability functions into the likelihood contribution model in Theorem to obtain a general likelihood model with predictors for the series system. Assess the Calibration of Other Related Bootstrapped Statistics The calibration of the bootstrapped confidence intervals were evaluated and shown to be quite robust. We could do a similar analysis for other bootstrapped statistics. For instance, we could assess the bootstrapped \\(95\\%\\) prediction interval for the probability that component \\(j\\) is the component cause of the next system failure given the data \\(\\mathcal{D}_n\\), \\[ \\Pr\\{K_{n+1} = j | \\mathcal{D}_n\\}. \\] References "],["conclusion.html", "9 Conclusion", " 9 Conclusion This paper presented maximum likelihood methods for estimating component reliability from masked failure data in series systems. We accounted for right-censoring and masking issues in our likelihood model, thereby achieving a rigorous framework for reliability analysis from limited observational data. Our simulation studies reveal that shape parameters are particularly sensitive to data and harder to estimate precisely compared to scale parameters. Right-censoring and masking were found to bias shape parameters positively, although scale parameters exhibited more robust behavior. Despite these challenges, bootstrapping techniques yielded well-calibrated confidence intervals even for small sample sizes. The sensitivities in shape parameters lead to a need for caution, especially when dealing with small sample sizes. Coverage probabilities for shape parameters, for instance, drop below \\(90\\%\\) in the presence of significant masking. On the other hand, scale parameters display more robust properties, maintaining well-calibrated confidence intervals even under challenging conditions. Overall, this work provides a rigorous framework for quantifying component reliability from limited observational data. The methods demonstrated accurate and robust performance despite the significant challenges introduced by masking and right-censoring. In light of our findings, to continue to refine our understanding and broaden the applicability of these methods, future work can focus on relaxing modeling assumptions, expanding the sensitivity analysis, and evaluating the viability of methods designed to reduce the variability of shape parameter estimates. "],["app-weibull-loglik-r.html", "A Log-likelihood Function", " A Log-likelihood Function The following R code implements the log-likelihood function for the Weibull series system with right censoring and masking of component failure data. It is implemented in the R library wei.series.md.c1.c2.c3 and is available on GitHub. For clarity and brevity, we removed some of the functionality that is not relevant to the analysis in this paper and produce a simplified version of the code below. #&#39; Generates a log-likelihood function for a Weibull series system with respect #&#39; to parameter `theta` (shape, scale) for masked data with candidate sets #&#39; that satisfy conditions C1, C2, and C3 and right-censored data. #&#39; #&#39; @param df (masked) data frame #&#39; @param theta parameter vector (shape1, scale1, ..., shapem, scalem) #&#39; @returns Log-likelihood with respect to `theta` given `df` loglik_wei_series_md_c1_c2_c3 &lt;- function(df, theta) { n &lt;- nrow(df) C &lt;- md_decode_matrix(df, candset) m &lt;- ncol(C) delta &lt;- df[[right_censoring_indicator]] t &lt;- df[[lifetime]] k &lt;- length(theta) shapes &lt;- theta[seq(1, k, 2)] scales &lt;- theta[seq(2, k, 2)] s &lt;- 0 for (i in 1:n) { s &lt;- s - sum((t[i] / scales)^shapes) if (delta[i]) { s &lt;- s + log(sum(shapes[C[i, ]] / scales[C[i, ]] * (t[i] / scales[C[i, ]])^(shapes[C[i, ]] - 1))) } } return(s) } "],["app-score-fn-r.html", "B Score Function", " B Score Function The following code is the score function (gradient of the log-likelihood function with respect to \\(\\boldsymbol{\\theta}\\)) for the Weibull series system with a likelihood model that includes masked component cause of failure and right-censoring. It is implemented in the R library wei.series.md.c1.c2.c3 and is available on GitHub. For clarity and brevity, we removed some of the functionality that is not relevant to the analysis in this paper. #&#39; Computes the score function (gradient of the log-likelihood function) for a #&#39; Weibull series system with respect to parameter `theta` (shape, scale) for masked #&#39; data with candidate sets that satisfy conditions C1, C2, and C3 and right-censored #&#39; data. #&#39; #&#39; @param df (masked) data frame #&#39; @param theta parameter vector (shape1, scale1, ..., shapem, scalem) #&#39; @returns Score with respect to `theta` given `df` score_wei_series_md_c1_c2_c3 &lt;- function(df, theta) { n &lt;- nrow(df) C &lt;- md_decode_matrix(df, candset) m &lt;- ncol(C) delta &lt;- df[[right_censoring_indicator]] t &lt;- df[[lifetime]] shapes &lt;- theta[seq(1, length(theta), 2)] scales &lt;- theta[seq(2, length(theta), 2)] shape_scores &lt;- rep(0, m) scale_scores &lt;- rep(0, m) for (i in 1:n) { rt.term.shapes &lt;- -(t[i] / scales)^shapes * log(t[i] / scales) rt.term.scales &lt;- (shapes / scales) * (t[i] / scales)^shapes # Initialize mask terms to 0 mask.term.shapes &lt;- rep(0, m) mask.term.scales &lt;- rep(0, m) if (delta[i]) { cindex &lt;- C[i, ] denom &lt;- sum(shapes[cindex] / scales[cindex] * (t[i] / scales[cindex])^(shapes[cindex] - 1)) numer.shapes &lt;- 1 / t[i] * (t[i] / scales[cindex])^shapes[cindex] * (1 + shapes[cindex] * log(t[i] / scales[cindex])) mask.term.shapes[cindex] &lt;- numer.shapes / denom numer.scales &lt;- (shapes[cindex] / scales[cindex])^2 * (t[i] / scales[cindex])^(shapes[cindex] - 1) mask.term.scales[cindex] &lt;- numer.scales / denom } shape_scores &lt;- shape_scores + rt.term.shapes + mask.term.shapes scale_scores &lt;- scale_scores + rt.term.scales - mask.term.scales } scr &lt;- rep(0, length(theta)) scr[seq(1, length(theta), 2)] &lt;- shape_scores scr[seq(2, length(theta), 2)] &lt;- scale_scores return(scr) } "],["app-sim-study-r.html", "C Scenario Simulation", " C Scenario Simulation The following R code is the Monte-carlo simulation code for running the various scenarios described in Section 7. #### Setup simulation parameters here #### theta &lt;- c( shape1 = 1.2576, scale1 = 994.3661, shape2 = 1.1635, scale2 = 908.9458, shape3 = NA, scale3 = 840.1141, shape4 = 1.1802, scale4 = 940.1342, shape5 = 1.2034, scale5 = 923.1631 ) shapes3 &lt;- c(1.1308) # shape 3 true parameter values to simulate scales3 &lt;- c(840.1141) # scale 3 true parameter values to simulate N &lt;- c(30, 60, 100) # sample sizes to simulate P &lt;- c(.215) # masking probabilities to simulate Q &lt;- c(.825) # right censoring probabilities to simulate R &lt;- 1000L # number of simulations per scenario B &lt;- 1000L # number of bootstrap samples max_iter &lt;- 125L # max iterations for MLE max_boot_iter &lt;- 125L # max iterations for bootstrap MLE n_cores &lt;- detectCores() - 1 # number of cores to use for parallel processing filename &lt;- &quot;data&quot; # filename prefix for output files #### Simulation code below here #### library(tidyverse) library(parallel) library(boot) library(algebraic.mle) # for `mle_boot` library(wei.series.md.c1.c2.c3) # for `mle_lbfgsb_wei_series_md_c1_c2_c3` etc file.meta &lt;- paste0(filename, &quot;.txt&quot;) file.csv &lt;- paste0(filename, &quot;.csv&quot;) if (file.exists(file.meta)) { stop(&quot;File already exists: &quot;, file.meta) } if (file.exists(file.csv)) { stop(&quot;File already exists: &quot;, file.csv) } shapes &lt;- theta[seq(1, length(theta), 2)] scales &lt;- theta[seq(2, length(theta), 2)] m &lt;- length(shapes) sink(file.meta) cat(&quot;boostrap of confidence intervals:\\n&quot;) cat(&quot; simulated on: &quot;, Sys.time(), &quot;\\n&quot;) cat(&quot; type: &quot;, ci_method, &quot;\\n&quot;) cat(&quot;weibull series system:\\n&quot;) cat(&quot; number of components: &quot;, m, &quot;\\n&quot;) cat(&quot; scale parameters: &quot;, scales, &quot;\\n&quot;) cat(&quot; shape parameters: &quot;, shapes, &quot;\\n&quot;) cat(&quot;simulation parameters:\\n&quot;) cat(&quot; shapes3: &quot;, shapes3, &quot;\\n&quot;) cat(&quot; scales3: &quot;, scales3, &quot;\\n&quot;) cat(&quot; N: &quot;, N, &quot;\\n&quot;) cat(&quot; P: &quot;, P, &quot;\\n&quot;) cat(&quot; Q: &quot;, Q, &quot;\\n&quot;) cat(&quot; R: &quot;, R, &quot;\\n&quot;) cat(&quot; B: &quot;, B, &quot;\\n&quot;) cat(&quot; max_iter: &quot;, max_iter, &quot;\\n&quot;) cat(&quot; max_boot_iter: &quot;, max_boot_iter, &quot;\\n&quot;) cat(&quot; n_cores: &quot;, n_cores, &quot;\\n&quot;) sink() for (scale3 in scales3) { for (shape3 in shapes3) { for (n in N) { for (p in P) { for (q in Q) { shapes[3] &lt;- shape3 theta[&quot;shape3&quot;] &lt;- shape3 cat(&quot;[starting scenario: scale3 = &quot;, scale3, &quot;, shape3 = &quot;, shape3, &quot;, n = &quot;, n, &quot;, p = &quot;, p, &quot;, q = &quot;, q, &quot;]\\n&quot;) tau &lt;- qwei_series(p = q, scales = scales, shapes = shapes) # we compute R MLEs for each scenario shapes.mle &lt;- matrix(NA, nrow = R, ncol = m) scales.mle &lt;- matrix(NA, nrow = R, ncol = m) shapes.lower &lt;- matrix(NA, nrow = R, ncol = m) shapes.upper &lt;- matrix(NA, nrow = R, ncol = m) scales.lower &lt;- matrix(NA, nrow = R, ncol = m) scales.upper &lt;- matrix(NA, nrow = R, ncol = m) iter &lt;- 0L repeat { retry &lt;- FALSE tryCatch( { repeat { df &lt;- generate_guo_weibull_table_2_data( shapes = shapes, scales = scales, n = n, p = p, tau = tau ) sol &lt;- mle_lbfgsb_wei_series_md_c1_c2_c3( theta0 = theta, df = df, hessian = FALSE, control = list(maxit = max_iter, parscale = theta) ) if (sol$convergence == 0) { break } cat(&quot;[&quot;, iter, &quot;] MLE did not converge, retrying.\\n&quot;) } mle_solver &lt;- function(df, i) { mle_lbfgsb_wei_series_md_c1_c2_c3( theta0 = sol$par, df = df[i, ], hessian = FALSE, control = list(maxit = max_boot_iter, parscale = sol$par) )$par } # do the non-parametric bootstrap sol.boot &lt;- boot(df, mle_solver, R = B, parallel = &quot;multicore&quot;, ncpus = n_cores) }, error = function(e) { cat(&quot;[error] &quot;, conditionMessage(e), &quot;\\n&quot;) cat(&quot;[retrying scenario: n = &quot;, n, &quot;, p = &quot;, p, &quot;, q = &quot;, q, &quot;\\n&quot;) retry &lt;&lt;- TRUE } ) if (retry) { next } iter &lt;- iter + 1L shapes.mle[iter, ] &lt;- sol$par[seq(1, length(theta), 2)] scales.mle[iter, ] &lt;- sol$par[seq(2, length(theta), 2)] tryCatch( { ci &lt;- confint(mle_boot(sol.boot), type = ci_method, level = ci_level) shapes.ci &lt;- ci[seq(1, length(theta), 2), ] scales.ci &lt;- ci[seq(2, length(theta), 2), ] shapes.lower[iter, ] &lt;- shapes.ci[, 1] shapes.upper[iter, ] &lt;- shapes.ci[, 2] scales.lower[iter, ] &lt;- scales.ci[, 1] scales.upper[iter, ] &lt;- scales.ci[, 2] }, error = function(e) { cat(&quot;[error] &quot;, conditionMessage(e), &quot;\\n&quot;) } ) if (iter %% 5 == 0) { cat(&quot;[iteration &quot;, iter, &quot;] shapes = &quot;, shapes.mle[iter, ], &quot;scales = &quot;, scales.mle[iter, ], &quot;\\n&quot;) } if (iter == R) { break } } df &lt;- data.frame( n = rep(n, R), rep(scale3, R), rep(shape3, R), p = rep(p, R), q = rep(q, R), tau = rep(tau, R), B = rep(B, R), shapes = shapes.mle, scales = scales.mle, shapes.lower = shapes.lower, shapes.upper = shapes.upper, scales.lower = scales.lower, scales.upper = scales.upper ) write.table(df, file = file.csv, sep = &quot;,&quot;, row.names = FALSE, col.names = !file.exists(file.csv), append = TRUE ) } } } } } "],["app-cand-model-r.html", "D Bernoulli Candidate Set Model", " D Bernoulli Candidate Set Model #&#39; Bernoulli candidate set model is a particular type of *uninformed* model. #&#39; This model satisfies conditions C1, C2, and C3. #&#39; The failed component will be in the corresponding candidate set with #&#39; probability 1, and the remaining components will be in the candidate set #&#39; with probability `p` (the same probability for each component). `p` #&#39; may be different for each system, but it is assumed to be the same for #&#39; each component within a system, so `p` can be a vector such that the #&#39; length of `p` is the number of systems in the data set (with recycling #&#39; if necessary). #&#39; #&#39; @param df masked data. #&#39; @param p a vector of probabilities (p[j] is the probability that the jth #&#39; system will include a non-failed component in its candidate set, #&#39; assuming the jth system is not right-censored). md_bernoulli_cand_c1_c2_c3 &lt;- function(df, p) { n &lt;- nrow(df) p &lt;- rep(p, length.out = n) Tm &lt;- md_decode_matrix(df, comp) m &lt;- ncol(Tm) Q &lt;- matrix(p, nrow = n, ncol = m) Q[cbind(1:n, apply(Tm, 1, which.min))] &lt;- 1 Q[!df[[right_censoring_indicator]], ] &lt;- 0 df %&gt;% bind_cols(md_encode_matrix(Q, prob)) } "],["app-series-quantile.html", "E Series System Quantile Function", " E Series System Quantile Function #&#39; Quantile function (inverse of the cdf). #&#39; By definition, the quantile `p` * 100% for a strictly monotonically increasing #&#39; cdf `F` is the value `t` that satisfies `F(t) - p = 0`. #&#39; We solve for `t` using newton&#39;s method. #&#39; #&#39; @param p vector of probabilities. #&#39; @param shapes vector of weibull shape parameters for weibull lifetime #&#39; components #&#39; @param scales vector of weibull scale parameters for weibull lifetime #&#39; components qwei_series &lt;- function(p, shapes, scales) { t0 &lt;- 1 repeat { t1 &lt;- t0 - sum((t0 / scales)^shapes) + log(1 - p) / sum(shapes * t0^(shapes - 1) / scales^shapes) if (abs(t1 - t0) &lt; tol) { break } t0 &lt;- t1 } return(t1) } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
