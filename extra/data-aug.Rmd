### Identifiability {-}
When estimating the parameters, it may sometimes be the case that the likelihood
function is not maximized at a unique point. This is known as the *identifiability*
problem. If the likelihood function is not maximized at a unique point, then a lot of the
theory we have developed so far breaks down \citep{mclachlan2007algorithm}.

In our case, since we are estimating the parameters of latent components,
identifiability is not guaranteed. We consider two examples where this
might occur, but there are many more.

1. The candidate sets could have been constructed in a way that
   prevents us from distinguishing between some of the components. For example,
   if the candidate sets in a sample have the characteristic that
   component $1$ is in a candidate set if and only if component $2$ is in
   a candidate set, then we do not have enough information to estimate the
   parameters of component $1$ and component $2$ separately. This could
   happen, for instance, if the failure analysis is done by a human, and
   he or she is only able to identity that a larger component failed, but
   not which of the smaller components inside it had failed. In this case,
   we may want to combine the two components into one component, and estimate
   the parameters of the combined component.

   In our Bernoulli candidate set model, this is something that can arise
   only by chance, and is unlikely to occur in practice, particularly for
   reasonably large samples.

2. The series system has a component that is the
least reliable by a significant margin and is the component cause
of failure for every system in the sample. In this case, our data may only
contain information about the least reliable component's parameters.

```{r flat-loglike-prof, echo = F, fig.align = "center", fig.cap="Log-likelihood Profile vs Shape Parameter for Component 1 ($k_1$): Non-unique MLE vs Unique MLE.\nBlue Dashed Line is the True Shape $k_1$."}
knitr::include_graphics("image/test-flat-likelihood.pdf")
```

We constructed a quick experiment to demonstrate (2) above. In this
experiment, we performed the following steps:

1. We use the the series system from [@Huairu-2013] as a base, but tweak it
slightly to design the MTTF of the last component (component 3) be be two
orders of magnitude smaller than the others. We did this by changing its
scale parameter to $\lambda_3 = 4.1141$.

2. Generated a data set of size $n = 30$ from this system with a right-censoring time
of $\tau = 6.706782$, corresponding to the $82.5\%$ quantile of the system's
lifetime, and with a masking probability $p = 0.215$ using the Bernoulli
candidate set model.

3. We found an MLE by maximizing the log-likelihood function
   with the data set generated in step 2. 
   As shown in the left plot in Figure \ref{fig:flat-loglike-prof}, the log-likelihood
   function is flat, and therefore there there is no unique MLE. It appears any
   value of $\hat k_1$ larger than $1.5$ will maximize the log-likelihood function.

   For a possible reason, we see that *every* candidate set in the data set contains
   component 3 because it was the component cause of failure in every system failure
   due to its significantly shorter MTTF. 

4. We tweaked the data set in step 2 by removing component 3 from the candidate set
in the first observation and adding component 1. This is permissible because the
observation was not right-censored, and so must have a non-empty candidate set.
This is a very small change to the data set, but as shown in the right plot in
Figure \ref{fig:flat-loglike-prof}, the log-likelihood function is no longer flat,
and there is a unique MLE. We also see that it is a much better estimate of the
true parameter value for $k_1$, although we did not do the analysis to assess whether
this generally holds.

However, by introducing this modification, we not only made the likelihood function
identifiable, but we also nudged the likelihood to peak at a smaller value to make
the shape parameter of the first component more likely to produce earlier failures.

See \hyperref[app:flat-like-code]{Appendix D} for the R code used to generate the data
set for this experiment.

According to this experiment, one could potentially justify either excluding these data
sets from the analysis, or tweak them slightly as we had done, since otherwise they
do not provide a unique MLE. In our simulation study, we mitgated these issues by
choosing parameter values that are representative of real-world systems where there is
no single component that is much less reliable than the others. We also use the Bernoulli
candidate set model, which is unlikely to produce candidate sets that are not informative
enough to estimate the parameters of the components, unless of course the masking
probability $p$ is very large.

After taking these precautions, we largely ignored identifiability issues in our simulation
study, with the exception that we discarded any data sets that did not converge to a solution
after 150 iterations.\footnote{The choice of $150$ iterations was driven by the computational demands of the
simulation study combined with the subsequent bootstrapping of the confidence intervals.
} A log-likelihood function that is flat can cause our convergence
criteria to take a long time to reach a solution. Therefore, a failure to converge within
150 iterations could be seen as evidence of potential identifiability issues.

Nonetheless, such scenarios occurred infrequently. During the bootstrapping of
confidence intervals, we included all MLEs, even those that did not converge. This worst-case analysis
approach was adopted because our main objective was to assess the performance of the BCa confidence intervals.
We were concerned that if we took any additional steps, we may unintentionally
bias the results in favor of producing narrow BCa confidence intervals with good coverage
probabilities.


### Data Augmentation {-}

The profile of the likelihood function in Figure \ref{fig:flat-loglike-prof} has a large flat region
the obtains the maximum, and thus any value in that region -- any value greater than $1.5$ -- is an MLE.
There just was not enough information to identify a unique point, which would ideally be a point closer to the true
value $k_1 = 1.26$.

If we augment the sample with additional data, then we can in theory accomplish two things:

1. We can make the likelihood function identifiable, and thus make the MLE unique. This may or may
   not have any particular advantages by itelf.

2. We can bias the MLE towards particular values or constraints, such as the prior belief that
   the components are approximately equally reliable.\footnote{We could also add constraints or
   regularization to the optimization problem to accomplish a similar goal, but we do not
   investigate this approach in this paper.}

If we have the prior belief that it is more likely that the components within a series system
are approximately of equal reliability, we can bias the MLE towards identical shape and scale
parameters by augmenting samples with additional data. We propose the following procedure to
augment samples with data that is compatible with this prior belief.
For each component, perform the following steps:

1. Resample $R$ system lifetimes from the original sample.
2. Add a bit of noise to each of the $R$ resampled system lifetimes, to make them slightly
   different from the original system lifetime and prevent identifiability issues for occuring.
3. Assign the component as the cause of the $R$ resampled and slightly altered system lifetimes.

As $R$ increases, the stronger the bias towards identical shape and scale parameters.
To just ensure an identifiable likelihood function, we can let $R = 1$, but depending
on the design of the system and the sample size, even this can have a significant effect
on the MLE. If the prior belief of nearly equal reliability among the components is reasonable,
these augmented samples might yield more precise and accurate estimates, otherwise
the augmentation may have an adverse effect.

See \hyperref[app:augment-code]{Appendix E} for the R code that may be used to augment the data.


# Appendix E: Data Augmentation {-}
```{r, eval=FALSE}
equal_reliability_augmentation <- function(df, R = 1) {

    df.censored <- df[df$delta == FALSE, ]
    df <- df[df$delta == TRUE, ]
    C <- md.tools::md_decode_matrix(df, "x")
    m <- ncol(C)

    for (k in 1:m) {
        obs <- df[sample(nrow(df), R, replace = TRUE), ]
        obs[, paste0("x", 1:m)] <- FALSE
        obs[, paste0("x", k)] <- TRUE
        df <- rbind(df, obs)
    }
    rbind(df, df.censored)
}
```



# Appendix D: Flat Log-likelihood Experiment {-}
\label{app:flat-like_code}
Here is the code to generate the data set for Figure \ref{fig:flat-loglike-prof}.

```{r flat-likelihood-code, eval=FALSE}
library(wei.series.md.c1.c2.c3)

# changed scale3 to make MTTF of component 3 much smaller than the others
theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 4.1141) 
shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]

n <- 30
p <- .215
q <- .825
set.seed(151234)
tau <- qwei_series(p = q, scales = scales, shapes = shapes)
df <- generate_guo_weibull_table_2_data(
    shapes = shapes, scales = scales, n = n, p = p, tau = tau)
df.tweaked <- df
df.tweaked[1,"x3"] <- FALSE # remove component 3 from first observation
df.tweaked[1,"x1"] <- TRUE  # add component 1 to first observation

sol <- mle_lbfgsb_wei_series_md_c1_c2_c3(
    theta0 = theta, df = df, hessian = FALSE,
    control = list(maxit = 1000))
sol.tweaked <- mle_lbfgsb_wei_series_md_c1_c2_c3(
    theta0 = theta, df = df.tweaked, hessian = FALSE,
    control = list(maxit = 1000))
l <- Vectorize(function(shape1) {
    theta <- sol$par
    theta[1] <- shape1
    loglik_wei_series_md_c1_c2_c3(df, theta)
}, vectorize.args = "shape1")
l.tweaked <- Vectorize(function(shape1) {
    theta <- sol.tweaked$par
    theta[1] <- shape1
    loglik_wei_series_md_c1_c2_c3(df.tweaked, theta)
}, vectorize.args = "shape1")
```



```{r flat-loglike-prof, out.width="60%", echo = F, fig.align = "center", fig.cap="Log-likelihood profile vs shape parameter for component 1 ($k_1$). The log-likelihood function is flat and any $k_1 > 1.5$ is a maximum, resulting in a non-unique MLE. The blue dashed line is the true shape $k_1$."}
knitr::include_graphics("image/flat-likelihood.pdf")
```


### Identifiability {-}
Identifiability refers to the uniqueness of the point at which the likelihood
is maximized. If not unique, some of the theory we have developed may break down
\citep{mclachlan2007algorithm}.

In our context, estimating latent components' parameters, identifiability
is not always guaranteed. Two examples include:

1. **Candidate Sets Construction**: The candidate sets are constructed such
   that component $1$ is present if and only if component $2$ is present,
   it becomes impossible to estimate their parameters separately. This may
   occur if an analyst can only identify a larger failed component without
   specifying the smaller components within it.\footnote{In this case,
   we may want to combine the components into a single component and estimate
   the parameters of the reduced system.}
   
   In our Bernoulli candidate set model, this is something that can arise only
   by chance. In Section \ref{sec:effect-masking}, we explore this issue by
   assessing the effect of varying the masking probability $p$ in the Bernoulli
   candidate set model on the MLE.

2. **Least Reliable Component**: If a series system has a significantly less
   reliable component that causes every system failure, the data may only
   contain information about that component's parameters.

   We explore this issue in Section \ref{sec:effect-reliability} by assessing
   the effect of varying the reliability of a single component on the MLE.

3. **Aggressive Right-Censoring**: If the right-censoring time $\tau$ is
   too short, the data may not contain enough information to estimate the
   parameters of any of the components.

   We do not explore this issue, but it is something to keep in mind when
   designing experiments. We do comment on the expected effect of right-censoring
   on the MLE in Section \ref{sec:effect-censoring}.

We constructed a quick experiment to demonstrate (2) above. In this
experiment, we performed the following steps:

1. We use the the series system from @Huairu-2013 as a base, but tweak it
slightly to specify the MTTF of the last component (component 3) to be two
orders of magnitude smaller than the others.

2. We generated a data set of size $n = 30$ from this system with a
right-censoring time of $\tau = 6.706782$ and with a masking probability
$p = 0.215$ using the Bernoulli candidate set model.

3. We found an MLE by maximizing the log-likelihood function with the data
set generated in step 2. 
   
As shown in Figure \ref{fig:flat-loglike-prof}, the log-likelihood
function is flat, and therefore there there is no unique MLE. It appears any
value of $\hat k_1$ larger than $1.5$ maximizes the log-likelihood function.
The true value $k_1$ is around $1.26$, so an MLE of $1.5$ is not too far off, but
we do not have enough information to identify that particular point as the MLE
over any other point in the flat region.

