---
title: "Estimating the reliability of components in a series systems from masked failure data"
author: "Alex Towell"
abstract: "We estimate the reliability of series systems from masked
failure data consisting of right-censored system lifetimes and masked
component cause of failures. Under a minimal set of conditions that permit
us to ignore how the component cause of failures are masked, we estimate the
sampling distribution of the MLE in a variety of situations. We find that as
long as the masking does not always include a particular component as a
potential cause of failure whenever another component is also a potential
cause, the MLE is unique and consistent. However, this is often not a realistic
situation in industrial settings. For example, a system may be repaired by
replacing an entire circuit board consisting of multiple components, in which
case whenever one of the components on that circuit board fails, all of the
components on the circuit board are potential component causes of failure.
In this case, we have a non-unique MLE, which provides less information
about the reliability of the series system and its components."
#date: "1/22/2022"
output:
  #bookdown::pdf_document2:
  #  extra_dependencies: ["amsthm","amsmath"]
  pdf_document:
    toc: yes
    toc_depth: 2
    number_sections: true
    extra_dependencies: ["graphicx","amsthm","amsmath","natbib","tikz"]
    df_print: kable
    keep_tex: true
    #after_body: appendix.html
indent: true
bibliography: refs.bib
csl: the-annals-of-statistics.csl
---

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\renewcommand{\v}[1]{\boldsymbol{#1}}
\newcommand{\T}{T}
\newtheorem{condition}{Condition}[section]
\renewcommand{\v}[1]{\boldsymbol{#1}}
\numberwithin{equation}{section}




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(printr)
options(digits=4)
library(series.system.estimation.masked.data)
library(tidyverse)
library(md.tools)
library(algebraic.mle)
```

Introduction
============
Accurate reliability specifications of components of a multi-component system,
such as mean time to failure (MTTF), can be quite valuable.
For instance, such information may be used to help identify the component cause
of system failure.
However, this information is often not available, or if it is available, it may
not be very accurate.
In this case, we may try to estimate the reliability of each component from
system failure data.

We are interested in estimating the reliability of  *series systems*.
In a series system, the system fails whenever any of its components fail.
The famous statistician George Box once opined that all models are wrong, but some
are useful, meaning that a theoretical model of real phenomenon is
incapable of representing its exact behaviour, but it may still be useful.
Indeed, sometimes it is even more useful, since a simplified model is more
understandable and only includes the most salient features.

Since many real-world systems are greatly impaired whenever a set of
its *critical* components fail, the series system model is often a very *useful*
approximation of its internal structure.
For example, people in a particular experiment may be regarded as series systems
where the components are some relevant set of vital organs. This model is
often a very useful abstraction that averages over many complicated, unobserved
details that may only have a small effect on an average person's lifespan.

In this paper, the experimental unit is a particular type of series system that
consists of $m$ components and we seek to estimate the reliability of the
components from *masked data* where we observe only partial information about
the system, e.g., the component lifetimes are masked (not observed in the sample).

We imagine that we conduct an experiment where we obtain the lifetimes
of the $m$ components, which has a sample space $(0,\infty)^m$.
An observed result is called an *outcome* of the experiment, e.g.,
$(1.2,0.5,1.1)$.
However, we assume that this data is masked in the following ways:

1. The experiments may be right-censored, which means that the experiment or
   observation may be suspended before the system fails.

2. We can only observe the lifetime of the component with the minimum
   lifetime. This constraint is imposed by the fact that the system
   is arranged in series and thus stops working whenever one of the components
   fails.
   
3. It may not be easy or possible to identify the failed component. We consider
   the case where we can only observe a set of components that includes the
   failed component. We refer to this set of components as the candidate set.\footnote{
   If the candidate set has only a single component, then we know the exact
   component cause of failure.}
   
   This constraint may be due to a variety of reasons.
   A canonical example is given by supposing that when the series system fails,
   it is repaired by simultaneously replacing some subset of the components,
   thus preventing isolating the specific failed component.
   
We take a random sample of $n$ such experiments, where the right-censored
system lifetimes are i.i.d. when conditioned on the right-censoring time, but
the candidate sets are not assumed to be independent but not necessarily
identically distributed.
In later sections, we consider a set of realistic conditions that allow us to
ignore the distribution of candidate sets, which allows us to construct a
reduced likelihood function for the masked failure data. Finally, we find the
MLE $\hat{\v\theta}$ that maximizes the reduced likelihood function, which is
the same MLE for "full" likelihood function where we know the distribution of
candidate sets.

Statistical model {#sec:statmod}
===============================
Consider a system with $m$ components, indexed by $1,2,\ldots,m$.
Each component only has two possible states, *failed* and *functioning*.
The state of the system also only has two possible states, *failed*
and *functioning*.
The lifetime of a system is the elapsed time from when the new, functioning
system is put into operation until it fails for the *first time*.
We are not interested in what happens to a system after it fails, e.g., we do
not consider repairing systems and putting them back into operation under
further observation.

A system that is in a functioning state if and only if at least $k$
of the $m$ components are in a functioning state is denoted a $k$-out-of-$m$
system.
We narrow our scope to $m$-out-of-$m$ systems, also known as *series systems*.
A series system is functioning if and only if every component is functioning.
Consequently, in a series system, precisely one component is the cause of
failure.

Our sample consists of lifetime data for i.i.d. series systems.
Since the component lifetimes are subject to chance variations, we denote the
lifetime of the $j$\textsuperscript{th} component of the $i$\textsuperscript{th}
system by the random variable $T_{i j}$.
We assume that the $m$ component lifetimes, $T_{i 1},\ldots,T_{i m}$, are
statistically independent and non-identical.
In a series system, whenever any component fails, the system fails.
Thus, the lifetime of the $i$\textsuperscript{th} system, $\T_i$, is given by
the component with the smallest lifetime,
$$
    \T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
$$

We assume that the component lifetimes may be adequately modeled by some
parametric probability distribution.
In what follows, matrices and vectors are denoted in boldface, e.g., $\v{x}$ is
a vector.
The $i$\textsuperscript{th} column and $(i,j)$-th element of a matrix $\v{A}$
is denoted respectively by $\v{A_j}$ and $\v{A}_{i j}$.
We let $\v{\theta_j}$ denote the parameter vector of the $j\textsuperscript{th}$
component, and each $\v{\theta_j}$ for $j=1,\ldots,m$ may be different sizes.
We define the parameter vector of the entire series system by
$$
    \v\theta = (\v{\theta_1},\ldots,\v{\theta_m}).
$$

The cumulative distribution function (cdf) for a random varible $T$ is given by
\begin{equation}
 F_T(t) = \Pr\{T \leq t\},
\end{equation}
where $\Pr\{T \in E\}$ denotes the probability that a random variable $T$
realizes a value in $E$.
The probability density function (pdf) for a continuous random varible $T$ is
given by
\begin{equation}
f_T(t) = \frac{d}{dt} F_T(t).
\end{equation}
If a random variable $X$ is discrete, then its probability mass function (pmf) $f$
is given by$f(x) = \Pr\{X = x\}$.

If a random variable $T$ has a parametric distribution indexed by a
parameter vector $\v\beta$, we denote its pdf by $f_T(t;\v\beta)$ and likewise
for other distribution functions, e.g., the $i$\textsuperscript{th} series
system has a cdf denoted by $F_{\T_i}(t;\v{\theta})$.
In the case of the lifetime distribution functions of components, we subscript
the distribution functions by component index instead of the symbol for the
random variable, e.g., the cdf of the $j$\textsuperscript{th} component for the
$i$-th system is denoted by $F_j(t;\v{\theta_j})$ instead of
$F_{T_{i j}}(t;\v{\theta_j})$.
If it is clear from the context which random variable a distribution
function is for, we may drop the subscripts, e.g., $F(t)$ instead of $F_T(t)$.
Finally, as an abuse of notation, we often write a function as $f(t)$ when we
really mean that $f$ is a function of variable $t$.

There are two particularly important functions in survival analysis, the
survival function and the hazard function.
\begin{definition}
The survival function, $R_T(t)$, of a random lifetime $T$ is the
probability that it realizes a value larger than some specified duration of time $t$,
\begin{equation}
\begin{split}
R_T(t) &= \Pr\{T > t\}\\
     &= 1 - F_T(t).
\end{split}
\end{equation}
In other words, $R_T(t)$ denotes the probability that $T$ survives longer than
$t$.
\end{definition}

The hazard function is a bit more subtle. For a random lifetime $T$, the
probability that a failure occurs between $t$ and $\Delta t$ given that no
failure occurs before time $t$ is given by
$$
\Pr\{T \leq t+\Delta t|T > t\} = \frac{\Pr\{t < T < t+\Delta t\}}{\Pr\{T > t\}}.
$$
The *failure rate* is given by the above divided by the length of the time
interval, $\Delta t$:
$$
\frac{\Pr\{t < T_{i j} < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_{i j} > t\}} =
    \frac{R_j(t;\theta_j) - R_j(t+\Delta t;\theta_j)}{R_j(t)}.
$$
\begin{definition}
\label{def:failure_rate}
The hazard function $h_T(t)$ for a continuous random variable $T$ is the
instantaneous failure rate at time $t$, which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_T(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < T < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T > t\}}\\
       &= \frac{f_T(t)}{R_T(t)}.
\end{split}
\end{equation}
\end{definition}

Two random variables $X$ and $Y$ have a joint pdf $f_{X,Y}(x,y)$.
Given the joint pdf $f(x,y)$, the marginal pdf of $X$ is given by
$$
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
$$
where $\mathcal{Y}$ is the support of $Y$. (If $Y$ is discrete, replace
the integration with a summation over $\mathcal{Y}$.)

The conditional pdf of $Y$ given $X=x$, $f_{Y|X}(y|x)$, is defined as
$$
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
$$
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of $X_1,\ldots,X_m$ is denoted by $f(x_1,\ldots,x_m)$.

## Masked data {#sec:data}
The object of interest is the (unknown) parameter value $\v\theta$. 
To estimate $\v\theta$, we need *data*.
In general, there are three types of life data: exact lifetime,
interval lifetime (where the lifetime is known to have occurred
between two points in time), and right censored data, where the suspension time
of the experiment is reached before the system fails.

In this paper, we limit our focus to exact lifetimes and right censored data.
In particular, we consider a sample of $n$ i.i.d. series systems, each of
which is put into operation at some time and and observed until either it fails
or is right-censored.
We denote the fixed right-censoring time by $\tau$.
Whether the $i$\textsuperscript{th} system is right-censored is a random
variable defined as $\delta_i = 1_{\{\tau_i < \T_i\}}$ where
$\delta_i = 1$ indicates right-censoring.
The right-censored time of the $i$\textsuperscript{th} system in the
sample, $S_i$, is given by
\begin{equation}
S_i = \min\{\T_i,\tau_i\}
\end{equation}
and the indicator for right censoring, $\delta_i$, is given by
\begin{equation}
\delta_i = 1_{\T_i > \tau_i}.
\end{equation}

We also have masked data about the component cause of failure in
the form of candidate sets.
A candidate set consists of some subset of component indices that plausibly
contain the index of the failed component.
The sample space of candidate sets are all subsets of $\{1,\ldots,m\}$, with the
exception of the empty set (thus there are $2^m - 1$ possible outcomes in the
sample space).
If a series system is right-censored ($\delta_i = 1$), then all of the
components are in the candidate set.
We typically denote the candidate set corresponding to the
$i$\textsuperscript{th} system in the sample by $c_i$.
Since the data generating process for candidate sets may be subject to chance
variations, we model it as a random set $\mathcal{C}_i$.
We may also view $\mathcal{C}_i$ as a random Boolean vector $X_{i 1},\ldots,X_{i m}$
where $X_{i j}=1$ denotes that the $j$\textsuperscript{th} component is in the
observed candidate set $c_i$.

We let $\{a_i\}_{i \leq k}$ denote the sequence $a_1,\ldots,a_k$. If it is
clear from the context, we may also use $\{a_i\}$.
The random sample of masked data is give by
$$
    \{(S_i,\delta_i,\mathcal{C}_i)\}_{i \leq n},
$$
where $\mathcal{C}_i$ is the random candidate set, $S_i$ is the right-censored
lifetime, and $\delta_i$ is the indicator for right-censoring for
the $i$\textsuperscript{th} series system.
We show a graphical model of how the masked data is generated in Figure
\ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{Dependency graph of the generative model for $(S_i,\delta_i,\mathcal{C}_i)$.
The elements in green are observed in the sample, while the elements in red
are unobserved. We see that $\mathcal{C}_i$ is a related to both the unobserved
component lifetimes $T_{i 1},\ldots,T_{i m}$ and other unknown and unobserved
covariates, like ambient temperature or the particular diagnostician who
generated the candidate set. These two complications for $\mathcal{C}_i$ are
why we in later sections seek a way to construct a reduced likelihood function
that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data for exact, right-censored system failure times with
candidate sets that mask the component cause of failure can be seen in Table 1
for an $3$-out-of-$3$ series system.

| Right-censored time ($S_i$) | Right censored ($\delta_i$) | Candidate set ($C_i$) |
| --------------------------- | --------------------------- | --------------------- |
| $4.3$                       | 0                           | $\{1,2\}$             |
| $1.3$                       | 0                           | $\{2\}$               |
| $5.4$                       | 1                           | $\{\}$                |
| $2.6$                       | 0                           | $\{2,3\}$             |
| $3.7$                       | 0                           | $\{1,2,3\}$           |
| $10$                        | 1                           | $\{\}$                |

: Right-censored lifetime data with masked component cause of failure.

Series system lifetime {#sec:ttf}
=================================
The reliability function of the series system is given by the following theorem.
\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R(t;\v\theta) = \Pr\{\T_i > t\}
$$
which may be rewritten as
$$
  R(t;\v\theta) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R(t;\v\theta) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R(t;\v\theta) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\v\theta) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
\end{proof}

\begin{theorem}
\label{thm:sys_pdf}
The random system lifetime $\T_i$ has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f(t;\v\theta) = \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f(t;\v\theta) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f(t;\v\theta)
    &= -\frac{d}{dt} R_1(t;\v{\theta_1})\prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j})\\
    &= f_1(t;\v\theta) \prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f(t;\v\theta) = \sum_{j=1}^{m-1} f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\v{\theta_j}) \frac{d}{dt} R_m(t;\v{\theta_m}),
$$
which simplifies to
$$
f(t;\v\theta)= \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}).
$$
\end{proof}

\begin{theorem}
\label{thm:sys_failure_rate}
The system lifetime $\T_i$ has a hazard function function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h(t;\v\theta) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h(t;\v\theta) = \frac{f_{\T_i}(t;\v\theta)}{R_{\T_i}(t;\v\theta)}.
$$
Plugging in expressions for these functions results in
$$
  h(t;\v\theta) = \frac{\sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k})}
      {\prod_{j=1}^m R_j(t;\v{\theta_j})},
$$
which can be simplified to
\begin{align*}
h_{T_i}(t;\v\theta)
    &= \sum_{j=1}^m \frac{f_j(t;\v{\theta_j})}{R_j(t;\v{\theta_j})}\\
    &= \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{align*}
\end{proof}

By definition,
$$
h(t;\v\theta) = \frac{f(t;\v\theta)}{R(t;\v\theta)},
$$
and thus
\begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f(t;\v\theta) &= h(t;\v\theta) R(t;\v\theta)\\
              &= \bigl\{\sum_{j=1}^m h_j(t;\v{\theta_j})\bigr\}
                 \bigl\{ \prod_{j=1}^m R_j(t;\v{\theta_j}) \bigr\},
\end{split}
\end{equation}
which we find to be a more convenient form than Equation \eqref{eq:sys_pdf}.

Component cause of failure {#sec:comp_cause}
============================================
If a series system failed at time $t$, precisely one of the components failed at
time $t$.
We model the component cause of the series system failure as a random variable.
\begin{definition}
The component cause of failure of the $i$\textsuperscript{th} series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}
Note that a more succinct way to define $K_i$ is given by
$$
K_i = \operatorname{argmin}_j \bigl\{ T_{i j} : j \in \{1,\ldots,m\}\bigr\}.
$$

The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.
\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$\T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) R_{\T_i}(t;\v\theta),
\end{equation}
where $h_j(t;\v{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_{\T_i}(t;\v\theta)$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a $3$-out-of-$3$ system.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\v\theta) = \prod_{j=1}^{3} f_j(t;\v{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\v\theta) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2}) f_3(t_3;\v{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2})
        R_3(t;\v{\theta_3}) dt_2\\
     &= f_1(t;\v{\theta_1}) R_2(t;\v{\theta_2}) R_3(t_1;\v{\theta_3}).
\end{align*}
Since $h_1(t;\v{\theta_1}) = f_1(t;\v{\theta_1}) / R_1(t;\v{\theta_1})$,
$$
f_1(t;\v{\theta_1}) = h_1(t;\v{\theta_1}) R_1(t;\v{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\v\theta)$
yields
\begin{align*}
f_{K_i,T_i}(j,t;\v\theta)
    &= h_1(t;\v{\theta_1}) \prod_{l=1}^m R_l(t;\v{\theta_l})\\
    &= h_1(t;\v{\theta_1}) R(t;\v{\theta}).
\end{align*}
Generalizing from this completes the proof.
\end{proof}

Masked data models
==================
We consider two types of masked data, *right-censored* system lifetime data 
(masking the system lifetime) and *masking the component cause of failure*. In
the following subsections, we derive the distribution functions for each of these
types of masking.

## Masked component cause of failure {#sec:candmod}
Suppose a diagnostician is unable to identify the precise component cause of the
failure, e.g., due to cost considerations he or she replaced multiple components
at once, successfully repairing the system but failing to precisely identity
the failed component.
In this case, the cause of failure is said to be *masked*.

The unobserved component lifetimes may have many covariates, like ambient
operating temperature, but the only covariate we observe in our masked data
model are the system's lifetime and additional masked data in the form of
a candidate set that is somehow correlated with the unobserved component
lifetimes. The joint distribution of $\mathcal{C}_i$ and $\T_i$ has the joint
pdf that may be written as
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = f_{\T_i}(t_i;\v{\theta})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i\}.
$$

We assume we know the pdf $f_{\T_i}(t_i;\v{\theta})$ but we do not know
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i\}$, i.e., the data generating
process for candidate sets is not known.
However, in order for the masked data $\mathcal{C}_i$ to provide information
about $\v{\theta}$, $\mathcal{C}_i$ must be correlated with the
$i$\textsuperscript{th} system in some useful way, in which case the conditional
distribution of $\mathcal{C}_i$ given $\T_i = t_i$ may be important even though
the object of statistical interest is the series system rather than the
candidate sets, which may be statistically problematic, e.g.,
$\mathcal{C}_1,\mathcal{C}_2,\ldots,\mathcal{C}_n$ may not be identically
distributed or there may be many important unobserved covariates.

To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which $\mathcal{C}_i$ is correlated with the
$i$\textsuperscript{th} system is given by assuming the following condition.
\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}
Assuming Condition \ref{cond:c_contains_k}, $\mathcal{C}_i$ must contain the
index of the failed component, but we can say little else about what other
component indices may appear in $\mathcal{C}_i$.

In order to derive the joint distribution of $\mathcal{C}_i$ and $\T_i$ assuming
Condition \ref{cond:c_contains_k}, we take the following approach.
We notice that $\mathcal{C}_i$ and $K_i$ are statistically dependent.
We denote the conditional pmf of $\mathcal{C}_i$ given $\T_i = t_i$ and
$K_i = j$ as
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i, K_i = j\}.
$$

Even though $K_i$ is not observable in our masked data model, we can still
consider the joint distribution of $\T_i$, $K_i$, and $\mathcal{C}_i$.
By Theorem \ref{thm:f_k_and_t}, the joint pdf of $\T_i$ and $K_i$ is given by
$$
f_{\T_i,K_i}(t_i,j;\v{\theta}) = h_j(t_i;\v{\theta_j}) R_{\T_i}(t_i;\v\theta),
$$
where $h_j(t_i;\v{\theta_j})$ is the hazard function for the
$j$\textsuperscript{th} component and $R_{\T_i}(t_i;\v{\theta})$ is the
reliability function of the system.
Thus, the joint pdf of $\T_i$, $K_i$, and $\mathcal{C}_i$ may be written as
\begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{\T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta})
    &= f_{\T_i,K_i}(t_i,k;\v{\theta}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\v{\theta_j}) R_{\T_i}(t_i;\v\theta)
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}.
\end{split}
\end{equation}
We are going to need the joint pdf of $\T_i$ and $\mathcal{C}_i$, which
may be obtained by summing over the support $\{1,\ldots,m\}$ of $K_i$ in
Equation \eqref{eq:joint_pdf_t_k_c},
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{\T_i}(t_i;\v\theta)
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
    \biggr\}.
$$
By Condition \ref{cond:c_contains_k},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\} = 0$ when $K_i = j$ and
$j \notin c_i$, and so we may rewrite the joint pdf of $T_i$ and
$\mathcal{C}_i$ as
\begin{equation}
\label{eq:part1}
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of $\v\theta$ (see Section \ref{sec:mle}), we
solve the simultaneous equations of the MLE and choose a solution
$\hat{\v\theta}$ that is a maximum for the likelihood function.
When we do this, we find that $\hat{\v\theta}$ depends on the unknown
conditional pmf $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}$.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}$.

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}
According to [@Fran-1991], in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In this
setting, Condition \ref{cond:equal_prob_failure_cause} generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$ may be factored out of the
summation in Equation \eqref{eq:part1}, and thus the joint pdf of $T_i$ and
$\mathcal{C}_i$ may be rewritten as
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j})
$$
where $j' \in c_i$.

If $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ is a function of
$\v{\theta}$, the MLEs are still dependent on the unknown
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ to find an MLE of
$\v{\theta}$. The last condition we assume achieves this result.
\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $\T_i$ and component cause
of failure $K_i$ are not functions of $\v{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $\T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\v\theta$.
\end{condition}

When Conditions \ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause},
and \ref{cond:masked_indept_theta} are satisfied, the joint pdf of $\T_i$ and
$\mathcal{C_i}$ is given by
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \beta_i R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
When we fix the sample and allow $\v{\theta}$ to vary, we obtain the
contribution to the likelihood from the $i$\textsuperscript{th} observation,
$$
l_i = R_{\T_i}(t_i;\v\theta) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
This derivation of $l_i$ is the one used in many papers on masked series
lifetime data (see [@Huairu-2013]).

Suppose we have jointly observed a candidate set and a series system
failure time and we are interested in the probability that a particular component
is the cause of the observed system failure.
\begin{theorem}
Assuming Conditions \ref{cond:c_contains_k} and \ref{cond:equal_prob_failure_cause},
the conditional probability of $K_i$ given $\mathcal{C}_i = c_i$ and $\T_i = t_i$
is given by
\begin{equation}
\Pr\{K_i = j|\T_i=t_i,\mathcal{C}_i=c_i\} =
 \frac{h_j(t_i|\v{\theta_j})}{\sum_{l \in c_i} h_l(t_i|\v{\theta_l})} 1_{\{j \in c_i\}}.
\end{equation}
\end{theorem}
\begin{proof}
The conditional probability $\Pr\{K_i = j|\T_i=t_i,\mathcal{C}_i=c_i\}$ may be
written as
$$
\Pr\{K_i = j|\T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{C_=c_i|K_i = j,T_i=t_i\} f_{K_i,\T_i}(j,t_i;\v\theta)}
    {\sum_{j=1}^m \Pr{}_{\!\v\theta}
        \{C_=c_i|K_i = j,\T_i=t_i\} f_{K_i,\T_i}(j,t_i;\v\theta)}.
$$
By Theorem \ref{thm:f_k_and_t},
$f_{K_i,T_i}(j,t_i;\v\theta) = h_j(t_i;\v\theta)R_{\T_i}(t_i;\v\theta)$.
We may make this substitution in the above equation and cancel the common
factors $R_{\T_i}(t_i;\v\theta)$ in the numerator and denominator, yielding
$$
\Pr\{K_i = j|\T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{C_=c_i|K_i = j,T_i=t_i\} h_j(t_i;\v\theta)}
         {\sum_{j=1}^m \Pr{}_{\!\v\theta}\{C_=c_i|K_i=j,\T_i=t_i\} h_j(t_i;\v\theta)}.
$$
Assuming Condition \ref{cond:c_contains_k}, we may rewrite the above as
$$
\Pr\{K_i = j|\T_i=t_i,\mathcal{C}_i=c_i\}
    = \frac{\Pr{}_{\!\v\theta}\{C_=c_i|K_i = j,T_i=t_i\} h_j(t_i;\v\theta)}
        {\sum_{l \in c_i} \Pr{}_{\!\v\theta}\{C_=c_i|K_i = l,\T_i=t_i\} h_j(t_i;\v\theta)}.
$$
Assuming Condition \ref{cond:equal_prob_failure_cause}, we may rewrite the above
as
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{\Pr{}_{\!\v\theta}\{C_=c_i|K_i = j',T_i=t_i\} h_j(t_i;\v{\theta_j})}
    {\Pr{}_{\!\v\theta}\{C_=c_i|K_i = j',T_i=t_i\} {\sum_{l \in c_i} h_l(t_i;\v{\theta_l})}},
$$
where $j' \in c_i$.
Finally, we may cancel the common probability masking factors in the numerator
and denominator, obtaining the result
$$
\Pr\{K_i = j|T_i=t_i,\mathcal{C}_i=c_i\} =
    \frac{h_j(t_i;\v{\theta_j})}
    {\sum_{l \in c_i} h_l(t_i;\v{\theta_l})}.
$$
\end{proof}
Note that we do not require Condition \ref{cond:masked_indept_theta} in deriving
the conditional distribution of $K_i$ given $\T_i$ and $\mathcal{C}_i$.

If we do not know a particular subset $\mathcal{C}_i$, then we let $\mathcal{C}_i$
contain all $m$ componentsn, $\{1,\ldots,m\}$, in which case we have
$$
\Pr\{K_i = j|T_i=t_i\} = \frac{h_j(t_i;\v{\theta_j})}{h(t_i;\v{\theta})}.
$$

## Right-censored data
In the previous two sections, we discussed the joint distribution of the
system lifetime $\T_i$ and the candidate set $\mathcal{C}_i$.
However, in our data model, we do not directly observe $\T_i$.
As described in Section \ref{sec:data}, we instead observe realizations of
$(S_i,\delta_i,\mathcal{C}_i)$ where $S_i = \min\{\T_i,\tau_i\}$ is the
right-censored system lifetime, $\delta_i = 1_{\{\T_i > \tau_i\}}$ is the
right-censoring indicator, and $\mathcal{C}_i$ is the candidate set.

In the candidate set model, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta} hold,
the candidate set includes the index of the failed component.
However, when right censoring occurs, there are no failed components, and
therefore, there is no masking for the cause of failure.
In this case, we let the candidate set be the empty set $\emptyset$.

We now present a theorem for the joint pdf of $S_i, \delta_i, \mathcal{C}_i$.
\begin{theorem}
\label{thm:joint_s_d_c}
The joint pdf of $S_i,\delta_i,\mathcal{C}_i$, where $\delta_i$
indicates right-censoring, is given by
\begin{equation}
f(s_i,\delta_i,c_i;\v\theta) = R_{\T_i}(s_i;\v\theta)
    (1_{\{c_i=\emptyset\}})^{\delta_i}
    \biggl(\beta_i \sum_{j \in c_i} h_j(s_i;\v{\theta_j})\biggr)^{1-\delta_i}.
\end{equation}
\end{theorem}
\begin{proof}
There are two cases to consider.

Case 1: If $\delta_i = 0$, then $S_i < \tau_i$. In this case, $S_i = \T_i$, and so
\begin{align}
f_{S_i,\mathcal{C}_i}(s_i,c_i;\v\theta)
    &= f_{\T_i,\mathcal{C}_i}(s_i,c_i;\v\theta) 1_{\{\delta_i = 0\}}\\
    &= \beta_i R_{\T_i}(s_i;\v\theta) \sum_{j \in c_i} h_j(s_i;\v\theta_j) 1_{\{\delta_i = 0\}}.
\end{align}

Case 2: If $\delta_i = 1$, then $S_i = \tau_i$ and $c_i = \emptyset$. In this
case, we only know that $\T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
\begin{align}
f_{S_i,\mathcal{C}_i}(s_i,c_i;\v\theta)
    &= \int_{\tau_i}^{\infty} f_{\T_i}(t;\v\theta) 1_{\{s_i = \tau_i \land \delta_i = 1 \land c_i = \emptyset\}} dt\\
    &= R_{\T_i}(s_i;\v\theta) 1_{\{\delta_i = 1 \land c_i = \emptyset\}}.
\end{align}

We can add these two cases together (since only one of them is active given the
indicator function), and so we can write the joint pdf as
\begin{align}
f(s_i,\delta_i,c_i;\v\theta)
    &= \beta_i R_{\T_i}(s_i;\v\theta) \sum_{j \in c_i} h_j(s_i;\v\theta_j) 1_{\{\delta_i = 0\}} +
       R_{\T_i}(s_i;\v\theta) 1_{\{\delta_i = 1 \land c_i = \emptyset\}}\\
    &= R_{\T_i}(s_i;\v\theta) \bigl(\beta_i \sum_{j \in c_i} h_j(s_i;\v\theta_j) 1_{\{\delta_i = 0\}} +
       1_{\{\delta_i = 1 \land c_i = \emptyset\}}\bigr)\\
    &= R_{\T_i}(s_i;\v\theta) (1_{\{c_i=\emptyset\}})^{\delta_i}
       \bigl(\beta_i \sum_{j \in c_i} h_j(s_i;\v{\theta_j})\bigr)^{1-\delta_i}.
\end{align}


\end{proof}
Maximum likelihood estimation {#sec:mle}
========================================
We use maximum likelihood estimation (MLE) to estimate the series system
parameter vector $\v\theta$ given the masked failure data described in Section
\ref{sec:data}.
This is achieved by maximizing the likelihood function $L(\v\theta)$ with
respect to $\v\theta$ so that, under the assumed model, the observed data is
most likely.
The point in the parameter space $\Omega$ that maximizes the likelihood function
is called the maximum likelihood estimate.

Likelihood function
-------------------
Given an i.i.d. random sample of masked data
$$
\left\{(S_i,\delta_i,\mathcal{C}_i)\right\}_{i \leq n}
$$
parameterized by $\v\theta \in \v\Omega$, the joint pdf is given by
$$
f\bigl(\{(s_i,\delta_i,\mathcal{C}_i)\}_{i \leq n};\v\theta\bigr)
    = \prod_{i=1}^n f(s_i,\delta_i,\mathcal{C}_i;\v\theta).
$$
The joint pdf computes the likelihood that the observed data,
$\{s_i,\delta_i,c_i\}_{i \leq n}$, occurs.
When we fix the observe data and instead allow the parameter $\v\theta$
to vary, we obtain what is called the likelihood function,
$$
L(\v\theta) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\v\theta).
$$
Substituting the joint pdf of $S_i,\delta_i,\mathcal{C}_i$ given by
Theorem \ref{thm:joint_s_d_c}, we obtain the likelihood function for our
specified data,
\begin{equation}
\label{eq:like}
L(\v\theta) =
    \prod_{i=1}^n R_{\T_i}(s_i;\v\theta) 
    \biggl\{
        \beta_i \sum_{j\in c_i} h_j(s_i;\v{\theta_j})
    \biggr\}^{1-\delta_i}.
\end{equation}
The maximum likelihood estimator is given by the estimate of $\v\theta$
that maximizes the likelihood function.

According to @bain, a point $\hat{\v\theta}$ in $\v\Omega$ at which $L(\v\theta)$
is a maximum is called the *maximum likelhood estimate* (MLE) of $\v\theta$.
That is, $\hat{\v\theta}$ is a value of $\v\theta$ that satisfies
\begin{equation}
\label{eq:mle}
f\bigl(\bigl\{(s_i,\delta_i,\mathcal{C}_i)\bigr\};\hat{\v\theta}\bigr)
    = \max_{\v\theta \in \v\Omega}
        f\bigl(\bigl\{(s_i,\delta_i,\mathcal{C}_i)_{i \leq n}\bigr\};\v\theta\bigr).
\end{equation}
Essentially, the MLE is the point in the parameter space that maximizes the
likelihood of the observed data.

Any point that maximizes the likelihood function also maximizes the log-likelihood
function.
Thus, for computational reasons, we work with the log-likelihood function.
\begin{theorem}
The log-likelihood function, $\ell(\v\theta)$, is given by the logarithm of the
likelihood function for the masked failure data that satisfies Conditions
\ref{cond:c_contains_k}, \ref{cond:c_contains_k}, and \ref{cond:c_contains_k},
\begin{equation}
\label{eq:loglike}
\ell(\v\theta) = \sum_{i=1}^n \log R_{\T_i}(s_i;\v\theta) +
    \sum_{i=1}^n (1-\delta_i) \log
    \biggl\{
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})
    \biggr\}.
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is just the logarithm of the likelihood function,
$$
\ell(\v\theta) = \log L(\v\theta).
$$
Substituting the likelihood function given by Equation \eqref{eq:like}, we
rewrite the log-likelihood function as
\begin{align*}
\ell(\v\theta)
    &= \log
    \biggl\{
        \prod_{i=1}^n R_{\T_i}(s_i;\v\theta) 
        \biggl\{
            \beta_i \sum_{j\in c_i} h_j(t_i;\v{\theta_j})
        \biggr\}^{1-\delta_i}
    \biggr\}\\
    &= \log \prod_{i=1}^n R_{\T_i}(s_i;\v\theta) + \log
        \prod_{i=1}^n \biggl\{
            \beta_i \sum_{j\in c_i} h_j(s_i;\v{\theta_j})
        \biggr\}^{1-\delta_i}\\
    &= \sum_{i=1}^n \log R_{\T_i}(s_i;\v\theta) +
       \sum_{i=1}^n (1-\delta_i) \log \biggl\{
            \sum_{j\in c_i} h_j(s_i;\v{\theta_j})
        \biggr\} + \text{constant},
\end{align*}
where we aggregate all the $\beta$ terms into the constant (with respect to
$\v\theta$) and so can be ignored in MLE.
\end{proof}

## Solving the maximum likelihood equations {#sec:iterative}
According to @bain, if $\v\Omega$ is a Cartesian product of $l$ intervals,
partial derivatives of $L(\v\theta)$ exist, and the MLEs do not occur on the
boundary of $\v\Omega$, then the MLEs will be solutions of the simultaneous
equations
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0
\end{equation}
for $j=1,\ldots,p$ where $p$ is the number of components in $\v\theta$.
We call these equations the *maximum likelihood equations*.
If multiple solutions to Equation \eqref{eq:mle_eq} exist, then a maximum of
these solutions with respect to the likelihood function should be selected.

If there is no closed-form solution to the maximum likelihood equations
(see \eqref{eq:mle_eq}), we may use iterative root-finding methods to
numerically approximate a solution.
A general approach is, if we have a guess $\v\theta^{(n)}$, take a step in
a "promising" direction $\v{d}^{(n)}$ to obtain the next guess,
\begin{equation}
\label{eq:iterative_update}
\v\theta^{(n+1)} = \v\theta^{(n)} + \alpha^{(n)} \v{d}^{(n)},
\end{equation}
where $\v\theta^{(0)}$ is the initial guess, e.g., a random point in 
the parameter space $\Omega$.

Assuming that at $\v\theta^{(n)}$, a sufficiently small step in the direction
$\v{d}^{(n)}$ results in an improvement with respect to the log-likelihood function,
we say that we *overshoot* if
$$
\ell(\v{\theta}^{(n+1)}) < \ell(\v{\theta}^{(n)}).
$$
The value $\alpha^{(n)}$ in Equation \eqref{eq:iterative_update} is a positive
real number chosen by a *line search* method so that we do not overshot, which
has an optimal value given by
$$
\alpha^{(n)} = \operatorname{argmax}_{\alpha}
    \ell(\v{\theta}^{(n)} + \alpha \v{d}^{(n)}).
$$
However, this may be too computationally expensive to compute, and so we use
a less optimal but faster method known as *backtracking*.
In the backtracking line search method, we determine $\alpha^{(n)}$ by initially
letting $\alpha^{(n)} = 1$ and then, if we overshoot, redo the update
with $\alpha^{(n)} \gets r \alpha^{(n)}$, $0 < r < 1$, repeating until we
do not overshoot.

Note that this is not necessarily the best course of action for finding
global maximums, since depending on our initial guess $\v\theta^{(0)}$, we may
get stuck in a local maximum.
There are many other methods that are more likely to find a global maximum,
such as sometimes moving in a direction that decreases the likelihood of the
guess. However, for our simulations, we found that we were always able to find
a global maximum using the iterative method described in Equation \eqref{eq:iterative_update}.

We do as many iterations in Equation \eqref{eq:iterative_update} as necessary to
satisfy some *stopping condition*, which is usually something simple like the
distance between $\v\theta^{(n)}$ and $\v\theta^{(n+1)}$ being sufficiently
small. Under the right conditions, for sufficiently large $n$,
$\v\theta^{(n)} \approx \hat{\v\theta}$.

Two popular iterative techniques are Newton-Raphson method and gradient ascent,
which respectively are defined by letting
$$
\v{d}^{(n)}_{\text{Newton-Raphson}} = J^{-1}(\v\theta^{(n)}) \nabla \ell(\v\theta^{(n)})
$$
and
$$
\v{d}^{(n)}_{\text{gradient ascent}} = \nabla \ell(\v\theta^{(n)}),
$$
where $J(\v\theta^{(n)})$ and $\nabla \ell(\v\theta^{(n)})$ are respectively
the observed Fisher information matrix (Hessian of the
log-likelihood function) and the score (gradient of the log-likelihood
function), each evaluated at $\v\theta^{(n)}$.

Depending upon the nature of the log-likelihood function, one or the other may
work better in practice.\footnote{For example, one approach may be more
computationally efficient.}
The R code for iterative methods is in Appendix C.


## Properties of the MLE
According to @bain, if certain regularity conditions are satisfied, then
solutions of the maximum likelihood equation \eqref{eq:mle_eq} have the
following desirable properties:

1. $\v{\hat\theta}$ exists and is unique.

2. $\v{\hat\theta}$ is an asymptotically unbiased estimator.

3. $\v{\hat\theta}$ is asymptotically the UMVUE, the uniform minimum variance unbiased
   estimator.
   
4. $\v{\hat\theta}$ is asymptotically normal with a mean $\v\theta$ and a
   variance-covariance that is the inverse of the Fisher information matrix,
   whose $(i,j)$-th component is given by
   $$
    I(\v\theta)_{i j} = n E_{\v\theta}\biggl(-\frac{\partial^2}{\partial \theta_i \partial \theta_j}
        \log f(S_i,\delta_i,\mathcal{C}_i;\v\theta)\biggr).
   $$
   
Since it is frequently problematic taking the expectation to derive $I(\theta)$,
we instead use the *observed* information matrix, which is conditioned on the
observed data,
$$
J(\v\theta)_{i j} = -\frac{\partial}{\partial \theta_i \partial \theta_j}
    \ell(\v\theta).
$$

Since we do not know $\v\theta$, we estimate $J(\v\theta)$ with $J(\v{\hat\theta})$.
Thus, assuming the regularity conditions are satisfied, then approximately,
$$
    \hat{\v\theta} \sim \mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))
$$
and as the sample size goes to infinity, $\hat{\v\theta}$ converges in
distribution to $\mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))$.


Analysis of the MLE {#sec:analysis_mle}
=======================================

## Bootstrapping {#sec:boot}

Suppose we have some random sample $\{\v{X_i}\}_1^n$ and we assume some
parametric model
$$
    \v{X_i} \sim f(\;\cdot\;;\v\beta).
$$
In the parametric Bootstrap, we estimate $\v\beta$ with $\hat{\v\beta}$ and
generate $B$ random samples of size $n$ from the estimate of the distribution,
$$
    \v{X_{i j}} \sim f(\;\cdot\;;\hat{\v\beta}).
$$
for $j=1,\ldots,B$ and $i=1,\ldots,n$, resulting in $B$ bootstrapped samples
each of size $n$,
$$
\mathcal{D} =
\bigl\{
    \{ \v{X_{1 1}}, \v{X_{1 2}}, \ldots, \v{X_{1 B}} \},
    \ldots,
    \{ \v{X_{n 1}}, \v{X_{n 2}}, \ldots, \v{X_{n B}} \}
\bigr\}.
$$

We may then apply a statistic to each sample in $\mathcal{D}$, such as the
MLE, to generate a empirical sampling distribution of the statistic.

The most common form of the bootstrap is the non-parametric bootstrap.
In the nonparametric bootstrap, the random data is created by resampling with
replacement from the original data.
Since we do not know (nor attempt to model) the distribution of
candidate sets, this non-parametric form is ideal, since we can simply resample
from the original data to approximate its empirical distribution.

Estimates of accuracy and precision {#sec:acc_prec}
---------------------------------------------------
We would like to measure the accuracy and precision of $\v{\hat\theta}$.
The bias
$$
\operatorname{b}(\v{\hat\theta}) = E(\v{\hat\theta}) - \v\theta
$$
is a measure of accuracy and variance is a measure of precision.

Assuming the regularity conditions are met, asymptotically the bias of the MLE
is $0$.
However, as claimed previously, these regularity conditions may not be met
depending upon how candidate sets are generated.
Since we are doing a simulation study, we know $\v\theta$, and therefore we can
estimate the bias.

Since the inverse of observed information matrix $J^{-1}(\v{\hat\theta})$ is an
estimate of the variance-covariance matrix of $\hat{\v\theta}$, we can estimate
the variance of the $j$\textsuperscript{th} component of $\hat{\v\theta}$ with
$$
\hat{\sigma}_j^2 = J^{-1}(\v{\hat\theta})_{j j}.
$$

The mean squared error, $\operatorname{MSE}$, is a measure of
estimator error that incorporates both the bias and the variance.
If the regularity conditions are met,
$$
\operatorname{MSE}(\v{\hat\theta}) =
    \operatorname{trace}\bigl(J^{-1}(\v{\hat\theta})\bigr) +
    \operatorname{b}^2(\v{\hat\theta})
$$
and as $n \to \infty$, the asymptotic $\operatorname{MSE}$ goes to
$$
\operatorname{trace}\bigl(J^{-1}(\hat{\v\theta})\bigr),
$$
in which case for sufficiently large samples,
$\operatorname{MSE}(\hat{\v\theta})$ is approximately equal to the trace of
inverse of the observed information matrix.

Assuming the regularity conditions, an independent $(1-\alpha) \times 100\%$
confidence interval for $\theta_j$ is given by
$$
\hat\theta_j \pm z_{1-\alpha/2} \sqrt{J^{-1}(\hat{\v\theta})_{j j}}.
$$

## Goodness of fit for real-world data {#sec:fit}
In the case of real-world data sets, we do not know the true distribution and
thus cannot use a performance measure that is a function of the unknowns,
like $\v\theta$. Moreover, the assumptions baked into our statistical model may
not be appropriate.

We seek to assess the goodness of fit of our model with respect to the observed
system lifetimes $T_1,\ldots,T_n$ using the Cramér-von Mises test, in which we
judge the goodness of fit of the estimated theoretical parametric cdf
$F_{\T_i}(t;\hat{\v\theta})$ to the empirical cdf $F_n$.

Components with exponentially distributed lifetimes {#expo}
===========================================================
Consider a series system in which the components have exponentially distributed
lifetimes.
The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{EXP}(\lambda_j)
$$
for $j=1,\ldots,m$.
Thus, $\v\lambda = \bigl(\lambda_1,\ldots,\lambda_m\bigr)$.
The random variable $T_{i j}$ has a reliability, pdf, and hazard function
given respectively by
\begin{align}
    \label{eq:expo_reliability}
    R_j(t;\lambda_j)     &= \exp(-\lambda_j t),\\
    \label{eq:expo_pdf}
    f_j(t;\lambda_j)     &= \lambda_j \exp(-\lambda_j t),\\
    \label{eq:expo_haz}
    h_j(\cdot;\lambda_j) &= \lambda_j
\end{align}
where $t > 0$ and $\lambda_j > 0$ is the failure rate (or inverse scale)
parameter.

The lifetime of the series system composed of $m$ components with exponentially
distributed lifetimes has a reliability function given by
\begin{equation}
\label{eq:sys_expo_reliability_function}
R(t;\v\theta) = \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t\biggr)
\end{equation}
where $t > 0$.
\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
    R(t;\v\lambda) = \prod_{j=1}^{m} R_j(t;\lambda_j).
$$
Plugging in the component reliability functions given by
Equation \eqref{eq:expo_reliability} obtains the result
\begin{align*}
R(t;\v\lambda) = \prod_{j=1}^{m}
    &= \exp(-\lambda_j t)\\
    &= \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t\biggr).
\end{align*}
\end{proof}

A series system with exponentially distributed lifetimes is also 
exponentially distributed.
\begin{theorem}
\label{thm:expo_series_family}
The random lifetime $T_{i j}$ of a series system composed of $m$ components with 
exponentially distributed lifetimes is exponentially distributed with a failure 
rate that is the sum of the component failure rates,
$$
    T_i \sim \operatorname{EXP} \biggl(\sum_{j=1}^{m} \lambda_j\biggr).
$$
\end{theorem}
\begin{proof}
By Equation \eqref{eq:sys_expo_reliability_function}, the series system has a
reliability function in the family of the exponential distribution with a
failure rate that is the sum of the component failure rates.
\end{proof}

The series system's failure rate function is given by
\begin{equation}
\label{eq:expo_sys_failure_rate}
h(\cdot;\v\lambda) = \sum_{j=1}^{m} \lambda_j
\end{equation}
whose proof follows from Theorem \ref{thm:sys_failure_rate}.

We see that the failure rate $\lambda = \sum_{j=1}^n \lambda$ is *constant*,
consistent with the the exponential distribution being the only continuous
distribution that has a constant failure rate.

The pdf of the series system is given by
\begin{equation}
\label{eq:expo_sys_pdf}
f(t;\v\lambda) = \biggl( \sum_{j=1}^{m} {\lambda_j} \biggr) \exp
    \biggl(-\sum_{j=1}^{m}{\lambda_j} t\biggr)
\end{equation}
where $t > 0$.
\begin{proof}
By definition,
$$
f(t;\v\lambda) = h(t;\v\lambda) R(t;\v\lambda).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:expo_sys_failure_rate} and \eqref{eq:expo_reliability} completes
the proof.
\end{proof}

The conditional probability that component $k$ is the cause of a system failure
at time $t$ is given by
\begin{equation}
\label{eq:expo_prob_K_given_S}
f_{K_i|\T_i}(k|t,\v\lambda) = f_{K_i}(k|\v\lambda) =
    \frac{\lambda_k}{\sum_{p=1}^{m} \lambda_p}
\end{equation}
where $k \in \{1,\ldots,m\}$ and $t > 0$.
\begin{proof}
By Theorem \ref{thm:f_given_s_form_2},
$$
f_{K_i|\T_i}(k;\v\lambda|t) = \frac{h_k(t;\lambda_k)}{h(t;\v\lambda)}.
$$
Plugging in the failure rate of the component indexed by $k$ and the failure
rate of the system given respectively by Equations \eqref{eq:expo_sys_failure_rate}
and \eqref{eq:expo_haz} completes the proof.
\end{proof}

Due to the constant failure rates of the components, $K_i$ and $\T_i$ are
mutually independent.
The joint pdf of $K_i$ and $\T_i$ is given by
\begin{equation}
\label{eq:expo_joint_k_s}
f_{K_i,T_i}(k,t;\v\lambda) = \lambda_k \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t\biggr)
\end{equation}
where $k \in \{1,\ldots,m\}$ and $t > 0$.
\begin{proof}
By definition,
$$
f_{K_i,T_i}(k,t;\v\lambda) =
    f_{K_i|T_i}(k;\v\lambda|t) f_{T_i}(t;\v\lambda).
$$
Plugging in the conditional probability and the marginal probability given
respectively by Equations \eqref{eq:expo_prob_K_given_S} and
\eqref{eq:expo_sys_pdf} completes the proof.
\end{proof}

## Maximum likelihood estimation
The log-likelihood for our masked data model when we assume Conditions
\ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause}, and
\ref{cond:masked_indept_theta} is given by
\begin{equation}
\label{eq:}
\ell(\v\lambda) =
    \sum_{i=1}^{n} (1-\delta_i) \log \biggl(\sum_{j \in c_i} \lambda_j \biggr) -
    \biggl( \sum_{i=1}^{n} s_i \biggr)
    \biggl( \sum_{j=1}^{m} \lambda_j \biggr).
\end{equation}
\begin{proof}
By Equation \eqref{eq:loglike}, 
$$
\ell(\v\theta) = \sum_{i=1}^n \log R(s_i;\v\theta) + \sum_{i=1}^n (1-\delta_i)
    \log \biggl\{ \sum_{k\in c_i} h_k(s_i;\v{\theta_k}) \biggr\}.
$$
Plugging in the component failure rate and system reliability functions given
respectively by Equations \eqref{eq:expo_haz} and
\eqref{eq:sys_expo_reliability_function} and simplifying completes the proof.
\end{proof}

The set of solutions to the MLE equations must be stationary points, i.e., a
point at which the score function of type $\mathbb{R}^m \mapsto \mathbb{R}^m$
is zero.
The $j$-th component of the output of the score function is given by
\begin{equation}
\label{eq:score_expo_j}
\frac{\partial \ell}{\partial \lambda_p} = 
  \sum_{i=1}^{n} \biggl( \sum_{j \in c_i} \lambda_j \biggr)^{-1}
  1_{\{p \in c_i \text{ and } \delta_i = 0\}} - \sum_{i=1}^{n} s_i.
\end{equation}

We may find an MLE by solving the maximum likelihood equation \eqref{eq:mle_eq},
i.e., a set of (stationary) points satisfying
$$
\frac{\partial \ell}{\partial \lambda_j}\Biggr|_{\hat\lambda_j} = 0
$$
for $j=1,\ldots,m$.
We approximate a solution to this problem by using the iterative
Newton-Raphson method as described in Section \ref{sec:iterative}.

The Newton-Raphson method needs the observed information matrix, which is a
function of $\v\lambda$ of type $\mathbb{R}^m \mapsto \mathbb{R}^{m \times m}$.
The $(j,k)$-th element of $J(\v\lambda)$ is given by
\begin{equation}
\label{eq:info_expo}
\frac{\partial^2 \ell}{\partial \lambda_j \partial \lambda_k} = 
  \sum_{i=1}^{n} \biggl( \sum_{j \in c_i} \lambda_j \biggr)^{-2}
  1_{\{j,k \in c_i \text{ and } \delta_i = 0\}}.
\end{equation}

## Simulation
We wish to simulate a masked data set consisting with Conditions
\ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause}, and
\ref{cond:masked_indept_theta}. We setup the parameters of this simulation with:
```{r}
# sample size
n <- 500
# m components in series system
m <- 3
# with probability q an observation will be right-censored
q <- 0.25
# true parameter value of series system
lambda <- c(2,3,2.5)  
# right censoring time
tau <- rep(-(1/sum(lambda))*log(q),n)

p <- .333
```

We generate a masked data sample of size $n=`r n`$, $m=`r m`$, $\tau_i=`r tau[1]`$,
$\v\lambda=(`r lambda`)'$, and using the simple Bernoulli candidate model 
that is consistent with conditions $C_1$, $C_2$, and $C_3$ with:
```{r,message=F,warning=F}
md <- tibble(t1=stats::rexp(n,rate=lambda[1]),
             t2=stats::rexp(n,rate=lambda[2]),
             t3=stats::rexp(n,rate=lambda[3])) %>%
    md_series_lifetime_right_censoring(tau) %>%
    md_bernoulli_cand_C1_C2_C3(p) %>%
    md_cand_sampler()
```

See Appendix A for more information about the `md_*` functions and see
The above code produces a data frame `md` whose first $10$ rows are given by
Table \ref{mytab2}.
```{r mytabl2,echo=F,eval=F}
md.tmp <- head(md,10)
md.tmp$x1 <- as.integer(md.tmp$x1)
md.tmp$x2 <- as.integer(md.tmp$x2)
md.tmp$x3 <- as.integer(md.tmp$x3)
md.tmp$delta <- as.integer(md.tmp$delta)
knitr::kable(
    md.tmp,
    caption="The first 10 observations of simulated masked data for exponentially distributed component lifetimes")
```

Now, we find the MLE with the following R code:
```{r eval=F}
loglike.exp <- md_loglike_exp_series_C1_C2_C3(md)
lambda.hat <- algebraic.mle::mle_gradient_ascent(
    l=loglike.exp,
    theta0=lambda)
points <- cbind(point(lambda.hat),as.matrix(lambda))
colnames(points) <- c("MLE","Parameter")
cbind(points,confint(lambda.hat))
```

In Section \ref{sec:acc_prec}, we consider various ways of analyzing the
accuracy and precision of the MLE $\hat{\v\lambda}$.
To do these estimations, we perform Bootstrapping to generate $R=1000$ MLEs.

TODO: I show a bunch of different estimates, like bias and confidence intervals,
using the Bootstrap method. I need to work on presenting only interesting
estimates in a way that helps show what the MLE is like for small samples,
then show how as the sample gets larger, we become consistent with the
asymptotic theory of the MLE. So, basically a graph whose x-axis is
sample size and whose y-axis is MSE or bias.

```{r, eval=F, cache=T}
N <- c(100,200,300,400,500)
for (n in N)
{
    md <- tibble(t1=stats::rexp(n,rate=lambda[1]),
                 t2=stats::rexp(n,rate=lambda[2]),
                 t3=stats::rexp(n,rate=lambda[3])) %>%
        md_series_lifetime() %>%
        md_bernoulli_candidate_C1_C2_C3(m)
    
    loglike.exp <- md_loglike_exp_series_C1_C2_C3(md)
    loglike.scr <- md_score_exp_series_C1_C2_C3(md)
    mle.exp <- mle_gradient_ascent(l=loglike.exp,theta0=lambda,score=loglike.scr)
    mle.boot <- mle_boot_loglike(mle=mle.exp,
                                 loglike.gen=md_loglike_exp_series_C1_C2_C3,
                                 data=md)
    
    cat("Sample size: ", n, "\n")
    cat("---------------\n")
    
    print("Estimate of bias using Bootstrap:")
    print(bias(mle.boot,lambda))
    
    print("Estimate of confidence intervals:")
    print("Asymptotic")
    print(confint(mle.exp))
    print("Bootstrap")
    print(confint(mle.boot))

    print("Estimate of variance-covariance:")
    print("Asymptotic")
    print(vcov(mle.exp))
    print("Bootstrap")
    print(vcov(mle.boot))
    
    print("Estimate of MSE:")
    print("Asymptotic")
    print(mse(mle.exp))
    print("Bootstrap")
    print(mse(mle.boot,lambda))
}
```

The asymptotic mean squared error of $\hat{\v\lambda}^{(1)},\ldots,\hat{\v\lambda}^{(N)}$
is approximately equal to the trace of the inverse of the Fisher information
matrix evaluated at $\v\lambda$, which is given by
```{r,echo=F,eval=F}
sum(diag(MASS::ginv(-numDeriv::hessian(md_loglike_exp_series_C1_C2_C3(md),lambda))))
```

## Candidate sets that generate non-unique MLEs
In some cases, the MLE may be non-unique due to a small sample size.
However, there are cases where as the sample size $n \to \infty$, the MLE
remains non-unique, i.e., it is not guaranteed to converge to any value.
One way in which this can occur for a series system with masked component causes
of failure is given by the following setup.

Let the exponential series system consist of $m=3$ components. Components
$1$ and $2$ are on the same circuit board, and component $3$ is on another
circuit board.
Whenever the series system fails, the potential components that may have caused
the failure is determined by replacing the entire circuit board.
Thus, when compoent $1$ or $2$ fails, the series system is fixed by replacing
the circuit board they both reside on, and thus the candidate set is 
$\{1,2\}$. If component $3$ fails, the candidate set is $\{3\}$.
In this case, the log-likelihood function is given by
$$
\ell(\lambda_1,\lambda_2,\lambda_3) = \eta_3 \log(\lambda_3) +
    \eta_{1 2} \log(\lambda_1+\lambda_2) -
    (\eta_3+\eta_{1 2}) \bar{s} (\lambda_1+\lambda_2+\lambda_3),
$$
where $\eta_3$ denotes the number of observations in which the candidate
set is $\{3\}$ and $\eta_{1 2}$ denotes the number of observations in which
the candidate set is $\{1,2\}$.

Let $\hat{\lambda}_1 = 1$, $\hat{\lambda}_2 = 2$, and $\hat{\lambda}_3 = 3$.
Then,
$$
\ell(1,2,3) = \eta_3 \log(3) +
    \eta_{1 2} \log(1+2) -
    (\eta_3+\eta_{1 2}) \bar{s}(1+2+3).
$$
However, if we interchange the values for $\hat{\lambda}_1$ and $\hat{\lambda}_2$,
we get the same value for the log-likehood function.
Thus, any time $(\hat{\lambda}_1,\hat{\lambda}_2,\hat{\lambda}_3)$ is an MLE, so
is $(\hat{\lambda}_2,\hat{\lambda}_1,\hat{\lambda}_3)$.
This is known as *non-identifiability*, which means that two or more values of
the parameters result in the same likelihood of the observed data.

To find an MLE, we take the derivative of $\ell$ with respect to the parameters,
obtaining the simultaneous equations
\begin{align}
\hat\lambda_1 + \hat\lambda_2 &= \frac{\eta_{1 2}}{(\eta_{1 2} + \eta_3) \bar{s}}\\
\hat\lambda_3                 &= \frac{\eta_3}{(\eta_{1 2} + \eta_3) \bar{s}}
\end{align}
We see that
$$
\hat\lambda_1 + \hat\lambda_2 =\frac{\eta_{1 2}}{(\eta_{1 2} + \eta_3) \bar{s}}
$$
defines a *line*, and thus any point on this line is an MLE of $\hat\lambda_1$
and $\hat\lambda_2$.

As a demonstration of this occurrence, we run $N = 500000$ simulations for an
exponential series system with a true parameter $\v\lambda = (2,3,2.5)'$, starting
at a random location with the parameter space $\Omega$.
In Table \ref{mytable}, we show a small sample of the generated masked data.

```{r mytable, echo=F,cache=T, eval=F}
library(dplyr)
library(ggplot2)
library(series.system.estimation.masked.data)
library(algebraic.mle)
library(cowplot)

n <- 10000
m <- 3
theta <- c(2,3,2.5)

md_block_candidate_m3 <- function(md)
{
    block <- function(k)
    {
        if (k == 1)
            return(c(T,T,F))
        if (k == 2)
            return(c(T,T,F))
        if (k == 3)
            return(c(F,F,T))
    }

    n <- nrow(md)
    x <- matrix(nrow=n,ncol=3)
    for (i in 1:n)
        x[i,] <- block(md$k[i])

    x <- tibble::as_tibble(x)
    colnames(x) <- paste0("x",1:3)
    md %>% dplyr::bind_cols(x)
}

md.nu <- tibble(t1=stats::rexp(n,theta[1]),
                t2=stats::rexp(n,theta[2]),
                t3=stats::rexp(n,theta[3])) %>%
    md_series_lifetime() %>%
    md_block_candidate_m3()

md.nu.tmp <- md.nu
md.nu.tmp$x1 <- as.integer(md.nu.tmp$x1)
md.nu.tmp$x2 <- as.integer(md.nu.tmp$x2)
md.nu.tmp$x3 <- as.integer(md.nu.tmp$x3)
#head(round(md.nu.tmp,digits=3),n=10)

knitr::kable(
    head(md.nu.tmp,n=10),
    caption="The first 10 observations of simulated masked data for exponentially distributed component lifetimes")
```

We find an MLE for each, and do density plots for $\hat\lambda_1$ and
$\hat\lambda_2$ on the left side and $\hat\lambda_3$ on the right side in
Figure \ref{fig:non-unique}.

```{r non-unique,fig.cap="The left figure shows the MLE for lambda1 and lambda2 is centered around the line lambda1 + lambda2 = 5, and the right figure shows that the MLE for lambda3 is highly concentrated around 2.5.",fig.show="hold",echo=F,warning=F,message=F,cache=T,eval=F}
loglike.nu.exp <- md_loglike_exp_series_C1_C2_C3(md.nu)
N <- 1000
loglikes <- numeric(N)
theta.nus <- matrix(nrow=N,ncol=3)
for (i in 1:N)
{
    theta.nu <- mle_gradient_ascent(
        l=loglike.nu.exp,
        theta0=runif(3,.1,10),
        stop_cond=function(x,y) abs(max(x-y)) < 1e-5)

    theta.nus[i,] <- point(theta.nu)
    loglikes[i] <- loglike.nu.exp(point(theta.nu))
}

dat <- data.frame(data.frame(x=theta.nus[,1], y=theta.nus[,2],z=loglikes))
plot1 <-ggplot(dat,aes(x=x,y=y)) +
    #geom_density2d() +
    geom_density_2d_filled() +
    #geom_contour()+
    labs(x="lambda1",y="lambda2") +
    xlim(0,5) +
    ylim(0,5)
dat2 <- data.frame(data.frame(x=theta.nus[,3],z=loglikes))

plot2 <-ggplot(dat2,aes(x=x)) +
    geom_density() +
    labs(x="lambda3") +
    xlim(2,3)

plot_grid(plot1, plot2, labels = "AUTO")
```




References
==========

&nbsp;
<div id="refs"></div>


Appendix {#app}
===============

\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

## Data {#app:data}


<!-- ```{r} -->
<!-- guo_series_weibull_md -->
<!-- ``` -->

## R code {#app:code}

