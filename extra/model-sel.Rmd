---
title: "Model Selection for Reliability Estimation in Series Systems"
author: "Alex Towell"
abstract: "This paper explores model selection for reliability estimation of components in multi-component series systems. A likelihood model is developed to account for right-censoring and candidate sets indicative of masked failure causes. Simulation studies demonstrate the sensitivity of maximum likelihood estimators to modeling assumptions. A reduced model with homogeneous component shapes simplifies analysis but may be inadequate for some systems. Appropriateness of the reduced model is assessed using likelihood ratio tests. Findings suggest the reduced model excels for well-designed systems. More complex models are favored given divergent component properties or large samples. Proper model specification balances simplicity against representativeness."
output:
    #bookdown::html_document2:
    #bookdown::pdf_document2:
    bookdown::gitbook:
        df_print: kable
        citation_package: natbib
    bookdown::pdf_book:
        toc: true
        #toc_depth: 3
        number_sections: true
        #extra_dependencies: ["hyperref", "graphicx","amsthm","amsmath","natbib","tikz"]
        extra_dependencies: ["tikz", "caption"]
        df_print: kable
        #keep_tex: true
        citation_package: natbib
indent: true
header-includes:
   - \usepackage{tikz}
   - \usepackage{caption}
   - \AtBeginDocument{\renewcommand{\v}[1]{\boldsymbol{#1}}}
   - \AtBeginDocument{\newtheorem{condition}{Condition}}
bibliography: refs.bib
link-citations: true
biblio-style: apalike
---


# Introduction

Estimating reliability of individual components in multi-component systems is challenging when only system-level failure data is observable. A likelihood model incorporating right-censoring and candidate sets was previously developed to enable component inference from such masked data [1]. Simulation studies revealed estimator behavior and sensitivity to modeling assumptions given limited samples.

A key question is choosing an appropriate model complexity. A reduced model assuming homogeneous component shapes simplifies analysis as the system becomes Weibull. However, deviations in component properties impact adequacy. Proper model specification requires balancing simplicity against representativeness.

This paper explores model selection for component reliability estimation in series systems. The likelihood model from [1] is summarized. Simulation studies demonstrate estimator sensitivity and assess reduced model appropriateness using likelihood ratio tests. Findings provide guidance on suitable models based on system design and available data.

# Series System Model
Consider a system with m components in series, where each component j has a Weibull distributed lifetime with scale Î»j and shape kj [1]. The system fails when any component fails. The system lifetime distribution is determined by component properties.

Only system failure times are observed, potentially right-censored. For failures, a candidate set indicates possible failed components. This masked data enables component inference despite lack of direct component observations.

A likelihood model for the masked data was derived in [1], accounting for right-censoring and candidate sets. The model relies on assumptions that candidate sets contain the failed component and components are equally likely to be masked given failure time and cause [1].

Maximum likelihood estimation produced accurate results despite small samples and significant masking and censoring [1]. However, shape parameters were more variable and challenging to estimate precisely. A reduced model with homogeneous shapes simplified analysis while reducing estimator variability.

Assessing Model Adequacy
While simpler models are favorable, they must adequately represent available data. The likelihood ratio test (LRT) assesses model fit. The test statistic compares likelihoods under the null (reduced) and alternative (full) models:

$$
\Lambda = -2(lR - lF)
$$

where $l_R$ and $l_F$ are log-likelihoods under reduced and full models respectively. Under standard regularity conditions, the test statistic asymptotically follows a $\chisq^2$ distribution with degrees of freedom equal to the difference in model parameters [2].

If $\Lambda$ exceeds a critical value, the null model is rejected. Otherwise, there is insufficient evidence against the null. Larger samples provide greater power to detect lack of fit.

# Loglikelihood Function
We denote the full model log-likelihood function by $\ell_F$ and the reduced model log-likelihood
by $\ell_R$. The reduced model is obtained by setting the shape parameter of each component to
be the same, i.e., $k_1 = \cdots = k_m = k$. Thus, the reduced model log-likelihood function is given by
$$
\ell_R(k, \lambda_1, \lambda_2, \cdots, \lambda_m) =
        \ell_F(k, \lambda_1, k, \lambda_2, \ldots, k, \lambda_m),
$$
The same may be done for the score and hessian of the log-likelihood functions.


# Simulation Study
A simulation study was conducted to demonstrate estimator sensitivity and explore reduced model adequacy using LRTs under various scenarios [1].

The base system had 5 Weibull components with similar but non-identical failure characteristics, representing a well-designed system without a single weak point. Samples were generated under moderate right-censoring and masking probabilities for the component cause of failure.

Varying sample size n showed reductions in estimator dispersion and confidence interval width with more observations. However, small samples exhibited bias for shape parameters. Scale parameters were more robust.

Increasing the masking probability expanded confidence intervals to maintain coverage, with scale parameters remaining unbiased up to significant masking. Shape parameters were more affected.

Reduced model adequacy was assessed by manipulating the shape parameter k3 of one component to make failure characteristics diverge. The LRT detected misfit with sufficient power only when $k_3$ substantially deviated from the well-designed case. For the nominal system, large samples did not produce evidence against the reduced model, supporting its adequacy in that case.

# Discussion
The simulation studies demonstrated estimator sensitivity but overall robust performance given the significant challenges introduced by limited data. Bootstrap confidence intervals proved valuable for characterizing uncertainty.

The reduced model with homogeneous shapes showed excellent fit for well-designed systems, even with extensive data. This supports employing the simplified model for such systems. Deviations in component properties quickly provided evidence against the reduced model. More complex models are then desirable, particularly with large samples.

In practice, the choice between models entails weighing simplicity against representativeness. Well-founded reduced models excel for well-designed systems. But data inconsistencies or divergent components motivate fuller models. Proper specification requires understanding system characteristics and estimator behavior.

By providing guidance on suitable models, this work improves component reliability assessment from limited system failure data. Further studies on modeling extensions were suggested in [1]. The methods enable quantifying latent component properties when failures are significantly masked.

## Scenario: Assessing the Impact of Changing the Scale Parameter of Component 3 {#scale-vs-mttf}

By Equation \@ref(eq:mttf-weibull), we see that MTTF$_j$ is proportional to the scale parameter $\lambda_j$, which means
when we decrease the scale parameter of a component, we proportionally decrease the MTTF.
In this scenario, we start with the well-designed series system described in Table \ref{tab:series-sys},
and we will manipulate the MTTF of component 3, MTTF$_3$, by changing its
scale parameter, $\lambda_3$, and observing the effect this has on the MLE. Since the other components
had a similiar MTTF, we will arbitrarily choose component 1 to represent the other components.
The bottom plot shows the coverage probabilities for all parameters.

In Figure \ref{fig:mttf-vs-ci}, we show the effect of changing the scale parameter of component $3$, $lambda_3$,
but map $\lambda_3$ to MTTF$_3$ to make it more intuitive to reason about. We vary the MTTF of component 3
from $300$ to $1500$ and the other components have their MTTFs fixed at around $900$, as shown in
Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
the right-censoring quantile to $q = 0.825$ (moderate censoring), and the sample size to $n = 100$ (moderate sample size).

```{r mttf-vs-ci, fig.cap=c("MTTF$_3$ vs MLE By Varying Scale", "Mean-Time-To-Failure (MTTF) of Component 3 by Varying Its Scale Parameter"), fig.align="center", echo = F}
# knitr::include_graphics("image/plot-scale3-vs-stats.pdf")
# knitr::include_graphics("image/5_system_scale3_vary.pdf")
knitr::include_graphics("image/5_system_mttf3_by_scale3.pdf")
```

### Key Observations

##### Coverage Probability (CP) {-}
When MTTF of component 3 is much smaller than other components,
the CP for $k_3$ is very well calibrated (approximately obtaining the nominal level $95\%$)
while the CP for other componentns are around $90\%$, which is still reasonable.
(This is the case even though the width of the CI for $k_3$ is extremely narrow compared to the others).
As MTTF$_3$ increases, the CP for $k_3$ decreases, while the CP for the other components increase
slightly. The scale parameters are generally well-calibrated for all of the components, except
for component 3 when its MTTF is large and it dips down to $90\%$. Despite the individual differences,
the mean of the CPs for shape and scale parameters hardly change.

##### Dispersion of MLEs {-}
For component 3, as its MTTF decreases, the dispersion of MLEs narrows,
indicating more precise estimates. Conversely, dispersion for other components widens. As MTTF
of component 3 increases, its dispersion widens while others narrow. This is consistent with
the fact that the smaller MTTF of component 3 means that, in this well-designed system at least,
it is more likely to be the component cause of failure, and so we have more information about
its parameters and are able to estimate them more accurately.

##### IQR of Bootstrapped CIs {-}
The dark blue vertical lines representing IQR are consistent with the dispersion of MLEs,
which is the ideal behavior, and suggests that the BCa confidence intervals are performing well.

##### Bias of MLEs {-}
For component 3, the bias of MLE for the scale parameter becomes slightly more negatively biased
as MTTF$_3$ increases, and the bias of the MLE for the shape parameter becomes slightly more positively
biased. The MLE for the shape and scale parameters for component 1 have a very small bias, if any,
and are not affected by the MTTF$_3$. The scale parameters are easier to estimate than the shape
parameters, and so they are less sensitive to changes in scale than the shape parameters, as
we will show in the next scenario.

## Scenario: Assessing the Impact of Changing the Shape Parameter of Component 3 {#shape3-vary}

The shape parameter determines the failure characteristics.
We vary the shape paramenter of component 3 from $0.1$ to $3.5$ and observe the effect
it has on the MLE.
When $k_3 < 1$, this indicates infant mortality, and when $k_3 > 1$, this indicates
wear-out failures.

We analyze the effect of component 3's shape parameter on the MLE and the bootstrapped confidence intervals for the
shape and scale parameters of components 1 and 3 (the component we are varying). First, we look at the effect
on the scale parameter.


```{r prob3-vs-mle, out.width="100%", fig.cap=c("Probability of Component 3 Failure vs MLE"), fig.align="center", echo = F}
knitr::include_graphics("image/5_system_shape3_fig.pdf")
```

### Key Observations

##### Coverage Probability (CP) {-}
The CP for the scale parameters
are well-calibrated and close to the nominal level of $0.95$ for all values of $\Pr\{K_i = 3\}$.
For the the shape parameter of component 3 ($k_3$) in
bold orange colors, we see that it is well-calibrated for all values of
$\Pr\{K_i = 3\}$, but actually may become too large for extreme values of $\Pr\{K_i = 3\}$.
The CP for the shape parameters of the other components decreases with $$\Pr\{K_i = 3\}$, dipping below $90\%$ for $\Pr\{K_i = 3\} > 0.4$. At a sample size of $n = 100$, the CP for the shape parameters of the other components is generally not well-calibrated for $\Pr\{K_i = 3\} > 0.4$.

##### Dispersion of MLEs {-}
The dispersion of the MLE for the shape and scale parameters of component 1, $k_1$ and $\lambda_1$,
is fairly steady but begins to increase rapdily at the extreme values of $\Pr\{K_i = 3\}$. This is indicative of
having less information about the failure characteristcs of component $1$ as component $3$ begins to dominate the
component cause of failure.
The dispersion of the shape parameter $k_3$ is initially quite large, indicative of having very little
information about the failure characteristcs of component 3 since it is unlikely to be the component cause of
failure, but its dispersion rapidly decreases as $\Pr\{K_i = 3\}$ increases and more information is
available about component 3's failure characteristics. In fact, it nearly becomes a point at $\Pr\{K_i = 3\} = 0.6$.
The dispersion of the the scale parameter of component $3$, $\lambda_1$, is quite steady and is less spread out
than the MLE for $\lambda_1$, but at extreme values of $\Pr\{K_i = 3\}$, it also begins to rapidly increase,
suggesting some complex interactions between the shape and scale parameters of component 3.

##### IQR of Bootstrapped CIs {-}
The CIs precisely track the dispersion of the MLEs, which is the ideal behavior,
and suggests that the BCa confidence intervals are performing well.

##### Bias of MLEs {-}
The MLE for the scale parameters are nearly unbiased and generally seem unaffected by changes
in $\Pr\{K_i = 3\}$. As $\Pr\{K_i = 3\}$ increases the MLE is adjusting $k_1$ to be more
positively biased, decreasing its infant morality rate to make it less likely to be the
component cause of failure, and adjusting $k_3$ to be less positively biased, increasing
its infant mortality rate, to make it more likely to be the component cause of failure.



# Simulation Study: Full Weibull Model vs Reduced (Homogenous Shape) Model {#full-vs-reduced}

In Section \@ref(weibull), we developed the full model with Weibull
components and we conducted a sensitivity analysis of its MLE in
Section \@ref(sim-study). Here, our focus shifts to a sensitivity
analysis aimed at understanding when it is appropriate to use the reduced model
that assumes homogeneity in the shape parameters of the components, as described
in Section \@ref(reduced-weibull). The reduced model offers interpretability
(the series system is itself Weibull) and reduced estimator variability (only
$m+1$ parameters instead of $2m$), but it is must adequately describe the data.

We aim to assess the appropriateness of the reduced model under varying
sample sizes and shape parameters of the third component ($k_3$). We employ
a simulation study using the likelihood ratio test (LRT) for this purpose,
where the null hypothesis, $H_0$, assumes homogenous shape parameters.

We take the well-designed series system described in Table \ref{tab:series-sys},
and manipulate the shape parameter of the third
component ($k_3$) to cause the components to have different failure characteristics.
Recall that $k_3 = 1.1308$ corresponds to a *well-designed* series system, where
component shapes are reasoanbly aligned. We also vary the sample size $n$ to assess
the impact of sample size on the appropriateness of the reduced model.

Figure \ref{fig:lrt-contour} provides a contour plot with varying 
sample sizes along the $x$-axis, shape of component 3 along the $y$-axis,
and median $p$-value along the color scale.
The contour lines corresponding to $p$-values of $0.05$ and $0.1$ are
often used as a threshold for statistical significance. Points outside
of these contours in dark blue are indicative of significant evidence
against the null model and points inside of these contours in light blue
for $0.1$ and green for $0.05$ are indicative of insufficient evidence
against the null model.

Figure \ref{fig:lrt-samp-size} provides a plot of the median $p$-value
against the sample size for the well-designed system, where the shape
parameter of component 3 is fixed at $1.1308$. The $95$th percentile
of the $p$-values is also provided as a more stringent criterion for
statistical significance.

```{r samp-size-shape-lrt, results='asis', echo = F}
cat('
\\begin{figure}
    \\centering
    \\begin{minipage}{.5\\textwidth}
        \\centering
        \\includegraphics[width=1\\linewidth]{image/fig-lrt/contour_plot.pdf}
        \\captionof{figure}{$p$-Value vs Sample Size and Shape $k_3$}
        \\label{fig:lrt-contour}
    \\end{minipage}%
    \\begin{minipage}{.5\\textwidth}
        \\centering
        \\includegraphics[width=1\\linewidth]{image/fig-lrt/n-vs-p-value.pdf}
        \\captionof{figure}{$p$-Value vs Sample Size for Well-Designed System}
        \\label{fig:lrt-samp-size}
    \\end{minipage}
\\end{figure}
')
```

#### Sensitivity to Sample Size ($n$) {-}

- The sample size is an essential aspect of hypothesis testing, as it affects the
test's power, which is the probability of correctly rejecting the null hypothesis
when it is false. In the contour plot in Figure \ref{fig:lrt-contour}, as $n$
increases, the contours trend lower.
This indicates that larger samples result in smaller median $p$-values,
implying that the power of the LRT increases with the sample size.
However, its power is quite low for small samples, particularly for values of
$k_3$ somewhat close to the shape parameters of the other components in the system.

- Recall that in the well-designed series system, $k_3 = 1.1308$. In this case,
even very large sample sizes do not produce evidence
against the null model, indicating robust compatibility.

- In Figure \ref{fig:lrt-samp-size}, we fix $k_3$ at $1.1308$ and vary the sample size. 
The median $p$-value only manages to drop below the $0.05$ theshold with
sample sizes around $10000$. In the more stringent criterion given
by the $95$th percentile of the $p$-values, nearly $30000$ observations are
necessary to reject the null hypothesis in $95\%$ of the simulations.

#### Sensivity to Shape Parameter ($k_3$) {-}

- In Figure \ref{fig:lrt-contour}, for a given shape parameter, increasing the
sample size tends to decrease the median $p$-value. Larger samples provide
more information about the parameters, which increases the power of the LRT.

- The median $p$-values in the vicinity of the line $k_3 = 1.1308$ are
high across various sample sizes, indicating that the null model is a good fit.
As $k_3$ deviates from this line, the median $p$-value diminishes,
indicating increasing evidence against the null model.

## Implications and Recommendations

The power of the test for a well-designed series system is quite
low, requiring many thousands of observations before the test has
sufficient power to reject the null hypothesis. But, this is not
necessarily a bad thing. The reduced model is quite simple and
interpretable, and is by definition a good fit for a well-designed
series system. 

The findings suggest that the reduced model is particularly apt
when the system is well-designed, even for very large samples.
Practitioners should weigh the trade-offs between the simplicity
of the reduced model and the adequacy in describing the data,
with consideration of the available sample size and the characteristics
of the system being modeled.

For systems believed to be well-designed, employing the null model is
supported both statistically and practically due to its simplicity,
reduced estimator variability, and analytical tractability.
In the absence of prior information, or if the shape parameter
significantly diverges from the well-designed value, the choice between
models should be undertaken with caution. More complex models may be
favorable, especially with large sample sizes.


# Likelihood Ratio Test {#app-lrt}

In order to determine if a reduced model (e.g., Weibull series system in
which all of the shape parameters are homogeneous) is appropriate,
a hypothesis test test may be conducted to determine if there is
statistically significant evidence in support of the null hypothesis $H_0$,
e.g., that all of the shape parameters are homogeneous.

Given that we employ a well-defined likelihood model, the likelihood ratio test
(LRT) is a good choice. The LRT statistic is given by
$$
\Lambda = -2 (\ell_R - \ell_F)
$$
where $\ell_R$ is the log-likelihood of the null (reduced) model (the log-likelihood of
the reduced model evaluated at its MLE given a random sample) and $\ell_F$ is the log-likelihood
of the full model. Under the null model, the LRT statistic is asymptotically distributed chi-squared
with $m-1$ degrees of freedom, where $m$ is the number of components in the series system,
$$
\Lambda \sim \chi^2_{m-1}.
$$
If the LRT statistic is greater than the critical value of the chi-squared distribution with $m-1$
degrees of freedom, $\chi^2_{m-1, 1-\alpha}$, where $\alpha$ denotes the significance level, then
we find the data to be incompatible with the null hypothesis $H_0$.





# Conclusion

A reduced model assuming homogeneous component shape parameters
was also assessed. This simplified model improves interpretability
as the system becomes Weibull, while reducing estimator variability.
The appropriateness of the reduced model was explored through an
LRT simulation study. For well-designed systems, the reduced model
showed excellent fit even for large samples. However, varying the
scale or shape of a single component quickly provided evidence
against the null model.

The reduced model thus appears favorable both statistically and
practically when systems are believed to be well-designed. However,
more complex models may be preferred given divergent component
properties or large samples. The choice between models should weigh
model simplicity against adequacy in describing available data.
