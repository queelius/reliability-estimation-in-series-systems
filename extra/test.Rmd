---
title: "Bootstrapping statistics of the maximum likelihood estimator of components in a series systems from masked failure data"
author: "Alex Towell"
abstract: "We estimate the parameters of a series system with Weibull component lifetimes from relatively small samples consisting of right-censored system lifetimes and masked component cause of failure. Under a set of conditions that permit us to ignore how the component cause of failures are masked, we assess the bias and variance of the estimator. Then, we assess the accuracy of the boostrapped variance and calibration of the confidence intervals of the MLE under a variety of scenarios."
output:
    pdf_document:
        toc: yes
        toc_depth: 2
        number_sections: true
        extra_dependencies: ["graphicx","amsthm","amsmath","natbib","tikz"]
        df_print: kable
        keep_tex: true
indent: true
bibliography: refs.bib
csl: the-annals-of-statistics.csl
---

\newcommand{\T}{T}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{condition}{Condition}
\renewcommand{\v}[1]{\boldsymbol{#1}}
\numberwithin{equation}{section}

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# cran packages
cran_packages <- c("readr", "latex2exp", "tidyverse", "devtools", "dplyr", "ggplot2", "gridExtra", "grid", "glue", "png")
github_packages <- c("queelius/md.tools", "queelius/wei.series.md.c1.c2.c3", "queelius/algebraic.mle")

for (pkg in cran_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg)
    }
    eval(parse(text = paste0("library(", pkg, ")")))
}
for (pkg in github_packages) {
    if (!require(pkg, quietly = TRUE)) {
        install_github(pkg)
    }
    # strip the namespace
    pkg <- gsub(".*\\/", "", pkg)
    eval(parse(text = paste0("library(", pkg, ")")))
}
```

Introduction
============
Accurately estimating the reliability of individual components in multi-component systems is an important problem in many engineering domains. However, component lifetimes and failure causes are often not directly observable. In a series system, only the system-level failure time may be recorded along with limited information about which component failed. Such *masked* data poses challenges for estimating component reliability. 

In this paper, we develop a maximum likelihood approach to estimate component reliability in series systems using right-censored lifetime data and candidate sets that contain the failed component. The key contributions are:

1. Deriving a likelihood model that accounts for right-censoring and masked failure causes through candidate sets. This allows the available masked data to be used for estimation.

2. Validating the accuracy, precision, and robustness of the maximum likelihood estimator through an extensive simulation study under different sample sizes, masking probabilities, and censoring levels. 

3. Demonstrating that bootstrapping provides well-calibrated confidence intervals for the MLEs even with small samples.

Together, these contributions provide a statistically rigorous methodology for learning about latent component properties from series system data. The methods are shown to work well even when failure information is significantly masked. This capability expands the range of applications where component reliability can be quantified from limited observations.

The remainder of this paper is organized as follows. First, we detail the series system and masked data models. Next, we present the likelihood construction and maximum likelihood theory. We then describe the bootstrap approach for variance and confidence interval estimation. Finally, we validate the methods through simulation studies under various data scenarios and sample sizes.

Series System Model {#sec:statmod}
==================================
We consider a system composed of $m$ components arranged in a series configuration. Each component and system has two possible states, functioning or failed.
We have $n$ systems whose lifetimes are independent and identically distributed (i.i.d.).
The lifetime of the $i$\textsuperscript{th} system denoted by the random variable $T_{i}$.
The lifetime of the $j$\textsuperscript{th} component in the $i$\textsuperscript{th} system is denoted by the random variable $T_{i j}$.
We assume the component lifetimes in a single system are statistically independent and non-identically distributed.
Here, lifetime is defined as the elapsed time from when the new, functioning component (or system) is put into operation until it fails for the first time.
A series system fails when any component fails, thus the lifetime of the $i$\textsuperscript{th} system is given by the component with the shortest lifetime,
$$
    \T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
$$

There are three particularly important distribution functions in survival analysis: the
survival function, the probability density function, and the hazard function.
The survival function, $R_{\T_i}(t)$, is the
probability that the $i$\textsuperscript{th} system has a lifespan larger than a duration $t$,
\begin{equation}
R_{\T_i}(t) = \Pr\{\T_i > t\}\\
\end{equation}
The probability density function (pdf) of $T_i$ is denoted by
$f_{T_i}(t)$ and may be defined as
$$
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
$$
Next, we introduce the hazard function.
The probability that a failure occurs between $t$ and $\Delta t$ given that no
failure occurs before time $t$ is given by
$$
\Pr\{\T_i \leq t+\Delta t|T_i > t\} = \frac{\Pr\{t < T_i < t+\Delta t\}}{\Pr\{T_i > t\}}.
$$
The failure rate is given by the dividing this equation by the length of the time
interval, $\Delta t$:
$$
\frac{\Pr\{t < T < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T > t\}} =
    \frac{R_T(t) - R(t+\Delta t)}{R_T(t)}.
$$
The hazard function $h_{\T_i}(t)$ for $T_i$ is the instantaneous failure rate at time $t$, which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_{T_i}(t) &= \lim_{\Delta t \to 0} \frac{\Pr\{t < \T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{\T_i > t\}}\\
       &= \frac{f_{T_i}(t)}{R_{T_i}(t)}.
\end{split}
\end{equation}
\end{definition}

The lifetime of the $j$\textsuperscript{th} component is assumed to follow a parametric distribution indexed
by a parameter vector $\v{\theta_j}$. The parameter vector of the overall system is defined as
$$
    \v\theta = (\v{\theta_1},\ldots,\v{\theta_m}).
$$

When a random variable $T$ is parameterized by a particular $\v\theta$, we denote the
reliability function by $R_T(t;\v\theta)$, and the same for other distribution functions.
If it is clear from the context which random variable a distribution function is for, we
drop the subscripts, e.g., $R(t)$ instead of $R_T(t)$.
As a special case, we denote the pdf of the $j$\textsuperscript{th} component by
$f_j(t;\v{\theta_j})$ and its reliability function by $R_j(t;\v{\theta_j})$.

Two random variables $X$ and $Y$ have a joint pdf $f_{X,Y}(x,y)$.
Given the joint pdf $f(x,y)$, the marginal pdf of $X$ is given by
$$
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
$$
where $\mathcal{Y}$ is the support of $Y$. (If $Y$ is discrete, replace
the integration with a summation over $\mathcal{Y}$.)

The conditional pdf of $Y$ given $X=x$, $f_{Y|X}(y|x)$, is defined as
$$
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
$$
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of $X_1,\ldots,X_m$ is denoted by $f(x_1,\ldots,x_m)$.

Next, we dive deeper into these concepts and provide mathematical derivations for
the reliability function, pdf, and hazard function of the series system.
We begin with the reliability function of the series system, as given by the following theorem.
\begin{theorem}
\label{thm:sys_reliability_function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
  R(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The reliability function is defined as
$$
  R(t;\v\theta) = \Pr\{\T_i > t\}
$$
which may be rewritten as
$$
  R(t;\v\theta) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R(t;\v\theta) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R(t;\v\theta) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\v\theta) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
\end{proof}

Theorem \ref{thm:sys_reliability_function} shows that the system's overall reliability is the product of the reliabilities of its individual components. This property is inherent to series systems and will be used in the subsequent derivations.

Next, we turn our attention to the pdf of the system lifetime, described in the following theorem.
\begin{theorem}
\label{thm:sys_pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f(t;\v\theta) = \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
By definition, the pdf may be written as
$$
    f(t;\v\theta) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f(t;\v\theta)
    &= -\frac{d}{dt} R_1(t;\v{\theta_1})\prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j})\\
    &= f_1(t;\v\theta) \prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f(t;\v\theta) = \sum_{j=1}^{m-1} f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\v{\theta_j}) \frac{d}{dt} R_m(t;\v{\theta_m}),
$$
which simplifies to
$$
f(t;\v\theta)= \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}).
$$
\end{proof}
Theorem \ref{thm:sys_pdf} shows the pdf of the system lifetime as a function of the pdfs and reliabilities of its components.

We continue with the hazard function of the system lifetime, defined in the next theorem.
\begin{theorem}
\label{thm:sys_failure_rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys_failure_rate}
  h(t;\v\theta) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{equation}
\end{theorem}
\begin{proof}
The $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h(t;\v\theta) = \frac{f_{\T_i}(t;\v\theta)}{R_{\T_i}(t;\v\theta)}.
$$
Plugging in expressions for these functions results in
$$
  h(t;\v\theta) = \frac{\sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k})}
      {\prod_{j=1}^m R_j(t;\v{\theta_j})},
$$
which can be simplified to
\begin{align*}
h_{T_i}(t;\v\theta)
    &= \sum_{j=1}^m \frac{f_j(t;\v{\theta_j})}{R_j(t;\v{\theta_j})}\\
    &= \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{align*}
\end{proof}

Theorem \ref{thm:sys_failure_rate} reveals that the system's hazard function is the sum of the hazard functions of its components.

By definition, the hazard function is the ratio of the pdf to the reliability
function,
$$
h(t;\v\theta) = \frac{f(t;\v\theta)}{R(t;\v\theta)},
$$
and we can rearrange this to get
\begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f(t;\v\theta) &= h(t;\v\theta) R(t;\v\theta)\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\v{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\v{\theta_j}) \biggr\},
\end{split}
\end{equation}
which we sometimes find to be a more convenient form than Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's reliability function, pdf, and hazard function. Next, we build upon these concepts to derive distributions related to the component cause of failure.

## Component Cause of Failure {#sec:comp_cause}
Whenever a series system fails, precisely one of the components is the cause.
We model the component cause of the series system failure as a random variable.
\begin{definition}
The component cause of failure of a series system is
denoted by the random variable $K_i$ whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.
\end{definition}
Note that a more succinct way to define $K_i$ is given by
$$
K_i = \operatorname{argmin}_j \bigl\{ T_{i j} : j \in \{1,\ldots,m\}\bigr\}.
$$

The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.
\begin{theorem}
\label{thm:f_k_and_t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$\T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) R_{\T_i}(t;\v\theta),
\end{equation}
where $h_j(t;\v{\theta_j})$ is the hazard function of the $j$\textsuperscript{th}
component and $R_{\T_i}(t;\v\theta)$ is the reliability function of the series
system.
\end{theorem}
\begin{proof}
Consider a $3$-out-of-$3$ system.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\v\theta) = \prod_{j=1}^{3} f_j(t;\v{\theta_j}).
$$
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j;\v\theta) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2}) f_3(t_3;\v{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2})
        R_3(t;\v{\theta_3}) dt_2\\
     &= f_1(t;\v{\theta_1}) R_2(t;\v{\theta_2}) R_3(t_1;\v{\theta_3}).
\end{align*}
Since $h_1(t;\v{\theta_1}) = f_1(t;\v{\theta_1}) / R_1(t;\v{\theta_1})$,
$$
f_1(t;\v{\theta_1}) = h_1(t;\v{\theta_1}) R_1(t;\v{\theta_1}).
$$
Making this substitution into the above expression for $f_{K_i,T_i}(j,t;\v\theta)$
yields
\begin{align*}
f_{K_i,T_i}(j,t;\v\theta)
    &= h_1(t;\v{\theta_1}) \prod_{l=1}^m R_l(t;\v{\theta_l})\\
    &= h_1(t;\v{\theta_1}) R(t;\v{\theta}).
\end{align*}
Generalizing from this completes the proof.
\end{proof}

Likelihood Model for Masked Data {#sec:like_model}
==================================================

The object of interest is the (unknown) parameter value $\v\theta$. 
To estimate this $\v\theta$, we need *data*.
In our case, we call it *masked data* because we do not necessarily observe
the event of interest, say a system failure, directly.
We consider two types of masking: masking the system failure lifetime and
masking the component cause of failure.

We generally encounter three types of system failure lifetime masking:

1. A system failure is observed at a particular point in time.
2. A system failure is observed to occur within a particular interval of time.
3. A system failure is not observed, but we know that the system survived at least
   until a particular point in time. This is known as *right-censoring*
   and can occur if, for instance, an experiment is terminated while the system
   is still functioning.


We generally encounter two types of component cause of failure masking:

1. The component cause of failure is observed.
2. The component cause of failure is not observed, but we know that the failed
   component is in some set of components. This is known as *masking* the
   component cause of failure.

Thus, the component cause of failure masking will take the form of candidate sets. A
candidate set consists of some subset of component labels that plausibly contains the
label of the failed component.
The sample space of candidate sets are all subsets of $\{1,\ldots,m\}$, thus
there are $2^m$ possible outcomes in the sample space.

In this paper, we limit our focus to observing *right censored* lifetimes and exact
lifetimes but with masked component cause of failures.
We consider a sample of $n$ i.i.d. series systems, each of
which is put into operation at some time and and observed until either it fails
or is right-censored.
We denote the right-censoring time of the $i$\textsuperscript{th} system by
$\tau_i$. 
We do not directly observe the system lifetime, $\T_i$, but rather, we observe
the right-censored lifetime, $S_i$, which is given by
\begin{equation}
    S_i = \min\{\tau_i, \T_i\},
\end{equation}
We also observe a right-censoring indicator, $\delta_i$, which is given by
\begin{equation}
    \delta_i = 1_{\T_i < \tau_i}
\end{equation}
where $1_{\text{condition}}$ is an indicator function that outputs $1$ if
*condition* is true and $0$ otherwise.
Here, $\delta_i = 1$ indicates the event of interest, a system failure, was
observed.

If a system failure lifetime is observed, then we also observe a candidate set
that contains the component cause of failure. We denote the candidate set for
the $i$\textsuperscript{th} system by $\mathcal{C}_i$, which is a subset of
$\{1,\ldots,m\}$.
Since the data generating process for candidate sets may be subject to chance
variations, it as a random set.

Consider we have an independent and identically distributed (i.i.d.) random sample of masked data,
$D = \{D_1, \ldots, D_n\}$, where each $D_i$ contanis the following:

- $S_i$, the system lifetime of the $i$\textsuperscript{th} system.
- $\delta_i$, the right-censoring indicator of the $i$\textsuperscript{th} system.
- $\mathcal{C}_i$, the set of candidate component causes of failure for the
  $i$\textsuperscript{th} system.

The masked data generation process is illustrated by Figure \ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$D_i = (S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data $D$ for exact, right-censored system failure times with
candidate sets that mask the component cause of failure can be seen in Table 1
for a series system with $m=3$ components.

System | Right-censoring time ($S_i$) | Right censoring indicator ($\delta_i$) | Candidate set ($\mathcal{C}_i$) |
------ | --------------------------- | --------------------------- | --------------------- |
   1   | $4.3$                       | 1                           | $\{1,2\}$             |
   2   | $1.3$                       | 1                           | $\{2\}$               |
   3   | $5.4$                       | 0                           | $\emptyset$           |
   4   | $2.6$                       | 1                           | $\{2,3\}$             |
   5   | $3.7$                       | 1                           | $\{1,2,3\}$           |
   6   | $10$                        | 0                           | $\emptyset$           |

: Right-censored lifetime data with masked component cause of failure.

In statistical modeling, the likelihood is a powerful tool that allows us to estimate the parameters of a model given observed data. In more mathematical terms, the likelihood function quantifies how probable our observed data are for various possible values of the parameters in a statistical model. We denote these parameters as $\v\theta$ in this discussion. 

In our model, we assume the data is governed by a pdf, which is determined by
a specific parameter, represented as $\v\theta$ within the parameter space $\v\Omega$.
The joint pdf of the data $D$ can be represented as follows:
$$
f(D ; \v\theta) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\v\theta),
$$
where $s_i$ is the observed system lifetime of the $i$\textsuperscript{th} system,
$\delta_i$ is the observed right-censoring indicator of the $i$\textsuperscript{th} system,
and $c_i$ is the observed candidate set of the $i$\textsuperscript{th} system.

This joint pdf tells us how likely we are to observe the particular data, $D$, given
the parameter $\v\theta$. When we keep the data constant and allow the parameter
$\v\theta$ to vary, we obtain what is called the likelihood function $L$, defined as
$$
L(\v\theta) = \prod_{i=1}^n L_i(\v\theta)
$$
where
$$
L_i(\v\theta) = f(s_i,\delta_i,c_i;\v\theta)
$$
is the likelihood contribution of the $i$\textsuperscript{th} system. In other words,
the likelihood function quantifies how likely different parameter values $\v\theta$
are, given the observed data.

For each type of data, right-censored data and masked component cause of
failure data, we will derive the *likelihood contribution* $L_i$, which refers to the
part of the likelihood function that this particular piece of data contributes to.

We present the following theorem for the likelihood contribution model.
\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\v\theta) =
\begin{cases}
    R_{\T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{\T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1,
\end{cases}
\end{equation}
where $\delta_i = 0$ indicates the $i$\textsuperscript{th} system is
right-censored at time $s_i$ and $\delta_i = 1$ indicates the $i$\textsuperscript{th} system
is observed to have failed at time $s_i$ and the component cause of failure
is masked by the candidate set is $c_i$.
\end{theorem}

In the follow subsections, we prove this result for each type of masked data, right-censored
system lifetime data $(\delta_i = 0)$ and masking of the component cause of failure
$(\delta_i = 1)$.

## Masked Component Cause of Failure {#sec:candmod}
Suppose a diagnostician is unable to identify the precise component cause of the
failure, e.g., due to cost considerations he or she replaced multiple components
at once, successfully repairing the system but failing to precisely identity
the failed component.
In this case, the cause of failure is said to be *masked*.

The unobserved component lifetimes may have many covariates, like ambient
operating temperature, but the only covariate we observe in our masked data
model are the system's lifetime and additional masked data in the form of
a candidate set that is somehow correlated with the unobserved component
lifetimes.

The key goal of our analysis is to estimate the parameters, $\v{\theta}$, which 
maximize the likelihood of the observed data, and to estimate the precision and
accuracy of this estimate using the Bootstrap method.

To achieve this, we first need to
assess the joint distribution of the system's continuous lifetime, $\T_i$, and the
discrete candidate set, $\mathcal{C}_i$, which can be written as
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = f_{\T_i}(t_i;\v{\theta})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i\},
$$
where $f_{\T_i}(t_i;\v{\theta})$ is the pdf of $\T_i$ and
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i\}$ is the conditional
pmf of $\mathcal{C}_i$ given $\T_i = t_i$.

We assume the pdf $f_{\T_i}(t_i;\v{\theta})$ is known, but we do not have knowledge
of $\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i\}$, i.e., the data generating
process for candidate sets is unknown.

However, it is critical that the masked data, $\mathcal{C}_i$, is correlated with the
$i$\textsuperscript{th} system. This way, the conditional distribution of $\mathcal{C}_i$
given $\T_i = t_i$ may provide information about $\v{\theta}$, despite our Statistical
interest being primarily in the series system rather than the candidate sets.

To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which $\mathcal{C}_i$ is correlated with the
$i$\textsuperscript{th} system is given by assuming the following condition.
\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}
Assuming Condition \ref{cond:c_contains_k}, $\mathcal{C}_i$ must contain the
index of the failed component, but we can say little else about what other
component indices may appear in $\mathcal{C}_i$.

In order to derive the joint distribution of $\mathcal{C}_i$ and $\T_i$ assuming
Condition \ref{cond:c_contains_k}, we take the following approach.
We notice that $\mathcal{C}_i$ and $K_i$ are statistically dependent.
We denote the conditional pmf of $\mathcal{C}_i$ given $\T_i = t_i$ and
$K_i = j$ as
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | \T_i = t_i, K_i = j\}.
$$

Even though $K_i$ is not observable in our masked data model, we can still
consider the joint distribution of $\T_i$, $K_i$, and $\mathcal{C}_i$.
By Theorem \ref{thm:f_k_and_t}, the joint pdf of $\T_i$ and $K_i$ is given by
$$
f_{\T_i,K_i}(t_i,j;\v{\theta}) = h_j(t_i;\v{\theta_j}) R_{\T_i}(t_i;\v\theta),
$$
where $h_j(t_i;\v{\theta_j})$ is the hazard function for the
$j$\textsuperscript{th} component and $R_{\T_i}(t_i;\v{\theta})$ is the
reliability function of the system.
Thus, the joint pdf of $\T_i$, $K_i$, and $\mathcal{C}_i$ may be written as
\begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{\T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta})
    &= f_{\T_i,K_i}(t_i,k;\v{\theta}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\v{\theta_j}) R_{\T_i}(t_i;\v\theta)
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}.
\end{split}
\end{equation}
We are going to need the joint pdf of $\T_i$ and $\mathcal{C}_i$, which
may be obtained by summing over the support $\{1,\ldots,m\}$ of $K_i$ in
Equation \eqref{eq:joint_pdf_t_k_c},
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{\T_i}(t_i;\v\theta)
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
    \biggr\}.
$$
By Condition \ref{cond:c_contains_k},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\} = 0$ when $K_i = j$ and
$j \notin c_i$, and so we may rewrite the joint pdf of $T_i$ and
$\mathcal{C}_i$ as
\begin{equation}
\label{eq:part1}
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of $\v\theta$ (see Section \ref{sec:mle}), we
solve the simultaneous equations of the MLE and choose a solution
$\hat{\v\theta}$ that is a maximum for the likelihood function.
When we do this, we find that $\hat{\v\theta}$ depends on the unknown
conditional pmf $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}$.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}$.

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|\T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}
According to [@Fran-1991], in many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis. In this
setting, Condition \ref{cond:equal_prob_failure_cause} generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$ may be factored out of the
summation in Equation \eqref{eq:part1}, and thus the joint pdf of $T_i$ and
$\mathcal{C}_i$ may be rewritten as
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j})
$$
where $j' \in c_i$.

If $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ is a function of
$\v{\theta}$, the MLEs are still dependent on the unknown
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ to find an MLE of
$\v{\theta}$. The last condition we assume achieves this result.
\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $\T_i$ and component cause
of failure $K_i$ are not functions of $\v{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $\T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\v\theta$.
\end{condition}

When Conditions \ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause},
and \ref{cond:masked_indept_theta} are satisfied, the joint pdf of $\T_i$ and
$\mathcal{C_i}$ is given by
$$
f_{\T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \beta_i R_{\T_i}(t_i;\v\theta)
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
When we fix the sample and allow $\v{\theta}$ to vary, we obtain the
contribution to the likelihood $L$ from the $i$\textsuperscript{th} observation
when the system lifetime is exactly known (i.e., $\delta_i = 1$) but the
component cause of failure is masked by a candidate set $c_i$:
\begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\v\theta) = R_{\T_i}(t_i;\v\theta) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}, 
if we observe an exact system failure time for the $i$-th system ($\delta_i = 1$),
but the component that failed is masked by a candidate set $c_i$, then its likelihood
contribution is given by Equation \eqref{eq:likelihood_contribution_masked}.

## Right-Censored Data

As described in Section \ref{sec:data}, we observe realizations of
$(S_i,\delta_i,\mathcal{C}_i)$ where $S_i = \min\{\T_i,\tau_i\}$ is the
right-censored system lifetime, $\delta_i = 1_{\{\T_i < \tau_i\}}$ is
the right-censoring indicator, and $\mathcal{C}_i$ is the candidate set.

In the previous section, we discussed the likelihood contribution from an
observation of a masked component cause of failure, i.e., $\delta_i = 1$. 
We now derive the likelihood contribution of a *right-censored* observation
$(\delta_i = 0$) in our masked data model.
\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\v\theta) = R_{\T_i}(s_i;\v\theta).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau_i$, and we only know that
$\T_i > \tau_i$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\v\theta) = \Pr\!{}_{\v\theta}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\v\theta) = R_{\T_i}(s_i;\v\theta).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the likelihood
contribution for the $i$\textsuperscript{th} system shown in Theorem
\ref{thm:likelihood_contribution},
$$
L_i(\v\theta) =
\begin{cases}
    R_{\T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{\T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$

We use this result in the next section to derive the maximum likelihood
estimator of $\v\theta$.

Maximum Likelihood Estimation {#sec:mle}
========================================

We use maximum likelihood estimation (MLE) to estimate the series system
parameter $\v\theta$ given the masked data described in Section
\ref{#sec:like_model}.
This is achieved by maximizing the likelihood function $L(\v\theta)$ with
respect to $\v\theta$ so that, under the assumed model, the observed data is
most likely.
The point in the parameter space $\Omega$ that maximizes the likelihood function
is called the maximum likelihood estimate.

According to @bain, a point $\hat{\v\theta}$ in $\v\Omega$ at which $L(\v\theta)$
is a maximum is called the *maximum likelhood estimate* (MLE) of $\v\theta$.
That is, $\hat{\v\theta}$ is a value of $\v\theta$ that satisfies
\begin{equation}
\label{eq:mle}
L(\hat{\v\theta}) = \max_{\v\theta \in \v\Omega} L(\v\theta).
\end{equation}
Essentially, the MLE is a point in the parameter space that is a maximum of the
likelihood of the observed data,
$$
\hat{\v\theta} \in \arg\max_{\v\theta \in \v\Omega} L(\v\theta).
$$

Any point that maximizes the likelihood function also maximizes the log-likelihood
function.
Thus, for both computational and analytical reasons, we work with the log-likelihood
function.
\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\v\theta)$, is given by
the log of the likelihood function for our masked data model,
\begin{equation}
\label{eq:loglike}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta)
\end{equation}
where
\begin{equation}
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) +
    \delta_i \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \bigr).
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is just the logarithm of the likelihood function,
$$
\ell(\v\theta) = \log L(\v\theta) = \log \prod_{i=1}^n L_i(\v\theta).
$$
Since $\log(A \cdot B) = \log(A) + \log(B)$, we may rewrite the log-likelihood
function as
$$
\ell(\v\theta) = \sum_{i=1}^n \log L_i(\v\theta).
$$
By Equation \eqref{eq:like}, $L_i$ is given by
$$
L_i(\v\theta) =
\begin{cases}
R_{\T_i}(s_i;\v\theta) &\text{ if } \delta_i = 0,\\
R_{\T_i}(s_i;\v\theta)
    \beta_i \sum_{j\in c_i} h_j(s_i;\v{\theta_j}) &\text{ if } \delta_i = 1.
\end{cases}
$$

We now consider these two cases separately, then combine them to obtain the
result in Theorem \ref{thm:loglike_total}.

\textbf{Case 1}: If the $i$-th system is right-censored, i.e., $\delta_i = 0$, then
$$
\ell_i(\v\theta) = \log R_{\T_i}(s_i;\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the
failure time is known, i.e., $\delta_i = 1$, then
\begin{align*}
\ell_i(\v\theta)
    &= \log R_{\T_i}(s_i;\v\theta) \beta_i \sum_{j\in c_i} h_j(s_i;\v{\theta_j})\\
    &= \log R_{\T_i}(s_i;\v\theta) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j})\bigr).
\end{align*}

Since $\beta_i$ is not a function of $\v\theta$ (see Condition \ref{cond:masked_indept_theta}),
$\log \beta_i$ is a constant with respect to $\v\theta$ and so may be ignored in MLE
(when we solve the maximum likelihood equations, any terms that do not depend on
$\v\theta$ will be eliminated by the gradient operator).
Thus, we may rewrite the above equation as
$$
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) +
        \log \biggl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \biggr).
$$
\end{proof}

## Solving the MLE {#sec:iterative}
According to @bain, if $\v\Omega$ is a Cartesian product of $l$ intervals,
partial derivatives of $L(\v\theta)$ exist, and the MLEs do not occur on the
boundary of $\v\Omega$, then the MLEs will be solutions of the simultaneous
equations
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0
\end{equation}
for $j=1,\ldots,p$ where $p$ is the number of components in $\v\theta$.
We call these equations the *maximum likelihood equations*.
If multiple solutions to Equation \eqref{eq:mle_eq} exist, each solution that
maximizes the likelihood function is a valid MLE.

If there is no closed-form solution to the maximum likelihood equations
\eqref{eq:mle_eq}, we may use iterative root-finding methods to
numerically approximate a solution.
A general approach is, if we have a guess $\v\theta^{(n)}$, take a step in
a "promising" direction $\v{d}^{(n)}$ to obtain the next guess,
\begin{equation}
\label{eq:iterative_update}
\v\theta^{(n+1)} = \v\theta^{(n)} + \alpha^{(n)} \v{d}^{(n)},
\end{equation}
where $\v\theta^{(0)}$ is an initial guess in the parameter space $\Omega$ that is
sufficiently close to the MLE $\hat{\v\theta}$. In our case, we use the
parameter vector $\v\theta^{(0)} = \v\theta$ as our initial guess, since we know
the true value $\v\theta$ in our simulation studies, but if plausible initial
guesses are not known, then global methods may be used to find a good initial
guess, like Simulated Annealing.

Assuming that at $\v\theta^{(n)}$, a sufficiently small step in the direction
$\v{d}^{(n)}$ results in an improvement with respect to the log-likelihood function,
we say that we *overshoot* if
$$
\ell(\v{\theta}^{(n+1)}) < \ell(\v{\theta}^{(n)}).
$$
The value $\alpha^{(n)}$ in Equation \eqref{eq:iterative_update} is a positive
real number chosen by a *line search* method so that we do not overshot, which
has an optimal value given by
$$
\alpha^{(n)} \in \operatorname{argmax}_{\alpha}
    \ell(\v{\theta}^{(n)} + \alpha \v{d}^{(n)}).
$$
However, this may be too computationally expensive to compute, and so we use
a less optimal but faster method known as *backtracking*.
In the backtracking line search method, we determine $\alpha^{(n)}$ by initially
letting $\alpha^{(n)} = 1$ and then, if we overshoot, redo the update
with $\alpha^{(n)} \gets r \alpha^{(n)}$, $0 < r < 1$, repeating until we
do not overshoot.

Note that this is not necessarily the best course of action for finding
global maximums, since depending on our initial guess $\v\theta^{(0)}$, we may
get stuck in a local maximum.
A global search method, like Simulated Annealing, may be used to find
better initial guesses.

We do as many iterations in Equation \eqref{eq:iterative_update} as necessary to
satisfy some *stopping condition*, which is usually something simple like the
distance between $\v\theta^{(n)}$ and $\v\theta^{(n+1)}$ being sufficiently
small. Under the right conditions, for sufficiently large $n$,
$\v\theta^{(n)} \approx \hat{\v\theta}$.

We use a popular technique known as Newton-Raphson method, which is obtained by
letting $\v{d}^{(n)}$ be defined as
$$
\v{d}^{(n)} = -J^{-1}(\v\theta^{(n)}) \nabla \ell(\v\theta^{(n)}),
$$
where $J(\v\theta^{(n)})$ and $\nabla \ell(\v\theta^{(n)})$ are respectively
the observed Fisher information matrix (Hessian of the
log-likelihood function) and the score (gradient of the log-likelihood
function), each evaluated at $\v\theta^{(n)}$.

## Properties of the MLE {#sec:mle_properties}

In this section, we discuss some properties of the MLE that are useful for
making statistical inferences about the parameter vector $\v\theta$.
According to @bain, if certain regularity conditions are satisfied, then
solutions of the maximum likelihood equation \eqref{eq:mle_eq} have the
following desirable properties:

1. $\v{\hat\theta}$ exists and is unique.

2. $\v{\hat\theta}$ is an asymptotically unbiased estimator.

3. $\v{\hat\theta}$ is asymptotically the UMVUE, the uniform minimum variance unbiased
   estimator.
   
4. $\v{\hat\theta}$ is asymptotically normal with a mean $\v\theta$ and a
   variance-covariance matrix that is the inverse of the Fisher information matrix (FIM),
   whose $(i,j)$-th component is given by
   $$
    I(\v\theta)_{i j} = n E_{\v\theta}\biggl(-\frac{\partial^2}{\partial \theta_i \partial \theta_j}
        \log f(S_i,\delta_i,\mathcal{C}_i;\v\theta)\biggr).
   $$
   
Usually, the *observed* FIM is used instead of the FIM, which is a conditioned on the
observed data,
$$
J(\v\theta)_{i j} = -\frac{\partial}{\partial \theta_i \partial \theta_j}
    \ell(\v\theta).
$$
If $\v\theta$ is unknown, $J(\v\theta)$ may be estimated with $J(\v{\hat\theta})$.
Thus, assuming the regularity conditions are satisfied, then approximately,
$$
    \hat{\v\theta} \sim \mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))
$$
and as the sample size goes to infinity, $\hat{\v\theta}$ converges in
distribution to $\mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))$.

This is the asymptotic sampling distribution of MLE, but for small samples, it can be
a very poor approximation. In the next section, we discuss the bootstrap
method. In particular, bootstrapping the variance and confidence intervals.

Bootstrapping the Variance and Confidence Intervals of the MLE {#sec:boot}
===============================================================
The bootstrap method is a powerful, general purpose tool for estimating
the sampling distribution of a statistic, in our case statistics of the MLE,
that does not rely on making strong assumptions about the
underlying distribution of the data.

The most common form of the Bootstrap method is the non-parametric Bootstrap.
In the non-parametric bootstrap, the random data is created by resampling with
replacement from the original data and then computing the statistic of interest
on the resampled data. This is repeated $B$ times, giving us $B$
bootstrap replicates of the statistic. The sampling distribution of
the statistic is then approximated by the empirical distribution of the
bootstrap replicates. Since we do not know (nor do we attempt to model) the way
candidate sets are generated, this non-parametric form is ideal.

We are particularly interested in two statistics of the MLE: the variance and
the confidence interval. We will bootstrap the confidence interval using the
percentile method, which does not explicitly depend on the variance estimate. However,
conceptually, they are still related: a higher variance should
generally lead to a larger confidence interval.

In our simulation study, we will assess the performance of the bootstrapped variance
by comparing it to the empirical variance of the MLE, and we will assess
the performance of the bootstrapped confidence interval by computing
its coverage probability, i.e., a 95% confidence interval should
contain the true value 95% of the time. We say that a confidence interval
has *good coverage* if its coverage probability is close to the nominal
confidence level.

If the confidence intervals have good coverage, a small confidence interval width
means we are more confident that the true value of $\v\theta$ is close to the MLE,
and a large confidence interval width means we are less confident that the true value
of $\v\theta$ is close to the MLE. However, if the confidence interval has poor
coverage, then the confidence interval width is not particularly informative.
Thus, we see that the confidence interval is only useful if it has good coverage,
and so we will focus on assessing the coverage probability of the confidence intervals
in our simulation study.

Simulation Study: Series System with Weibull Components {#sec:weibull}
=============================================
In the real world, systems are quite complex:

1. They are not perfect series systems.

2. The components in a system are not independent.

3. The lifetimes of the components are not precisely modeled by
   any named probability distributions.
   
4. The components may depend on many other unobserved factors.

With these caveats in mind, we model the data as coming from a Weibull series
system of $m = 5$ components, and other factors, like ambient temperature, are
either negligible (on the distribution of component lifetimes) or are more or less
constant.

The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{WEI}(\v{\theta_j})
$$
where $\v{\theta_j} = (k_j, \lambda_j)$ for $j=1,\ldots,m$.
Thus, $\v\theta = (\v{\theta_1},\ldots,\v{\theta_m})' = \bigl(k_1,\lambda_1,\ldots,k_m,\lambda_m\bigr)$.
The random variable $T_{i j}$ has a reliability function, pdf, and hazard function
given respectively by
\begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align}
where $t > 0$ is the lifetime, $\lambda_j > 0$ is the scale parameter and $k_j > 0$
is the shape parameter. The shape parameters $k_1, \ldots, k_m$ have the following interpretations:

\begin{enumerate}
\item[$k_j < 1$] The hazard function decreases with respect to time. For instance,
  this may occur as a result of defective components being weeded out early. This
  is known as the *infant mortality* phase.
\item[$k_j = 1$] The hazard function is constant with respect to time. This is an
  idealized case that is rarely observed in practice, but may be useful for modeling
  purposes.
\item[$k_j > 1$] The hazard function increases with respect to time. For instance,
  this may occur as a result of components wearing out. This is known as the
  *aging* phase.
\end{enumerate}

The lifetime of the series system composed of $m$ Weibull components
has a reliability function given by
\begin{equation}
\label{eq:sys_weibull_reliability_function}
R(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}
\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
R(t;\v\theta) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R(t;\v\theta)
    &= \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}\\
    &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
\end{proof}

The Weibull series system's hazard function is given by
\begin{equation}
\label{eq:sys_weibull_failure_rate_function}
h(t;\v\theta) =
    \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{equation}
whose proof follows from Theorem \ref{thm:sys_failure_rate}.

The pdf of the series system is given by
\begin{equation}
\label{eq:sys_weibull_pdf}
f(t;\v\theta) =
\biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{equation}
\begin{proof}
By definition,
$$
f(t;\v\theta) = h(t;\v\theta) R(t;\v\theta).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}


## System Reliability
```{r sim-study-design, echo = F}
theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)
shapes <- theta[grepl("shape", names(theta))]
scales <- theta[grepl("scale", names(theta))]
```

A series system is only as reliable as its least reliable component. 
In order to make the simulation study representative of real-world scenarios, at least
for systems designed to be reliable, we choose parameter values that are representative of
real-world systems where there is no single component that is much less reliable than the
others.

One way to define reliability is by the mean time to failure (MTTF), which is the
expected value of the lifetime, which for the Weibull distribution is given by
$$
\text{MTTF} = k \, \Gamma(1 + 1/\lambda),
$$
where $\Gamma$ is the gamma function.

We consider the data from [@Huairu-2013], which includes a study of the reliability of a series system
with three Weibull components with shape and scale parameters given by
\begin{equation}
\begin{aligned}
    k_1 = `r theta["shape1"]` &\quad \lambda_1 = `r theta["scale1"]`\\
    k_2 = `r theta["shape2"]` &\quad \lambda_2 = `r theta["scale2"]`\\
    k_3 = `r theta["shape3"]` &\quad \lambda_3 = `r theta["scale3"]`.
\end{aligned}
\end{equation}

Our approach is to extend this system to a five component system by adding two
more components with shape and scale parameters given by
\begin{equation}
\begin{aligned}
    k_4 = `r theta["shape4"]` &\quad \lambda_4 = `r theta["scale4"]`\\
    k_5 = `r theta["shape5"]` &\quad \lambda_5 = `r theta["scale5"]`.
\end{aligned}
\end{equation}

```{r table-2, table.attr = "style='width:50%;'", echo = F, results = 'asis', fig.cap = "Component Reliability", fig.align = "center"}

# let's also show the MTTF of the entire series system
mttf.sys <- integrate(function(t) { t * dwei_series(t, scales=scales,
    shapes=shapes) }, lower=0, upper=Inf)$value
mttf <- gamma(1 + 1/shapes) * scales
components <- data.frame(
  "MTTF" = mttf,
  row.names = paste("Component", 1:5)
)

components <- rbind(components, "Series System" = mttf.sys)
knitr::kable(components, caption = "Meean Time To Failure of Weibull Components and Series System")
```

As shown by Table 2, there are no components that are significantly less reliable
than any of the others.
Note that a series system in which, say, one of the components does have a significantly
shorter MTTF would also pose significant challenges to estimating the parameters of
the system from our masked failure data, since the failure time of the series system
would be dominated by the failure time of the least reliable component.

## Weibull Likelihood Model for Masked Data

In Section \ref{sec:like_model}, we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
$i$\textsuperscript{th} system is given by the following theorem.
\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\v\theta) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\v\theta) =
\begin{cases}
    R_{\T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{\T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{\T_i}$ is given by
$$
R_{\T_i}(t_i;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\v{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the following result.
\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\v\theta) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\v\theta$ since they do not
affect the MLE.
\end{corollary}

Since the systems are independent, the log-likelihood of the entire sample of $n$
observations is given by
\begin{equation}
\label{eq:weibull_log_likelihood}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta).
\end{equation}

## Numerically Solving the MLE

We may find an MLE by solving the maximum likelihood equation \eqref{eq:mle_eq},
i.e., a point $\v{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)$ satisfying
$\nabla_{\theta} \ell(\v{\hat\theta}) = \v{0}$, where $\nabla_{\v\theta}$
is the gradient of the log-likelihood function with respect to $\v\theta$, otherwise
known as the score.

To solve these ML equations, we use the Newton-Raphson method described in Section
\ref{sec:iterative}. In order to use the Newton-Raphson method, we need to
compute the gradient and Hessian of the log-likelihood function.

We analytically derive the gradient of the log-likelihood function (score), since it
is useful to have for the Newton-Raphson method, but we do not do the same for the
Hessian of the log-likelihood for the following reasons:

1. The Hessian is not necessarily needed since we often use some faster method to
approximate the Hessian, e.g., the BFGS method. Technically, we could also
numerically approximate the gradient too, but the gradient is much easier
to derive than the Hessian, and moreover, knowing the score precisely also
enables us to more accurately approximate the Hessian by taking the Jacobian
of the gradient.

2. The Hessian is more difficult to derive than the score, and so it is more
likely that we will make a mistake when deriving the Hessian. 

The following theorem derives the score function.
\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\v\theta) = \biggl(
    \frac{\partial \ell_i(\v\theta)}{\partial k_1},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\v\theta)}{\partial k_m},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the log-likelihood
contribution of the $i$-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation so the proof
has been omitted, but the result has been verified by using a very precise numerical
approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions is
the same of their gradients, and so the score function of the entire sample is given
by
\begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\v\theta) = \sum_{i=1}^n \nabla \ell_i(\v\theta).
\end{equation}

### Issues with the MLE {#sec:opt_rescale}

\textbf{Identifiability}: When estimating the parameters of latent components, we
must be careful to ensure that the parameters are identifiable such that the
likelihood function is maximized at a unique point. If the likelihood
function is not maximized at a unique point, then the MLE is not unique, and
a lot of the theory we have developed so far breaks down.

One way in which this problem may arise is if the data is not informative
enough. For example, if we have a series system and in the observed masked data
component $1$ is in the candidate set if and only if component $2$ is in
candidate set, then we do not have enough information to estimate the
parameters of component $1$ and component $2$ separately. In this case, we
could combine these two components into a single component. We lagely avoid this
problem by using the Bernoulli candidate set model, but sometimes it may still arise
by chance.

\textbf{Parameter rescaling}: When the parameters under investigation span different
orders of magnitude, parameter rescaling can significantly improve the performance
and reliability of optimization algorithms. Parameter rescaling gives an optimizer a
sense of the typical size of each parameter, enabling it to adjust its steps
accordingly. This is crucial in scenarios like ours, where shape and scale parametes
are a few orders of magnitude apart. Without rescaling, the optimization routine may
struggle, taking numerous small steps for larger parameters and overshooting for
smaller ones.

Speed of convergence was particularly important in our case, since in our simulation
study, we employ the Bootstrap method to estimate the sampling distribution of the
MLE, which requires us to estimate the MLE for many data sets. We found that
parameter rescaling significantly improved the speed of convergence, which allowed
us to run our simulation study in a tractable amount of time.

In the `optim` algorithm in the R package `stats`, we achieve this result by
assigning a `parscale` vector in line with the parameter magnitudes. It does not
matter what the values of the `parscale` vector are, only their relative magnitudes.
It does not need to be very precise, but since we are doing a simulation study,
we know the true parameter values and can use that information to scale them
appropriately.

## Simulation Design

In this section, we describe the design of our simulation study. We first describe
the simulation scenarios we consider, and then we describe how we generate data
for each scenario.

### Bernoulli Candidate Set Model

In our simulation study, we must generate data that satisfies the masking
conditions described in Section \ref{sec:candmod}.
There are many ways to satisfying the masking conditions. We choose the simplest
method, which we call the *Bernoulli candidate set model*. In this model, each
non-failed component is included in the candidate set with
a fixed probability $p$, independently of all other components and independently
of $\v\theta$, and the failed component is always included in the candidate set.

### Right-Censoring Model

We employ a very simple right-censoring model, where the right-censoring time
$\tau$ is fixed and independent of $\v\theta$ and the censoring time $S_i$ of
the $i$\textsuperscript{th} system.

We parameterize $\tau$ by quantiles of the series system, e.g., if $q = 0.8$,
then $\tau(q)$ is the $80\%$ quantile of the series system such that $80\%$ of
the series systems are observed (fail before time $\tau(q)$) and $20\%$ of the
series systems fail after time $\tau(q)$ (are right-censored).

### Scenarios

we vary the sample size $n$, the Bernoulli masking probability $p$ of including each
non-failed component in the candidate set, and the right-censoring time $\tau$. We
then analyze the performance of the MLE under these various scenarios.

Here is an outline of the simulation study analysis:

1. Set up simulation parameters for various scenarios of interest, such as
   generating data to examine the relationship between bias and masking probability
   for different sample sizes and right-censoring times.

2. Generate $R$ data sets for each scenario (some combination of $n$, $p$, and
   $\tau$).

3. Estimate the parameters for each data set, giving us $R$ estimates of the
   parameters. We use these data sets as an empirical estimate of
   the sampling distribution of the MLE for each scenario.
   
4. Using the empirical sampling distribution of the MLE, estimate various 
   performance measures of the MLE, like bias, variance, MSE, and coverage
   probability for each scenario.

5. Analyze and visualize the results, e.g., by plotting the bias, variance, MSE,
   and coverage probability as a function of $n$ for different combinations of $p$
   and $\tau$. 

   We then interpret the results and discuss the performance of the MLE estimator
   under various conditions. We expect that as $n \to \infty$, the bias and MSE
   will go to $0$ and the coverage probability will go to $0.95$ (when
   constructing $95\%$ confidence intervals). Of course, we do not expect these
   results to hold for finite $n$, but we would like to see how the bias, MSE, and
   coverage probability change as we vary $n$, $p$, and $\tau$.

For how we generate a scenario, see Appendix A.


> So, now we just resample from the data with replacement, and fit the Weibull series
> model to each bootstrap sample. We do this $B = 1000$ times, giving us $B$ bootstrap
> replicates of the MLE $\hat{\v\theta}^{(1)},\ldots,\hat{\v\theta}^{(B)}$.

> As a ground truth, we will use the empirical distribution of the MLE
> under our data model
> under a variety of simulation scenarios where we vary the sample size,
> the right censoring time, and the so-called masking probability of the
> candidate sets, where a higher masking probability means that the
> candidate sets are more likely to contain non-failed components.


### Verification

To verify that our likelihood model is correct, we load the Table 2 data from
[@Huairu-2013] and fit the Weibull series model to the data to see if we can
recover the MLE they reported. When we fit the Weibull series model to this data by
maximizing the likelihood function, we obtain the following fit for the shape and
scale parameters given respectively by
$$
    \hat{k}_1 = `r theta["shape1"]`,
    \hat{k}_2 = `r theta["shape2"]`,
    \hat{k}_3 = `r theta["shape3"]`,
$$
and
$$
    \hat{\lambda}_1 = `r theta["scale1"]`,
    \hat{\lambda}_2 = `r theta["scale2"]`,
    \hat{\lambda}_3 = `r theta["scale3"]`,
$$
which is in agreement with the MLE they reported. Satisfied that our likelihood model
is correct, we proceed with the simulation study.

## Bias, variance, and MSE of the MLE {#sec:acc_prec}

First, we estimate the bias, variance, and MSE of the MLE under various scenarios.
This is useful for understanding the accuracy and precision of the MLE under
different conditions. It is unrelated to the bootstrap method, but it is useful to
compute these quantities before we assess the bootstrapped variance and confidence
intervals.

A measure of the accuracy of $\v{\hat\theta}$ is the bias, which is defined as
$$
\operatorname{b}(\v{\hat\theta}) = E(\v{\hat\theta}) - \v\theta.
$$
We cannot analytically derive the bias, so we estimate the bias using the empirical
sampling distribution,
$$
\hat{\operatorname{b}}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}(\v{\hat\theta}) - \theta_j.
$$

We estimate the precision of $\hat\theta_j$ with the variance and MSE. The variance
of $\v{\hat\theta}$ is defined as
$$
\operatorname{Var}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - E_{\hat{\v\theta} \sim \text{data}}(\hat\theta_j))^2\bigr),
$$
where the expectation is taken with respect to the empirical sampling distribution.
The mean squared error is a measure of estimator error that incorporates both the
bias and the variance, and is defined as
$$
\operatorname{MSE}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - \theta_j)^2\bigr).
$$

Assuming the regularity conditions for the MLE are met, the MSE converges in
probability to the variance.

## Simulation Scenarios

We consider many different scenarios, where we vary the sample size $n$, the masking
probability $p$, and the right-censoring time $\tau$. We then analyze the performance
of the MLE under these various scenarios by estimating the bias, variance, and MSE
of the MLE.

### Absolute bias vs. sample size with a masking probability but no right-censoring {-}

In this scenario, we want to see the bias of the MLE as a function of the sample
size $n$ from $n = 30$ to $n = 800$ for a fixed masking probability $p = 0.2$ and no
right-censoring $(\tau = \infty)$. Recall that the masking probability is the
probability of including each non-failed component.


```{r plot-bias-p-0.2-vs-sample-size, echo =F, fig.cap="Bias vs. sample size (masking probability 0.2)", fig.align="center"}
knitr::include_graphics("image/plot-p-0.2-bias-vs-sample-size.pdf")
```

In Figure \ref{fig:plot-bias-p-0.2-vs-sample-size}, we plot the absolute bias
$|\operatorname{bias}(\hat\theta)|$ on a log scale against the sample size. 
However, because the absolute bias is quite large for small sample sizes and small
for large sample sizes, we use a log scale. Furthermore, we show the absolute bias
for the shape and scale parameters separately, since the scale parameters are much
larger than the shape parameters.

Here are some important observations Figure \ref{fig:plot-bias-p-0.2-vs-sample-size} reveals:

1. For both shape and scale parameters, we see that the absolute bias seems to be
decreasing to zero as the sample size increases. This is not surprising since we
expect the MLE to be consistent, i.e., $\v{\hat\theta}$ converges in probability
to $\v\theta$ as the sample size increases to infinity. Still, it is reassuring to
see that the bias seems to be behaving as expected.

2. For the shape parameters, which are small (the shape parameters have true values
a little larger than 1), the bias is relatively large for sample sizes up to
$100$.

3. For the scale parameters, which are quite large (the scale parameters have true
values around 1000). Like with the shape parameters, the bias is relatively large for
sample sizes up to $100$, but seems to stabilize and reach relatively small values
after that.

### Scenario: Bias vs. sample size and masking probability and no right-censoring {-}

Now, we take a larger view and plot the bias (without taking its absolute value
as we had done previously) against the masking probabilities $p = 0$
(no masking) to $p = 0.4$ (significant masking) for sample sizes 100, 400, and 800.

For the shape parameters, at a sample of size 100, we see significant bias and we
also see that it is very sensitive to the masking probability. See Figure
\ref{fig:plot-bias-shapes-vs-p-sample-size-100-400-800}. However, for sample sizes
of 400 and 800, the bias is relatively small and unaffected by the masking
probability.

```{r plot-bias-shapes-vs-p-sample-size-100-400-800, echo =F, fig.cap="Shape Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-shapes-p-vs-sample-size-100-400-800.pdf")
```

For the scale parameters, a similar pattern emerges, although we see that even
for sample size 400, there is evidence that the bias is still affected by the masking
probability. See Figure \ref{fig:plot-bias-scales-vs-p-sample-size-100-400-800}.

The smallest bias, as expected, occurs for sample sizes of $800$. The bias 
for $\lambda_1$ (scale parameter 1) at the masking probability $0.3$ is an interesting
case, since it jumps up at that point for some reason. We used only $R = 100$
replications, so it is plausible it would decrease with more replications.
Regardless, the overall trend is that the bias decreases as the sample size increases, and its
dependence on the masking probability is relatively small with sufficiently large
sample sizes.

```{r plot-bias-scales-vs-p-sample-size-100-400-800, echo =F, fig.cap="Scale Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-scales-p-vs-sample-size-100-400-800.pdf")
```

#### Scenario: Bias vs. right-censoring time and sample size with a fixed masking probability {-}

In this scenario, we want to isolate the effect of the right-censoring time $\tau$
on the bias. We fix the masking probability to $p = 0.215$, in line with the masking
probability we estimate for the Table 2 data set in [@Huairu-2013].

We plot the bias against the right-censoring
time for sample sizes 50, 150, and 300. See Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}.
On the $x$-axis, we report the right-censoring time as a quantile of the
Weibull series distribution so that we can more clearly see the effect of the right-censoring
on the bias, e.g., the $50\%$ quantile is the time at which $50\%$ of the
systems are expected to fail.

```{r plot-bias-tau-vs-sample-size-50-150-300, echo =F, fig.cap="Bias vs. right-censoring time and sample sizes 50, 150, and 300", fig.align="center"}
knitr::include_graphics("image/plot-bias-tau-vs-sample-size-50-150-300.pdf")
```

A few observations about Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}:

1. The bias decreases as the right-censoring time increases. This is expected since we have
more information about the system when the right-censoring time is larger.

2. The bias decreases as the sample size increases, which is also expected since we
have more information about the system when the sample size is larger.

3. The bias is relatively small for sample sizes 150 and 300, but for sample size 50,
the bias is quite large, particularly for the shape parameters. This is not
surprising since the sample size is quite small, and so we do not expect the MLE to
be very accurate.


### Variance

```{r var.plot, echo=F, fig.cap="Variance vs. sample size", fig.align="center"}

# Load the data
sim_data <- read_csv("./results/data/data-fim.csv")

# rename columns for plotting
sim_data <- sim_data %>% rename(
  "Shape 1" = mle.1,
  "Scale 1" = mle.2,
  "Shape 2" = mle.3,
  "Scale 2" = mle.4,
  "Shape 3" = mle.5,
  "Scale 3" = mle.6,
  "Shape 4" = mle.7,
  "Scale 4" = mle.8,
  "Shape 5" = mle.9,
  "Scale 5" = mle.10
)

# Filter data
filtered_data <- sim_data %>% filter(p == 0 & q == 1)

# Compute variances
shape_data <- filtered_data %>% select(n, "Shape 1", "Shape 2", "Shape 3", "Shape 4", "Shape 5")
scale_data <- filtered_data %>% select(n, "Scale 1", "Scale 2", "Scale 3", "Scale 4", "Scale 5")

shape_variances <- shape_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)
scale_variances <- scale_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)

# Reshape the data for ggplot2
shape_variances_long <- shape_variances %>% gather(key = "Parameter", value = "Variance", -n)
scale_variances_long <- scale_variances %>% gather(key = "Parameter", value = "Variance", -n)

# Plotting
ggplot(shape_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Shape Parameter MLEs with respect to Sample Size") +
  theme_minimal()

ggplot(scale_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Scale Parameter MLEs with respect to Sample Size") +
  theme_minimal()
```

## Coverage Probability of Bootstrapped Confidence Intervals {#sec:coverage_prob}
Under a variety of scenarios, we will bootstrap a $95\%$-confidence interval for
$\v\theta$ using the percentile method, and we will evaluate its accuracy by
computing the coverage probability.

We want the coverage probability to be close to the nominal level, $95\%$, because if
the coverage probability is too low, then we will be too confident in the precision
and accuracy of the MLE, and if the coverage probability is too high, then we will
not be confident enough in the precision and accuracy of the MLE.

To estimate the coverage probability, we use the following procedure:

1. For a given scenario, we generate $R = 300$ data sets.

2. We find an MLE for each of $R$ data sets.

3. We bootstrap the $95\%$-confidence interval for each MLE.

4. We compute the coverage probability by computing the proportion of times
   the true parameter $\v\theta$ is contained in $95\%$-confidence
   interval.

## Simulation Scenarios

### Scenario: Coverage probability vs. sample size with a fixed masking probability and no right-censoring {-}

We want to isolate the effect of the coverage probability as a function of the sample
size $n$. We fix the masking probability to $p = 0.2$ and without right-censoring $(\tau = \infty)$
and vary the sample size from $n = 30$ to $n = 800$. See Figure \ref{fig:plot-coverage-p-three-vs-sample-size}.

```{r plot-coverage-p-three-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size for masking probability $0.3$", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p_0.3-vs-sample-size.pdf")
```

Here are some key observations:

1. It is immediately obvious that the scale parameters (dashed lines) have a much
   lower coverage probability than the shape parameters (solid lines), particularly
   for small sample sizes less than $n = 200$.
   
   In general, the scale parameters appear to be more difficult to estimate than the
   shape parameters. 

2. As the sample size increases, the coverage probability for the shape parameters
   and scale parameters approaches the nominal level, $95\%$.
   
   This suggests that the sampling distribution of the MLE is converging in
   distribution to a multivariate normal distribution with mean $\v\theta$ and
   variance-covariance given by the inverse of the FIM, consistent with the
   asymptotic theory.

### Scenario: Coverage probability vs. sample size and masking probability without right-censoring {-}

We want to get a larger picture of how the coverage probability depends on the
sample size $n$ and masking probability $p$. We fix the right-censoring time to
$\tau = \infty$ and vary the sample size from $n = 30$ to $n = 800$ and vary the
masking probability from $p = 0$ (no masking) to $p = 0.4$ and then compute the
coverage probability for each combination of sample size $n$ and masking probability
$p$.

```{r plot-coverage-p-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p-vs-sample-size.pdf")
```

The results of this analysis are summarized by Figure \ref{fig:plot-coverage-p-vs-sample-size}.
Here are some key observations:

1. For sample sizes $n \leq 100$, the coverage probability for the shape parameters
is close to the nominal level, $95\%$, only for small masking probabilities. However,
as the sample size increases, the coverage probability for the shape parameters
quickly approaches the nominal level, $95\%$, for all masking probabilities reported
here.

2. For the scale parameters, the coverage probability is too low for all sample sizes
$n < 200$ for all masking probabilities reported here. For small sample sizes, the
confience intervals particularly for the scale parameters, should probably be taken
with a grain of salt.

In Section \ref{sec:boot}, we explore an alternative way to construct confidence
intervals using the bootstrap method, which is generally a more accurate way to
compute confidence intervals. Unlike the inverse of the observed FIM, it does not
assume that the sampling distribution of the MLE is asymptotically normal, and so
it is more robust to violations of this assumption.


# Conclusion

We have developed a likelihood model for series systems with latent components
and right-censoring. We have provided evidence that, as long as certain regularity
conditions are met, the MLE is asymptotically unbiased and consistent.

# References {-}

Please see below for a full list of references.

```{r refmgr, echo=FALSE, results='asis'}

cat("<div id=\"refs\"></div>")
```

# Appendix {#app}

```{r eval=F}
#\renewcommand{\thesection}{\Alph{section}}
#\setcounter[section}{0}
```

## Data {#app:data}

## Simulation Code {-}

```{r eval = FALSE}
#############################################################
# Simulation data generating process for specified scenario #
# (n, p, q), where:                                         #
#    - n is a vector of sample sizes                        #
#     - n is a vector of sample sizes                       #
#     - p is a vector of masking probabilities              #
#     - q is a vector of right-censoring quantiles of the   #
#       Weibull series distribution.                        #
#############################################################

# here is the R libary we developed for this project
library(wei.series.md.c1.c2.c3) 

# for parallel processing
library(parallel)

# you can set a seed for reproducibility of the experimental run
# however, if you use parallel processing, this simple approach will not work.
# set.seed(1234)

###############################################
# Here is an example of how to run a scenario #
###############################################

# set the simulation name to be used in the file names
sim.name <- "sim-2"
# set the sample sizes
ns <- c(30, 40, 50, 75, 100, 200, 400, 800)
# set the masking probabilities
ps <- seq(0, 0,1, 0.2, 0.3, 0.4)
# set the right-censoring quantiles
qs <- c(0.5, 0.6, 0.7, 0.8, 0.9, 0.95)
# set the number of replicates
R <- 100
# set the number of CPU cores to use
ncores <- 4

# true parameter values
theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]

# helps the MLE optimization routine converge more quickly and reliably
# by scaling the parameters to be of similar magnitude
parscale <- c(1, 1000, 1, 1000, 1, 1000, 1, 1000, 1, 1000)

sim.run <- function(sim.name, n, p, q, R = 1000) {
    mles <- list()
    problems <- list()

    tau <- wei.series.md.c1.c2.c3::qwei_series(
        p = q, scales = scales, shapes = shapes)

    cat("n =", n, ", p =", p, ", q = ", q, ", tau = ", tau, "\n")

    for (r in 1:R) {
        result <- tryCatch({

            df <- wei.series.md.c1.c2.c3::generate_guo_weibull_table_2_data(
                shapes = shapes,
                scales = scales,
                n = n,
                p = p,
                tau = tau)

            sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
                df = df,
                theta0 = theta,
                reltol = 1e-7,
                parscale = parscale,
                maxit = 2000L)
            mles <- append(mles, list(sol))

            if (r %% 10 == 0) {
                cat("r = ", r, ": ", sol$par, "\n")
            }

        }, error = function(e) {
            cat("Error at iteration", r, ":")
            print(e)
            problems <- append(problems, list(list(
                error = e, n = n, p = p, q = q, tau = tau, df = df)))
        })
    }
  
    if (length(mles) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, mles = mles),
            file = paste0("./results/", sim.name, "/results_", n, "_", p, "_", q, ".rds"))
    }

    if (length(problems) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, problems = problems),
            file = paste0("./problems/", sim.name, "/problems_", n, "_", p, "_", q, ".rds"))
    }
}

params <- expand.grid(n = ns, p = ps, q = qs)
result <- mclapply(
    1:nrow(params),
    function(i) sim.run(sim.name, params$n[i], params$p[i], params$q[i], R),
    mc.cores = ncores)

```


## Appendix B: Simulation of scenarios using the Bootstrap method {-}

```{r, eval=FALSE}
#############################################################
# in this scenario, we want to see how we can use the bootstrap
# method to estimate the confidence intervals more precisely (better calibration
# of confidence intervals) for small sample sizes.
# we'll use it to construct a 95% confidence interval for the estimator. we'll
# compare this result to the asymptotic theory confidence interval.
# finally, we'll generate CIs by each method, asymptotic (inverse FIM) and 
# bootstrap (cov), and compare the coverage probabilities.
#############################################################

library(boot)
library(parallel)
library(wei.series.md.c1.c2.c3)

theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]

# number of CPU cores to use in bootstrap for parallel processing
ncores <- 4

# helps the MLE optimization routine converge more quickly and reliably
parscale <- c(1, 1000, 1, 1000, 1, 1000, 1, 1000, 1, 1000)

#set.seed(134849131)

# sample sizes
ns <- c(30, 50, 100, 200, 400)
# masking probabilities, no masking and 21.5% masking
ps <- c(0, 0.215)
# quantiles of weibull series distribution, no right-censoring and 25% right-censoring
qs <- c(1, 0.75)

sim.name <- "sim-1-boot"

sim.boot.run <- function(n, p, q, R = 1000) {

    problems <- list()

    tau <- wei.series.md.c1.c2.c3::qwei_series(
        p = q, scales = scales, shapes = shapes)

    cat("n =", n, ", p =", p, ", q = ", q, ", tau = ", tau, "\n")
  
    result <- tryCatch({
        df <- wei.series.md.c1.c2.c3::generate_guo_weibull_table_2_data(
            shapes = shapes,
            scales = scales,
            n = n,
            p = p,
            tau = tau)

        sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
            df = df,
            theta0 = theta,
            reltol = 1e-7,
            parscale = parscale,
            maxit = 2000L)

        cat("mle: ", sol$par, "\n")

        sol.boot <- boot(df, function(df, i) {
            sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
                df = df[i, ],
                theta0 = sol$par,
                reltol = 1e-7,
                parscale = parscale,
                maxit = 1000L)
            cat("boot: ", sol$par, "\n")
            sol$par
        }, ncpus = ncores, R = R)

        saveRDS(list(n = n, p = p, q = q, tau = tau, mle = sol, mle.boot = sol.boot),
            file = paste0("./results/", sim.name, "/results_", n, "_", p, "_", q, ".rds"))

        }, error = function(e) {
            print(e)
            problems <- append(problems, list(list(
                error = e, n = n, p = p, q = q, tau = tau, df = df)))
        })

    if (length(problems) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, problems = problems),
                file = paste0("./problems/", sim.name, "/problems_", n, "_", p, "_", q, ".rds"))
    }
}
  
params <- expand.grid(n = ns, p = ps, q = qs)
result <- mclapply(
    1:nrow(params),
    function(i) sim.boot.run(sim.name, params$n[i], params$p[i], params$q[i]),
    mc.cores = ncores)
```
