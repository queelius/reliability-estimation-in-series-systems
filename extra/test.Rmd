---
title: "Bootstrapping statistics of the maximum likelihood estimator of components in a series systems from masked failure data"
author: "Alex Towell"
abstract: "We estimate the parameters of a series system with Weibull component lifetimes from relatively small samples consisting of right-censored system lifetimes and masked component cause of failure. Under a set of conditions that permit us to ignore how the component cause of failures are masked, we assess the bias and variance of the estimator. Then, we assess the accuracy of the boostrapped variance and calibration of the confidence intervals of the MLE under a variety of scenarios."
output:
    pdf_document:
        toc: yes
        toc_depth: 2
        number_sections: true
        extra_dependencies: ["graphicx","amsthm","amsmath","natbib","tikz"]
        df_print: kable
        keep_tex: true
indent: true
bibliography: refs.bib
csl: the-annals-of-statistics.csl
---

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{condition}{Condition}
\renewcommand{\v}[1]{\boldsymbol{#1}}
\numberwithin{equation}{section}

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# cran packages
cran_packages <- c("readr", "latex2exp", "tidyverse", "devtools", "dplyr", "ggplot2", "gridExtra", "grid", "glue", "png")
github_packages <- c("queelius/md.tools", "queelius/wei.series.md.c1.c2.c3", "queelius/algebraic.mle")

for (pkg in cran_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg)
    }
    eval(parse(text = paste0("library(", pkg, ")")))
}
for (pkg in github_packages) {
    if (!require(pkg, quietly = TRUE)) {
        install_github(pkg)
    }
    # strip the namespace
    pkg <- gsub(".*\\/", "", pkg)
    eval(parse(text = paste0("library(", pkg, ")")))
}
```


## Solving the MLE {#sec:iterative}
According to @bain, if $\v\Omega$ is a Cartesian product of $l$ intervals,
partial derivatives of $L(\v\theta)$ exist, and the MLEs do not occur on the
boundary of $\v\Omega$, then the MLEs will be solutions of the simultaneous
equations
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0
\end{equation}
for $j=1,\ldots,p$ where $p$ is the number of components in $\v\theta$.
We call these equations the *maximum likelihood equations*.
If multiple solutions to Equation \eqref{eq:mle_eq} exist, each solution that
maximizes the likelihood function is a valid MLE.

If there is no closed-form solution to the maximum likelihood equations
\eqref{eq:mle_eq}, we may use iterative root-finding methods to
numerically approximate a solution.
A general approach is, if we have a guess $\v\theta^{(n)}$, take a step in
a "promising" direction $\v{d}^{(n)}$ to obtain the next guess,
\begin{equation}
\label{eq:iterative_update}
\v\theta^{(n+1)} = \v\theta^{(n)} + \alpha^{(n)} \v{d}^{(n)},
\end{equation}
where $\v\theta^{(0)}$ is an initial guess in the parameter space $\Omega$ that is
sufficiently close to the MLE $\hat{\v\theta}$. In our case, we use the
parameter vector $\v\theta^{(0)} = \v\theta$ as our initial guess, since we know
the true value $\v\theta$ in our simulation studies, but if plausible initial
guesses are not known, then global methods may be used to find a good initial
guess, like Simulated Annealing.

Assuming that at $\v\theta^{(n)}$, a sufficiently small step in the direction
$\v{d}^{(n)}$ results in an improvement with respect to the log-likelihood function,
we say that we *overshoot* if
$$
\ell(\v{\theta}^{(n+1)}) < \ell(\v{\theta}^{(n)}).
$$
The value $\alpha^{(n)}$ in Equation \eqref{eq:iterative_update} is a positive
real number chosen by a *line search* method so that we do not overshot, which
has an optimal value given by
$$
\alpha^{(n)} \in \operatorname{argmax}_{\alpha}
    \ell(\v{\theta}^{(n)} + \alpha \v{d}^{(n)}).
$$
However, this may be too computationally expensive to compute, and so we use
a less optimal but faster method known as *backtracking*.
In the backtracking line search method, we determine $\alpha^{(n)}$ by initially
letting $\alpha^{(n)} = 1$ and then, if we overshoot, redo the update
with $\alpha^{(n)} \gets r \alpha^{(n)}$, $0 < r < 1$, repeating until we
do not overshoot.

Note that this is not necessarily the best course of action for finding
global maximums, since depending on our initial guess $\v\theta^{(0)}$, we may
get stuck in a local maximum.
A global search method, like Simulated Annealing, may be used to find
better initial guesses.

We do as many iterations in Equation \eqref{eq:iterative_update} as necessary to
satisfy some *stopping condition*, which is usually something simple like the
distance between $\v\theta^{(n)}$ and $\v\theta^{(n+1)}$ being sufficiently
small. Under the right conditions, for sufficiently large $n$,
$\v\theta^{(n)} \approx \hat{\v\theta}$.

We use a popular technique known as Newton-Raphson method, which is obtained by
letting $\v{d}^{(n)}$ be defined as
$$
\v{d}^{(n)} = -J^{-1}(\v\theta^{(n)}) \nabla \ell(\v\theta^{(n)}),
$$
where $J(\v\theta^{(n)})$ and $\nabla \ell(\v\theta^{(n)})$ are respectively
the observed Fisher information matrix (Hessian of the
log-likelihood function) and the score (gradient of the log-likelihood
function), each evaluated at $\v\theta^{(n)}$.

## Properties of the MLE {#sec:mle_properties}

In this section, we discuss some properties of the MLE that are useful for
making statistical inferences about the parameter vector $\v\theta$.
According to @bain, if certain regularity conditions are satisfied, then
solutions of the maximum likelihood equation \eqref{eq:mle_eq} have the
following desirable properties:

1. $\v{\hat\theta}$ exists and is unique.

2. $\v{\hat\theta}$ is an asymptotically unbiased estimator.

3. $\v{\hat\theta}$ is asymptotically the UMVUE, the uniform minimum variance unbiased
   estimator.
   
4. $\v{\hat\theta}$ is asymptotically normal with a mean $\v\theta$ and a
   variance-covariance matrix that is the inverse of the Fisher information matrix (FIM),
   whose $(i,j)$-th component is given by
   $$
    I(\v\theta)_{i j} = n E_{\v\theta}\biggl(-\frac{\partial^2}{\partial \theta_i \partial \theta_j}
        \log f(S_i,\delta_i,\mathcal{C}_i;\v\theta)\biggr).
   $$
   
Usually, the *observed* FIM is used instead of the FIM, which is a conditioned on the
observed data,
$$
J(\v\theta)_{i j} = -\frac{\partial}{\partial \theta_i \partial \theta_j}
    \ell(\v\theta).
$$
If $\v\theta$ is unknown, $J(\v\theta)$ may be estimated with $J(\v{\hat\theta})$.
Thus, assuming the regularity conditions are satisfied, then approximately,
$$
    \hat{\v\theta} \sim \mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))
$$
and as the sample size goes to infinity, $\hat{\v\theta}$ converges in
distribution to $\mathcal{N}(\hat{\v\theta},J^{-1}(\hat{\v\theta}))$.

This is the asymptotic sampling distribution of MLE, but for small samples, it can be
a very poor approximation. In the next section, we discuss the bootstrap
method. In particular, bootstrapping the variance and confidence intervals.

Bootstrapping the Variance and Confidence Intervals of the MLE {#sec:boot}
===============================================================
The bootstrap method is a powerful, general purpose tool for estimating
the sampling distribution of a statistic, in our case statistics of the MLE,
that does not rely on making strong assumptions about the
underlying distribution of the data.

The most common form of the Bootstrap method is the non-parametric Bootstrap.
In the non-parametric bootstrap, the random data is created by resampling with
replacement from the original data and then computing the statistic of interest
on the resampled data. This is repeated $B$ times, giving us $B$
bootstrap replicates of the statistic. The sampling distribution of
the statistic is then approximated by the empirical distribution of the
bootstrap replicates. Since we do not know (nor do we attempt to model) the way
candidate sets are generated, this non-parametric form is ideal.

We are particularly interested in two statistics of the MLE: the variance and
the confidence interval. We will bootstrap the confidence interval using the
percentile method, which does not explicitly depend on the variance estimate. However,
conceptually, they are still related: a higher variance should
generally lead to a larger confidence interval.

In our simulation study, we will assess the performance of the bootstrapped variance
by comparing it to the empirical variance of the MLE, and we will assess
the performance of the bootstrapped confidence interval by computing
its coverage probability, i.e., a 95% confidence interval should
contain the true value 95% of the time. We say that a confidence interval
has *good coverage* if its coverage probability is close to the nominal
confidence level.

If the confidence intervals have good coverage, a small confidence interval width
means we are more confident that the true value of $\v\theta$ is close to the MLE,
and a large confidence interval width means we are less confident that the true value
of $\v\theta$ is close to the MLE. However, if the confidence interval has poor
coverage, then the confidence interval width is not particularly informative.
Thus, we see that the confidence interval is only useful if it has good coverage,
and so we will focus on assessing the coverage probability of the confidence intervals
in our simulation study.

Simulation Study: Series System with Weibull Components {#sec:weibull}
=============================================
In the real world, systems are quite complex:

1. They are not perfect series systems.

2. The components in a system are not independent.

3. The lifetimes of the components are not precisely modeled by
   any named probability distributions.
   
4. The components may depend on many other unobserved factors.

With these caveats in mind, we model the data as coming from a Weibull series
system of $m = 5$ components, and other factors, like ambient temperature, are
either negligible (on the distribution of component lifetimes) or are more or less
constant.

The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{WEI}(\v{\theta_j})
$$
where $\v{\theta_j} = (k_j, \lambda_j)$ for $j=1,\ldots,m$.
Thus, $\v\theta = (\v{\theta_1},\ldots,\v{\theta_m})' = \bigl(k_1,\lambda_1,\ldots,k_m,\lambda_m\bigr)$.
The random variable $T_{i j}$ has a reliability function, pdf, and hazard function
given respectively by
\begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align}
where $t > 0$ is the lifetime, $\lambda_j > 0$ is the scale parameter and $k_j > 0$
is the shape parameter. The shape parameters $k_1, \ldots, k_m$ have the following interpretations:

\begin{enumerate}
\item[$k_j < 1$] The hazard function decreases with respect to time. For instance,
  this may occur as a result of defective components being weeded out early. This
  is known as the *infant mortality* phase.
\item[$k_j = 1$] The hazard function is constant with respect to time. This is an
  idealized case that is rarely observed in practice, but may be useful for modeling
  purposes.
\item[$k_j > 1$] The hazard function increases with respect to time. For instance,
  this may occur as a result of components wearing out. This is known as the
  *aging* phase.
\end{enumerate}

The lifetime of the series system composed of $m$ Weibull components
has a reliability function given by
\begin{equation}
\label{eq:sys_weibull_reliability_function}
R(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{equation}
\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
R(t;\v\theta) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R(t;\v\theta)
    &= \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}\\
    &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
\end{proof}

The Weibull series system's hazard function is given by
\begin{equation}
\label{eq:sys_weibull_failure_rate_function}
h(t;\v\theta) =
    \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},
\end{equation}
whose proof follows from Theorem \ref{thm:sys_failure_rate}.

The pdf of the series system is given by
\begin{equation}
\label{eq:sys_weibull_pdf}
f(t;\v\theta) =
\biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{equation}
\begin{proof}
By definition,
$$
f(t;\v\theta) = h(t;\v\theta) R(t;\v\theta).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
\end{proof}


## System Reliability
```{r sim-study-design, echo = F}
theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)
shapes <- theta[grepl("shape", names(theta))]
scales <- theta[grepl("scale", names(theta))]
```

A series system is only as reliable as its least reliable component. 
In order to make the simulation study representative of real-world scenarios, at least
for systems designed to be reliable, we choose parameter values that are representative of
real-world systems where there is no single component that is much less reliable than the
others.

One way to define reliability is by the mean time to failure (MTTF), which is the
expected value of the lifetime, which for the Weibull distribution is given by
$$
\text{MTTF} = k \, \Gamma(1 + 1/\lambda),
$$
where $\Gamma$ is the gamma function.

We consider the data from [@Huairu-2013], which includes a study of the reliability of a series system
with three Weibull components with shape and scale parameters given by
\begin{equation}
\begin{aligned}
    k_1 = `r theta["shape1"]` &\quad \lambda_1 = `r theta["scale1"]`\\
    k_2 = `r theta["shape2"]` &\quad \lambda_2 = `r theta["scale2"]`\\
    k_3 = `r theta["shape3"]` &\quad \lambda_3 = `r theta["scale3"]`.
\end{aligned}
\end{equation}

Our approach is to extend this system to a five component system by adding two
more components with shape and scale parameters given by
\begin{equation}
\begin{aligned}
    k_4 = `r theta["shape4"]` &\quad \lambda_4 = `r theta["scale4"]`\\
    k_5 = `r theta["shape5"]` &\quad \lambda_5 = `r theta["scale5"]`.
\end{aligned}
\end{equation}

```{r table-2, table.attr = "style='width:50%;'", echo = F, results = 'asis', fig.cap = "Component Reliability", fig.align = "center"}

# let's also show the MTTF of the entire series system
mttf.sys <- integrate(function(t) { t * dwei_series(t, scales=scales,
    shapes=shapes) }, lower=0, upper=Inf)$value
mttf <- gamma(1 + 1/shapes) * scales
components <- data.frame(
  "MTTF" = mttf,
  row.names = paste("Component", 1:5)
)

components <- rbind(components, "Series System" = mttf.sys)
knitr::kable(components, caption = "Meean Time To Failure of Weibull Components and Series System")
```

As shown by Table 2, there are no components that are significantly less reliable
than any of the others.
Note that a series system in which, say, one of the components does have a significantly
shorter MTTF would also pose significant challenges to estimating the parameters of
the system from our masked failure data, since the failure time of the series system
would be dominated by the failure time of the least reliable component.

## Weibull Likelihood Model for Masked Data

In Section \ref{sec:like_model}, we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
$i$\textsuperscript{th} system is given by the following theorem.
\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\v\theta) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\v{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the following result.
\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\v\theta) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\v\theta$ since they do not
affect the MLE.
\end{corollary}

Since the systems are independent, the log-likelihood of the entire sample of $n$
observations is given by
\begin{equation}
\label{eq:weibull_log_likelihood}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta).
\end{equation}

## Numerically Solving the MLE

We may find an MLE by solving the maximum likelihood equation \eqref{eq:mle_eq},
i.e., a point $\v{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)$ satisfying
$\nabla_{\theta} \ell(\v{\hat\theta}) = \v{0}$, where $\nabla_{\v\theta}$
is the gradient of the log-likelihood function with respect to $\v\theta$, otherwise
known as the score.

To solve these ML equations, we use the Newton-Raphson method described in Section
\ref{sec:iterative}. In order to use the Newton-Raphson method, we need to
compute the gradient and Hessian of the log-likelihood function.

We analytically derive the gradient of the log-likelihood function (score), since it
is useful to have for the Newton-Raphson method, but we do not do the same for the
Hessian of the log-likelihood for the following reasons:

1. The Hessian is not necessarily needed since we often use some faster method to
approximate the Hessian, e.g., the BFGS method. Technically, we could also
numerically approximate the gradient too, but the gradient is much easier
to derive than the Hessian, and moreover, knowing the score precisely also
enables us to more accurately approximate the Hessian by taking the Jacobian
of the gradient.

2. The Hessian is more difficult to derive than the score, and so it is more
likely that we will make a mistake when deriving the Hessian. 

The following theorem derives the score function.
\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\v\theta) = \biggl(
    \frac{\partial \ell_i(\v\theta)}{\partial k_1},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\v\theta)}{\partial k_m},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the log-likelihood
contribution of the $i$-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation so the proof
has been omitted, but the result has been verified by using a very precise numerical
approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions is
the same of their gradients, and so the score function of the entire sample is given
by
\begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\v\theta) = \sum_{i=1}^n \nabla \ell_i(\v\theta).
\end{equation}

### Issues with the MLE {#sec:opt_rescale}

\textbf{Identifiability}: When estimating the parameters of latent components, we
must be careful to ensure that the parameters are identifiable such that the
likelihood function is maximized at a unique point. If the likelihood
function is not maximized at a unique point, then the MLE is not unique, and
a lot of the theory we have developed so far breaks down.

One way in which this problem may arise is if the data is not informative
enough. For example, if we have a series system and in the observed masked data
component $1$ is in the candidate set if and only if component $2$ is in
candidate set, then we do not have enough information to estimate the
parameters of component $1$ and component $2$ separately. In this case, we
could combine these two components into a single component. We lagely avoid this
problem by using the Bernoulli candidate set model, but sometimes it may still arise
by chance.

\textbf{Parameter rescaling}: When the parameters under investigation span different
orders of magnitude, parameter rescaling can significantly improve the performance
and reliability of optimization algorithms. Parameter rescaling gives an optimizer a
sense of the typical size of each parameter, enabling it to adjust its steps
accordingly. This is crucial in scenarios like ours, where shape and scale parametes
are a few orders of magnitude apart. Without rescaling, the optimization routine may
struggle, taking numerous small steps for larger parameters and overshooting for
smaller ones.

Speed of convergence was particularly important in our case, since in our simulation
study, we employ the Bootstrap method to estimate the sampling distribution of the
MLE, which requires us to estimate the MLE for many data sets. We found that
parameter rescaling significantly improved the speed of convergence, which allowed
us to run our simulation study in a tractable amount of time.

In the `optim` algorithm in the R package `stats`, we achieve this result by
assigning a `parscale` vector in line with the parameter magnitudes. It does not
matter what the values of the `parscale` vector are, only their relative magnitudes.
It does not need to be very precise, but since we are doing a simulation study,
we know the true parameter values and can use that information to scale them
appropriately.

## Simulation Design

In this section, we describe the design of our simulation study. We first describe
the simulation scenarios we consider, and then we describe how we generate data
for each scenario.

### Bernoulli Candidate Set Model

In our simulation study, we must generate data that satisfies the masking
conditions described in Section \ref{sec:candmod}.
There are many ways to satisfying the masking conditions. We choose the simplest
method, which we call the *Bernoulli candidate set model*. In this model, each
non-failed component is included in the candidate set with
a fixed probability $p$, independently of all other components and independently
of $\v\theta$, and the failed component is always included in the candidate set.

### Right-Censoring Model

We employ a very simple right-censoring model, where the right-censoring time
$\tau$ is fixed and independent of $\v\theta$ and the censoring time $S_i$ of
the $i$\textsuperscript{th} system.

We parameterize $\tau$ by quantiles of the series system, e.g., if $q = 0.8$,
then $\tau(q)$ is the $80\%$ quantile of the series system such that $80\%$ of
the series systems are observed (fail before time $\tau(q)$) and $20\%$ of the
series systems fail after time $\tau(q)$ (are right-censored).

### Scenarios

we vary the sample size $n$, the Bernoulli masking probability $p$ of including each
non-failed component in the candidate set, and the right-censoring time $\tau$. We
then analyze the performance of the MLE under these various scenarios.

Here is an outline of the simulation study analysis:

1. Set up simulation parameters for various scenarios of interest, such as
   generating data to examine the relationship between bias and masking probability
   for different sample sizes and right-censoring times.

2. Generate $R$ data sets for each scenario (some combination of $n$, $p$, and
   $\tau$).

3. Estimate the parameters for each data set, giving us $R$ estimates of the
   parameters. We use these data sets as an empirical estimate of
   the sampling distribution of the MLE for each scenario.
   
4. Using the empirical sampling distribution of the MLE, estimate various 
   performance measures of the MLE, like bias, variance, MSE, and coverage
   probability for each scenario.

5. Analyze and visualize the results, e.g., by plotting the bias, variance, MSE,
   and coverage probability as a function of $n$ for different combinations of $p$
   and $\tau$. 

   We then interpret the results and discuss the performance of the MLE estimator
   under various conditions. We expect that as $n \to \infty$, the bias and MSE
   will go to $0$ and the coverage probability will go to $0.95$ (when
   constructing $95\%$ confidence intervals). Of course, we do not expect these
   results to hold for finite $n$, but we would like to see how the bias, MSE, and
   coverage probability change as we vary $n$, $p$, and $\tau$.

For how we generate a scenario, see Appendix A.

### Verification

To verify that our likelihood model is correct, we load the Table 2 data from
[@Huairu-2013] and fit the Weibull series model to the data to see if we can
recover the MLE they reported. When we fit the Weibull series model to this data by
maximizing the likelihood function, we obtain the following fit for the shape and
scale parameters given respectively by
$$
    \hat{k}_1 = `r theta["shape1"]`,
    \hat{k}_2 = `r theta["shape2"]`,
    \hat{k}_3 = `r theta["shape3"]`,
$$
and
$$
    \hat{\lambda}_1 = `r theta["scale1"]`,
    \hat{\lambda}_2 = `r theta["scale2"]`,
    \hat{\lambda}_3 = `r theta["scale3"]`,
$$
which is in agreement with the MLE they reported. Satisfied that our likelihood model
is correct, we proceed with the simulation study.

## Bias, variance, and MSE of the MLE {#sec:acc_prec}

First, we estimate the bias, variance, and MSE of the MLE under various scenarios.
This is useful for understanding the accuracy and precision of the MLE under
different conditions. It is unrelated to the bootstrap method, but it is useful to
compute these quantities before we assess the bootstrapped variance and confidence
intervals.

A measure of the accuracy of $\v{\hat\theta}$ is the bias, which is defined as
$$
\operatorname{b}(\v{\hat\theta}) = E(\v{\hat\theta}) - \v\theta.
$$
We cannot analytically derive the bias, so we estimate the bias using the empirical
sampling distribution,
$$
\hat{\operatorname{b}}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}(\v{\hat\theta}) - \theta_j.
$$

We estimate the precision of $\hat\theta_j$ with the variance and MSE. The variance
of $\v{\hat\theta}$ is defined as
$$
\operatorname{Var}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - E_{\hat{\v\theta} \sim \text{data}}(\hat\theta_j))^2\bigr),
$$
where the expectation is taken with respect to the empirical sampling distribution.
The mean squared error is a measure of estimator error that incorporates both the
bias and the variance, and is defined as
$$
\operatorname{MSE}(\hat\theta_j) =
    E_{\hat{\v\theta} \sim \text{data}}\bigl((\hat\theta_j - \theta_j)^2\bigr).
$$

Assuming the regularity conditions for the MLE are met, the MSE converges in
probability to the variance.

## Simulation Scenarios

We consider many different scenarios, where we vary the sample size $n$, the masking
probability $p$, and the right-censoring time $\tau$. We then analyze the performance
of the MLE under these various scenarios by estimating the bias, variance, and MSE
of the MLE.

### Absolute bias vs. sample size with a masking probability but no right-censoring {-}

In this scenario, we want to see the bias of the MLE as a function of the sample
size $n$ from $n = 30$ to $n = 800$ for a fixed masking probability $p = 0.2$ and no
right-censoring $(\tau = \infty)$. Recall that the masking probability is the
probability of including each non-failed component.


```{r plot-bias-p-0.2-vs-sample-size, echo =F, fig.cap="Bias vs. sample size (masking probability 0.2)", fig.align="center"}
knitr::include_graphics("image/plot-p-0.2-bias-vs-sample-size.pdf")
```

In Figure \ref{fig:plot-bias-p-0.2-vs-sample-size}, we plot the absolute bias
$|\operatorname{bias}(\hat\theta)|$ on a log scale against the sample size. 
However, because the absolute bias is quite large for small sample sizes and small
for large sample sizes, we use a log scale. Furthermore, we show the absolute bias
for the shape and scale parameters separately, since the scale parameters are much
larger than the shape parameters.

Here are some important observations Figure \ref{fig:plot-bias-p-0.2-vs-sample-size} reveals:

1. For both shape and scale parameters, we see that the absolute bias seems to be
decreasing to zero as the sample size increases. This is not surprising since we
expect the MLE to be consistent, i.e., $\v{\hat\theta}$ converges in probability
to $\v\theta$ as the sample size increases to infinity. Still, it is reassuring to
see that the bias seems to be behaving as expected.

2. For the shape parameters, which are small (the shape parameters have true values
a little larger than 1), the bias is relatively large for sample sizes up to
$100$.

3. For the scale parameters, which are quite large (the scale parameters have true
values around 1000). Like with the shape parameters, the bias is relatively large for
sample sizes up to $100$, but seems to stabilize and reach relatively small values
after that.

### Scenario: Bias vs. sample size and masking probability and no right-censoring {-}

Now, we take a larger view and plot the bias (without taking its absolute value
as we had done previously) against the masking probabilities $p = 0$
(no masking) to $p = 0.4$ (significant masking) for sample sizes 100, 400, and 800.

For the shape parameters, at a sample of size 100, we see significant bias and we
also see that it is very sensitive to the masking probability. See Figure
\ref{fig:plot-bias-shapes-vs-p-sample-size-100-400-800}. However, for sample sizes
of 400 and 800, the bias is relatively small and unaffected by the masking
probability.

```{r plot-bias-shapes-vs-p-sample-size-100-400-800, echo =F, fig.cap="Shape Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-shapes-p-vs-sample-size-100-400-800.pdf")
```

For the scale parameters, a similar pattern emerges, although we see that even
for sample size 400, there is evidence that the bias is still affected by the masking
probability. See Figure \ref{fig:plot-bias-scales-vs-p-sample-size-100-400-800}.

The smallest bias, as expected, occurs for sample sizes of $800$. The bias 
for $\lambda_1$ (scale parameter 1) at the masking probability $0.3$ is an interesting
case, since it jumps up at that point for some reason. We used only $R = 100$
replications, so it is plausible it would decrease with more replications.
Regardless, the overall trend is that the bias decreases as the sample size increases, and its
dependence on the masking probability is relatively small with sufficiently large
sample sizes.

```{r plot-bias-scales-vs-p-sample-size-100-400-800, echo =F, fig.cap="Scale Bias vs. masking probability for sample sizes 100, 400, and 800", fig.align="center"}
knitr::include_graphics("image/plot-bias-scales-p-vs-sample-size-100-400-800.pdf")
```

#### Scenario: Bias vs. right-censoring time and sample size with a fixed masking probability {-}

In this scenario, we want to isolate the effect of the right-censoring time $\tau$
on the bias. We fix the masking probability to $p = 0.215$, in line with the masking
probability we estimate for the Table 2 data set in [@Huairu-2013].

We plot the bias against the right-censoring
time for sample sizes 50, 150, and 300. See Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}.
On the $x$-axis, we report the right-censoring time as a quantile of the
Weibull series distribution so that we can more clearly see the effect of the right-censoring
on the bias, e.g., the $50\%$ quantile is the time at which $50\%$ of the
systems are expected to fail.

```{r plot-bias-tau-vs-sample-size-50-150-300, echo =F, fig.cap="Bias vs. right-censoring time and sample sizes 50, 150, and 300", fig.align="center"}
knitr::include_graphics("image/plot-bias-tau-vs-sample-size-50-150-300.pdf")
```

A few observations about Figure \ref{fig:plot-bias-tau-vs-sample-size-50-150-300}:

1. The bias decreases as the right-censoring time increases. This is expected since we have
more information about the system when the right-censoring time is larger.

2. The bias decreases as the sample size increases, which is also expected since we
have more information about the system when the sample size is larger.

3. The bias is relatively small for sample sizes 150 and 300, but for sample size 50,
the bias is quite large, particularly for the shape parameters. This is not
surprising since the sample size is quite small, and so we do not expect the MLE to
be very accurate.


### Variance

```{r var.plot, echo=F, fig.cap="Variance vs. sample size", fig.align="center"}

# Load the data
sim_data <- read_csv("./results/data/data-fim.csv")

# rename columns for plotting
sim_data <- sim_data %>% rename(
  "Shape 1" = mle.1,
  "Scale 1" = mle.2,
  "Shape 2" = mle.3,
  "Scale 2" = mle.4,
  "Shape 3" = mle.5,
  "Scale 3" = mle.6,
  "Shape 4" = mle.7,
  "Scale 4" = mle.8,
  "Shape 5" = mle.9,
  "Scale 5" = mle.10
)

# Filter data
filtered_data <- sim_data %>% filter(p == 0 & q == 1)

# Compute variances
shape_data <- filtered_data %>% select(n, "Shape 1", "Shape 2", "Shape 3", "Shape 4", "Shape 5")
scale_data <- filtered_data %>% select(n, "Scale 1", "Scale 2", "Scale 3", "Scale 4", "Scale 5")

shape_variances <- shape_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)
scale_variances <- scale_data %>% group_by(n) %>% summarise_all(var, na.rm = TRUE)

# Reshape the data for ggplot2
shape_variances_long <- shape_variances %>% gather(key = "Parameter", value = "Variance", -n)
scale_variances_long <- scale_variances %>% gather(key = "Parameter", value = "Variance", -n)

# Plotting
ggplot(shape_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Shape Parameter MLEs with respect to Sample Size") +
  theme_minimal()

ggplot(scale_variances_long, aes(x = n, y = Variance, colour = Parameter)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(x = "Sample Size (n)", y = "Variance", 
       title = "Variance of Scale Parameter MLEs with respect to Sample Size") +
  theme_minimal()
```

## Coverage Probability of Bootstrapped Confidence Intervals {#sec:coverage_prob}
Under a variety of scenarios, we will bootstrap a $95\%$-confidence interval for
$\v\theta$ using the percentile method, and we will evaluate its accuracy by
computing the coverage probability.

We want the coverage probability to be close to the nominal level, $95\%$, because if
the coverage probability is too low, then we will be too confident in the precision
and accuracy of the MLE, and if the coverage probability is too high, then we will
not be confident enough in the precision and accuracy of the MLE.

To estimate the coverage probability, we use the following procedure:

1. For a given scenario, we generate $R = 300$ data sets.

2. We find an MLE for each of $R$ data sets.

3. We bootstrap the $95\%$-confidence interval for each MLE.

4. We compute the coverage probability by computing the proportion of times
   the true parameter $\v\theta$ is contained in $95\%$-confidence
   interval.

## Simulation Scenarios

### Scenario: Coverage probability vs. sample size with a fixed masking probability and no right-censoring {-}

We want to isolate the effect of the coverage probability as a function of the sample
size $n$. We fix the masking probability to $p = 0.2$ and without right-censoring $(\tau = \infty)$
and vary the sample size from $n = 30$ to $n = 800$. See Figure \ref{fig:plot-coverage-p-three-vs-sample-size}.

```{r plot-coverage-p-three-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size for masking probability $0.3$", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p_0.3-vs-sample-size.pdf")
```

Here are some key observations:

1. It is immediately obvious that the scale parameters (dashed lines) have a much
   lower coverage probability than the shape parameters (solid lines), particularly
   for small sample sizes less than $n = 200$.
   
   In general, the scale parameters appear to be more difficult to estimate than the
   shape parameters. 

2. As the sample size increases, the coverage probability for the shape parameters
   and scale parameters approaches the nominal level, $95\%$.
   
   This suggests that the sampling distribution of the MLE is converging in
   distribution to a multivariate normal distribution with mean $\v\theta$ and
   variance-covariance given by the inverse of the FIM, consistent with the
   asymptotic theory.

### Scenario: Coverage probability vs. sample size and masking probability without right-censoring {-}

We want to get a larger picture of how the coverage probability depends on the
sample size $n$ and masking probability $p$. We fix the right-censoring time to
$\tau = \infty$ and vary the sample size from $n = 30$ to $n = 800$ and vary the
masking probability from $p = 0$ (no masking) to $p = 0.4$ and then compute the
coverage probability for each combination of sample size $n$ and masking probability
$p$.

```{r plot-coverage-p-vs-sample-size, echo =F, fig.cap="Coverage probability vs. sample size", fig.align="center"}
knitr::include_graphics("image/plot-coverage-p-vs-sample-size.pdf")
```

The results of this analysis are summarized by Figure \ref{fig:plot-coverage-p-vs-sample-size}.
Here are some key observations:

1. For sample sizes $n \leq 100$, the coverage probability for the shape parameters
is close to the nominal level, $95\%$, only for small masking probabilities. However,
as the sample size increases, the coverage probability for the shape parameters
quickly approaches the nominal level, $95\%$, for all masking probabilities reported
here.

2. For the scale parameters, the coverage probability is too low for all sample sizes
$n < 200$ for all masking probabilities reported here. For small sample sizes, the
confience intervals particularly for the scale parameters, should probably be taken
with a grain of salt.

In Section \ref{sec:boot}, we explore an alternative way to construct confidence
intervals using the bootstrap method, which is generally a more accurate way to
compute confidence intervals. Unlike the inverse of the observed FIM, it does not
assume that the sampling distribution of the MLE is asymptotically normal, and so
it is more robust to violations of this assumption.


# Conclusion

We have developed a likelihood model for series systems with latent components
and right-censoring. We have provided evidence that, as long as certain regularity
conditions are met, the MLE is asymptotically unbiased and consistent.

# References {-}

Please see below for a full list of references.

```{r refmgr, echo=FALSE, results='asis'}

cat("<div id=\"refs\"></div>")
```

# Appendix {#app}

```{r eval=F}
#\renewcommand{\thesection}{\Alph{section}}
#\setcounter[section}{0}
```

## Data {#app:data}

## Simulation Code {-}

```{r eval = FALSE}
#############################################################
# Simulation data generating process for specified scenario #
# (n, p, q), where:                                         #
#    - n is a vector of sample sizes                        #
#     - n is a vector of sample sizes                       #
#     - p is a vector of masking probabilities              #
#     - q is a vector of right-censoring quantiles of the   #
#       Weibull series distribution.                        #
#############################################################

# here is the R libary we developed for this project
library(wei.series.md.c1.c2.c3) 

# for parallel processing
library(parallel)

# you can set a seed for reproducibility of the experimental run
# however, if you use parallel processing, this simple approach will not work.
# set.seed(1234)

###############################################
# Here is an example of how to run a scenario #
###############################################

# set the simulation name to be used in the file names
sim.name <- "sim-2"
# set the sample sizes
ns <- c(30, 40, 50, 75, 100, 200, 400, 800)
# set the masking probabilities
ps <- seq(0, 0,1, 0.2, 0.3, 0.4)
# set the right-censoring quantiles
qs <- c(0.5, 0.6, 0.7, 0.8, 0.9, 0.95)
# set the number of replicates
R <- 100
# set the number of CPU cores to use
ncores <- 4

# true parameter values
theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]

# helps the MLE optimization routine converge more quickly and reliably
# by scaling the parameters to be of similar magnitude
parscale <- c(1, 1000, 1, 1000, 1, 1000, 1, 1000, 1, 1000)

sim.run <- function(sim.name, n, p, q, R = 1000) {
    mles <- list()
    problems <- list()

    tau <- wei.series.md.c1.c2.c3::qwei_series(
        p = q, scales = scales, shapes = shapes)

    cat("n =", n, ", p =", p, ", q = ", q, ", tau = ", tau, "\n")

    for (r in 1:R) {
        result <- tryCatch({

            df <- wei.series.md.c1.c2.c3::generate_guo_weibull_table_2_data(
                shapes = shapes,
                scales = scales,
                n = n,
                p = p,
                tau = tau)

            sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
                df = df,
                theta0 = theta,
                reltol = 1e-7,
                parscale = parscale,
                maxit = 2000L)
            mles <- append(mles, list(sol))

            if (r %% 10 == 0) {
                cat("r = ", r, ": ", sol$par, "\n")
            }

        }, error = function(e) {
            cat("Error at iteration", r, ":")
            print(e)
            problems <- append(problems, list(list(
                error = e, n = n, p = p, q = q, tau = tau, df = df)))
        })
    }
  
    if (length(mles) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, mles = mles),
            file = paste0("./results/", sim.name, "/results_", n, "_", p, "_", q, ".rds"))
    }

    if (length(problems) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, problems = problems),
            file = paste0("./problems/", sim.name, "/problems_", n, "_", p, "_", q, ".rds"))
    }
}

params <- expand.grid(n = ns, p = ps, q = qs)
result <- mclapply(
    1:nrow(params),
    function(i) sim.run(sim.name, params$n[i], params$p[i], params$q[i], R),
    mc.cores = ncores)

```


## Appendix B: Simulation of scenarios using the Bootstrap method {-}

```{r, eval=FALSE}
#############################################################
# in this scenario, we want to see how we can use the bootstrap
# method to estimate the confidence intervals more precisely (better calibration
# of confidence intervals) for small sample sizes.
# we'll use it to construct a 95% confidence interval for the estimator. we'll
# compare this result to the asymptotic theory confidence interval.
# finally, we'll generate CIs by each method, asymptotic (inverse FIM) and 
# bootstrap (cov), and compare the coverage probabilities.
#############################################################

library(boot)
library(parallel)
library(wei.series.md.c1.c2.c3)

theta <- c(shape1 = 1.2576, scale1 = 994.3661,
           shape2 = 1.1635, scale2 = 908.9458,
           shape3 = 1.1308, scale3 = 840.1141,
           shape4 = 1.1802, scale4 = 940.1141,
           shape5 = 1.3311, scale5 = 836.1123)

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]

# number of CPU cores to use in bootstrap for parallel processing
ncores <- 4

# helps the MLE optimization routine converge more quickly and reliably
parscale <- c(1, 1000, 1, 1000, 1, 1000, 1, 1000, 1, 1000)

#set.seed(134849131)

# sample sizes
ns <- c(30, 50, 100, 200, 400)
# masking probabilities, no masking and 21.5% masking
ps <- c(0, 0.215)
# quantiles of weibull series distribution, no right-censoring and 25% right-censoring
qs <- c(1, 0.75)

sim.name <- "sim-1-boot"

sim.boot.run <- function(n, p, q, R = 1000) {

    problems <- list()

    tau <- wei.series.md.c1.c2.c3::qwei_series(
        p = q, scales = scales, shapes = shapes)

    cat("n =", n, ", p =", p, ", q = ", q, ", tau = ", tau, "\n")
  
    result <- tryCatch({
        df <- wei.series.md.c1.c2.c3::generate_guo_weibull_table_2_data(
            shapes = shapes,
            scales = scales,
            n = n,
            p = p,
            tau = tau)

        sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
            df = df,
            theta0 = theta,
            reltol = 1e-7,
            parscale = parscale,
            maxit = 2000L)

        cat("mle: ", sol$par, "\n")

        sol.boot <- boot(df, function(df, i) {
            sol <- wei.series.md.c1.c2.c3::mle_nelder_wei_series_md_c1_c2_c3(
                df = df[i, ],
                theta0 = sol$par,
                reltol = 1e-7,
                parscale = parscale,
                maxit = 1000L)
            cat("boot: ", sol$par, "\n")
            sol$par
        }, ncpus = ncores, R = R)

        saveRDS(list(n = n, p = p, q = q, tau = tau, mle = sol, mle.boot = sol.boot),
            file = paste0("./results/", sim.name, "/results_", n, "_", p, "_", q, ".rds"))

        }, error = function(e) {
            print(e)
            problems <- append(problems, list(list(
                error = e, n = n, p = p, q = q, tau = tau, df = df)))
        })

    if (length(problems) != 0) {
        saveRDS(list(n = n, p = p, q = q, tau = tau, problems = problems),
                file = paste0("./problems/", sim.name, "/problems_", n, "_", p, "_", q, ".rds"))
    }
}
  
params <- expand.grid(n = ns, p = ps, q = qs)
result <- mclapply(
    1:nrow(params),
    function(i) sim.boot.run(sim.name, params$n[i], params$p[i], params$q[i]),
    mc.cores = ncores)
```
