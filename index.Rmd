---
title: "Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data"
author: "Alex Towell"
abstract: "Accurately estimating reliability of individual components in multi-component systems is challenging when only system-level failure data is observable. This paper develops maximum likelihood techniques to estimate component reliability from right-censored lifetimes and candidate sets indicative of masked failure causes in series systems. A likelihood model accounts for right-censoring and candidate sets. Extensive simulation studies demonstrate accurate and robust performance of the maximum likelihood estimator despite small samples and significant masking and censoring. The bias-corrected accelerated bootstrap provides well-calibrated confidence intervals. The methods expand the capability to quantify latent component properties from limited system reliability data. Key contributions include derivations of likelihood models and validation of estimation techniques via simulations. Together, these advance rigorous component reliability assessment from masked failure data."
output:
    #bookdown::html_document2:
    #bookdown::pdf_document2:
    bookdown::gitbook:
        df_print: kable
        citation_package: natbib
    bookdown::pdf_book:
        toc: true
        #toc_depth: 3
        number_sections: true
        #extra_dependencies: ["hyperref", "graphicx","amsthm","amsmath","natbib","tikz"]
        extra_dependencies: ["tikz", "caption"]
        df_print: kable
        #keep_tex: true
        citation_package: natbib
indent: true
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{tikz}
   - \usepackage{caption}
   - \usepackage{amsthm}
   - \renewcommand{\v}[1]{\boldsymbol{#1}}
   - \theoremstyle{definition}
   - \newtheorem{condition}{Condition}
   - \theoremstyle{plain}
bibliography: refs.bib
link-citations: true
#biblio-style: apalike
#biblio-style: plainnat
natbiboptions: "numbers"
biblio-style: amsplain
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(md.tools)
library(algebraic.mle)
library(wei.series.md.c1.c2.c3)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(glue)
library(png)
library(kableExtra)

# \newtheorem{definition}{Definition}
# \newtheorem{theorem}{Theorem}
# \newtheorem{corollary}{Corollary}
# \numberwithin{equation}{section}
```

Introduction
============
Accurately estimating the reliability of individual components in multi-component
systems is an important challenge, as component lifetimes and failure causes are
often not directly observable. In a series system [@Agustin-2011], only
system-level failure times may be recorded along with limited information about
the failed component. Such masked data poses difficulties for assessing component
reliability.

This paper develops and validates maximum likelihood techniques to estimate
component reliability from right-censored lifetime data and candidate sets
indicative of component failure causes. The key results are:

- Deriving a likelihood model incorporating right-censoring and candidate sets
  to enable masked data to be used for parameter estimation.

- Demonstrating through simulation studies that the maximum likelihood estimator
  performs well despite small samples and significant masking and
  right-censoring.
  
- Estimation of scale parameters is more robust than shape parameters in the
  Weibull model.

- Showing that bootstrapping provides reasonably well-calibrated confidence
  intervals for the maximum likelihood estimates, even with small sample sizes.

The remainder of the paper details the series system and likelihood models,
maximum likelihood estimation methodology, bootstrap confidence interval
estimation, and extensive simulation studies exploring estimator behavior under
various sample sizes, masking levels, and model assumptions. Together, these
contributions provide a statistically rigorous framework for learning about
latent component properties from limited observational data on system
reliability. The proposed methods expand the capability to quantify component
lifetimes in situations where failure data is significantly masked.

Series System Model {#statmod}
==================================
Consider a system composed of $m$ components arranged in a series configuration.
Each component and system has two possible states, functioning or failed.
We have $n$ systems whose lifetimes are independent and identically distributed (i.i.d.).
The lifetime of the $i$\textsuperscript{th} system is denoted by the random variable $T_{i}$
and the lifetime of its $j$\textsuperscript{th} component is denoted by the random variable $T_{i j}$.
We assume the component lifetimes in a single system are statistically independent and non-identically distributed.
Here, lifetime (or lifespan) is defined as the elapsed time from when the new, functioning component
(or system) is put into operation until it fails for the first time.
A series system fails when any component fails, thus the lifetime of the $i$\textsuperscript{th}
system is given by the component with the shortest lifetime,
$$
    T_i = \min\bigr\{T_{i 1},T_{i 2}, \ldots, T_{i m} \bigr\}.
$$

There are three particularly important distribution functions in reliability analysis: the
reliability function, the probability density function, and the hazard function.
The reliability function, $R_{T_i}(t)$, is the
probability that the $i$\textsuperscript{th} system has a lifetime greater than 
given duration $t$,
\begin{equation}
R_{T_i}(t) = \Pr\{T_i > t\}\\
\end{equation}
The probability density function (pdf) of $T_i$ is denoted by
$f_{T_i}(t)$ and may be defined as
$$
    f_{T_i}(t) = -\frac{d}{dt} R_{T_i}(t).
$$
Next, we introduce the hazard function.
The probability that a failure occurs between $t$ and $\Delta t$ given that no
failure occurs before time $t$ is given by
$$
\Pr\{T_i \leq t+\Delta t|T_i > t\} = \frac{\Pr\{t < T_i < t+\Delta t\}}{\Pr\{T_i > t\}}.
$$
The failure rate is given by the dividing this equation by the length of the time
interval, $\Delta t$:
$$
\frac{\Pr\{t < T_i < t+\Delta t\}}{\Delta t} \frac{1}{\Pr\{T_i > t\}} =
    -\frac{R_{T_i}(t+\Delta t) - R_{T_i}(t)}{\Delta t} \frac{1}{R_{T_i}(t)}.
$$
The hazard function $h_{T_i}(t)$ for $T_i$ is the instantaneous failure rate at time $t$,
which is given by
\begin{equation}
\label{eq:failure_rate}
\begin{split}
h_{T_i}(t) 
  &= -\lim_{\Delta t \to 0} \frac{R_{T_i}(t+\Delta t) - R_{T_i}(t)}{\Delta t}
    \frac{1}{R_{T_i}(t)}\\
  &= \left(-\frac{d}{dt} R_{T_i}(t)\right) \frac{1}{R_{T_i}(t)} = \frac{f_{T_i}(t)}{R_{T_i}(t)}.
\end{split}
\end{equation}

The lifetime of the $j$\textsuperscript{th} component is assumed to follow a parametric distribution indexed
by a parameter vector $\v{\theta_j}$. The parameter vector of the overall system is defined as
$$
    \v\theta = (\v{\theta_1},\ldots,\v{\theta_m}).
$$

When a random variable $X$ is parameterized by a particular $\v\theta$, we denote the
reliability function by $R_X(t;\v\theta)$, and the same for the other distribution functions.
As a special case, for the components in a series system, we subscript by their labels, e.g,
the pdf of the $j$\textsuperscript{th} component is denoted by $f_j(t;\v{\theta_j})$. Two continuous
random variables $X$ and $Y$ have a joint pdf $f_{X,Y}(x,y)$.
Given the joint pdf $f(x,y)$, the marginal pdf of $X$ is given by
$$
f_X(x) = \int_{\mathcal{Y}} f_{X,Y}(x,y) dy,
$$
where $\mathcal{Y}$ is the support of $Y$. (If $Y$ is discrete, replace
the integration with a summation over $\mathcal{Y}$.)

The conditional pdf of $Y$ given $X=x$, $f_{Y|X}(y|x)$, is defined as
$$
f_{X|Y}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
$$
We may generalize all of the above to more than two random variables, e.g.,
the joint pdf of $X_1,\ldots,X_m$ is denoted by $f(x_1,\ldots,x_m)$.

Next, we dive deeper into these concepts and provide mathematical derivations for
the reliability function, pdf, and hazard function of the series system.
We begin with the reliability function of the series system, as given by the following theorem.

::: {.theorem #sys-reliability-function}
The series system has a reliability function given by
\begin{equation}
\label{eq:sys_reliability_function}
R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
\end{equation}
:::

::: {.proof}
The reliability function is defined as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_i > t\}
$$
which may be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{\min\{T_{i 1},\ldots,T_{i m}\} > t\}.
$$
For the minimum to be larger than $t$, every component must be larger than $t$,
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t,\ldots,T_{i m} > t\}.
$$
Since the component lifetimes are independent, by the product rule the above may
be rewritten as
$$
  R_{T_i}(t;\v\theta) = \Pr\{T_{i 1} > t\} \times \cdots \times \Pr\{T_{i m} > t\}.
$$
By definition, $R_j(t;\v\theta) = \Pr\{T_{i j} > t\}$.
Performing this substitution obtains the result
$$
  R_{T_i}(t;\v\theta) = \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
:::

Theorem \@ref(thm:sys-reliability-function) shows that the system's overall reliability is the
product of the reliabilities of its individual components. This is an important relationship
in all series systems and will be used in the subsequent derivations. Next, we turn our
attention to the pdf of the system lifetime, described in the following theorem.

::: {.theorem #sys-pdf}
The series system has a pdf given by
\begin{equation}
\label{eq:sys_pdf}
f_{T_i}(t;\v\theta) = \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k\neq j}}^m R_k(t;\v{\theta_j}).
\end{equation}
:::

::: {.proof}
By definition, the pdf may be written as
$$
    f_{T_i}(t;\v\theta) = -\frac{d}{dt} \prod_{j=1}^m R_j(t;\v{\theta_j}).
$$
By the product rule, this may be rewritten as
\begin{align*}
  f_{T_i}(t;\v\theta)
    &= -\frac{d}{dt} R_1(t;\v{\theta_1})\prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j})\\
    &= f_1(t;\v\theta) \prod_{j=2}^m R_j(t;\v{\theta_j}) -
      R_1(t;\v{\theta_1}) \frac{d}{dt} \prod_{j=2}^m R_j(t;\v{\theta_j}).
\end{align*}
Recursively applying the product rule $m-1$ times results in
$$
f_{T_i}(t;\v\theta) = \sum_{j=1}^{m-1} f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}) -
    \prod_{j=1}^{m-1} R_j(t;\v{\theta_j}) \frac{d}{dt} R_m(t;\v{\theta_m}),
$$
which simplifies to
$$
f_{T_i}(t;\v\theta)= \sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k}).
$$
:::

Theorem \@ref(thm:sys-pdf) shows the pdf of the system lifetime is a function of
the pdfs and reliabilities of its components. We continue with the hazard
function of the system lifetime, defined in the next theorem.

::: {.theorem #sys-failure-rate}
The series system has a hazard function given by
\begin{equation}
\label{eq:sys-failure-rate}
  h_{T_i}(t;\v\theta) = \sum_{j=1}^m h_j(t;\v{\theta_j}).
\end{equation}
:::

::: {.proof}
By Equation \eqref{eq:failure_rate}, the $i$\textsuperscript{th} series system lifetime has a hazard function defined as
$$
  h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)}.
$$
Plugging in expressions for these functions results in
$$
  h_{T_i}(t;\v\theta) = \frac{\sum_{j=1}^m f_j(t;\v{\theta_j})
    \prod_{\substack{k=1\\k \neq j}}^m R_k(t;\v{\theta_k})}
      {\prod_{j=1}^m R_j(t;\v{\theta_j})},
$$
which can be simplified to
$$
h_{T_i}(t;\v\theta) = \sum_{j=1}^m \frac{f_j(t;\v{\theta_j})}{R_j(t;\v{\theta_j})} = \sum_{j=1}^m h_j(t;\v{\theta_j}).
$$
:::

Theorem \@ref(thm:sys-failure-rate) reveals that the system's hazard function is the sum
of the hazard functions of its components. By definition, the hazard function is the ratio of
the pdf to the reliability function,
$$
h_{T_i}(t;\v\theta) = \frac{f_{T_i}(t;\v\theta)}{R_{T_i}(t;\v\theta)},
$$
and we can rearrange this to get
\begin{equation}
\label{eq:sys_pdf_2}
\begin{split}
f_{T_i}(t;\v\theta) &= h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta)\\
              &= \biggl\{\sum_{j=1}^m h_j(t;\v{\theta_j})\biggr\}
                 \biggl\{ \prod_{j=1}^m R_j(t;\v{\theta_j}) \biggr\},
\end{split}
\end{equation}
which we find to be a more convenient form than Equation \eqref{eq:sys_pdf}.

In this section, we derived the mathematical forms for the system's reliability,
probability density, and hazard functions. Next, we build upon these concepts to
derive distributions related to the component cause of failure.

## Component Cause of Failure {#comp-cause}
Whenever a series system fails, precisely one of the components is the cause.
We denote the component cause of failure of a series system by the random
variable $K_i$, whose support is given by $\{1,\ldots,m\}$.
For example, $K_i=j$ indicates that the component indexed by $j$ failed first, i.e.,
$$
    T_{i j} < T_{i j'}
$$
for every $j'$ in the support of $K_i$ except for $j$.
Since we have series systems, $K_i$ is unique.

The system lifetime and the component cause of failure has a joint distribution
given by the following theorem.

::: {.theorem #f-k-and-t}
The joint pdf of the component cause of failure $K_i$ and series system lifetime
$T_i$ is given by
\begin{equation}
\label{eq:f_k_and_t}
  f_{K_i,T_i}(j,t;\v\theta) = h_j(t;\v{\theta_j}) \prod_{l=1}^m R_l(t;\v\theta),
\end{equation}
where $h_l(t;\v{\theta_j})$ and $R_l(t;\v{\theta_l})$ are respectively the hazard
and reliability functions of the $l$\textsuperscript{th} component.
:::

::: {.proof}
Consider a series system with $3$ components.
By the assumption that component lifetimes are mutually independent,
the joint pdf of $T_{i 1},T_{i 2},T_{i 3}$ is given by
$$
    f(t_1,t_2,t_3;\v\theta) = \prod_{j=1}^{3} f_j(t_j;\v{\theta_j}),
$$
where $f_j(t_j;\v{\theta_j})$ is the pdf of the $j$\textsuperscript{th} component.
The first component is the cause of failure at time $t$ if $K_i = 1$ and
$T_i = t$, which may be rephrased as the likelihood that $T_{i 1} = t$,
$T_{i 2} > t$, and $T_{i 3} > t$. Thus,
\begin{align*}
f_{K_i,T_i}(j,t;\v\theta) 
    &= \int_t^{\infty} \int_t^{\infty}
        f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2}) f_3(t_3;\v{\theta_3})
        dt_3 dt_2\\
     &= \int_t^{\infty} f_1(t;\v{\theta_1}) f_2(t_2;\v{\theta_2})
        R_3(t;\v{\theta_3}) dt_2\\
     &= f_1(t;\v{\theta_1}) R_2(t;\v{\theta_2}) R_3(t_1;\v{\theta_3}).
\end{align*}
By definition, $f_1(t;\v{\theta_1}) = h_1(t;\v{\theta_1}) R_1(t;\v{\theta_1})$,
and when we make this substitution into the above expression for $f_{K_i,T_i}(j,t;\v\theta)$,
we obtain the result
$$
f_{K_i,T_i}(j,t;\v\theta) = h_1(t;\v{\theta_1}) \prod_{l=1}^m R_l(t;\v{\theta_l}).
$$
Generalizing this result completes the proof.
:::

Theorem \@ref(thm:f-k-and-t) shows that the joint pdf of the component cause of
failure and system lifetime is a function of the hazard functions and reliability
functions of the components. This result will be used in the Section \@ref(like-model)
to derive the likelihood function for the masked data.

The probability that the $j$\textsuperscript{th} component is the cause of failure
is given by the following theorem.

::: {.theorem #prob-k}
The probability that the $j$\textsuperscript{th} component is the cause of failure
is given by
\begin{equation}
\label{eq:prob_k}
\Pr\{K_i = j\} = E_{\v\theta}
\biggl[
    \frac{h_j(T_i;\v{\theta_j})}
         {\sum_{l=1}^m h_l(T_i ; \v{\theta_l})}
\biggr]
\end{equation}
where $K_i$ is the random variable denoting the component cause of failure of the
$i$\textsuperscript{th} system and $T_i$ is the random variable denoting the
lifetime of the $i$\textsuperscript{th} system.
:::

::: {.proof}
The probability the $j$\textsuperscript{th} component is the cause of failure is given by
marginalizing the joint pdf of $K_i$ and $T_i$ over $T_i$,
$$
\Pr\{K_i = j\} = \int_0^{\infty} f_{K_i,T_i}(j,t;\v\theta) dt.
$$
By Theorem \@ref(thm:f-k-and-t), this is equivalent to
\begin{align*}
\Pr\{K_i = j\}
    &= \int_0^{\infty} h_j(t;\v{\theta_j}) R_{T_i}(t;\v\theta) dt\\
    &= \int_0^{\infty} \biggl(\frac{h_j(t;\v{\theta_j})}{h_{T_i}(t ; \v\theta)}\biggr) f_{T_i}(t ; \v\theta) dt\\
    &= E_{\v\theta}\biggl[\frac{h_j(T_i;\v{\theta_j})}{\sum_{l=1}^m h_l(T_i ; \v{\theta_l})}\biggr].
\end{align*}
:::

::: {.theorem #prob-k-given-t}
The probability that the $j$\textsuperscript{th}
component is the cause of system failure given that we know the system failure
occured at time $t_i$ is given by
$$
\Pr\{K_i = j|T_i=t_i\} = \frac{h_j(t_i;\v{\theta_j})}{\sum_{l=1}^m h_l(t_i;\v{\theta_l})}.
$$
:::

::: {.proof}
By the definition of conditional probability,
\begin{align*}
    \Pr\{K_i = j|T_i = t_i\} 
        &= \frac{f_{K_i,T_i}(j, t_i;\v\theta)}{f_{T_i}(t_i;\v\theta)}\\
        &= \frac{h_j(t_i;\v{\theta_j}) R_{T_i}(t_i;\v\theta)}{f_{T_i}(t_i;\v\theta)}.
\end{align*}
Since $f_{T_i}(t_i;\v\theta) = h_{T_i}(t_i;\v\theta) R_{T_i}(t_i;\v\theta)$,
we make this substitution and simplify to obtain
$$
\Pr\{K_i = j|T_i = t_i\} = \frac{h_j(t_i;\v{\theta_j})}{\sum_{l=1}^m h_l(t_i;\v{\theta_l})}.
$$
:::

Theorems \@ref(thm:prob-k) and \@ref(thm:prob-k-given-t) are closely related.
Theorem \@ref(thm:prob-k-given-t) is a special case of Theorem \@ref(thm:prob-k)
when $T_i$ is known to be $t_i$.

## System and Component Reliabilities {#reliability}

A common measure of reliability is mean time to failure (MTTF). The
MTTF is defined as the expectation of the lifetime,
\begin{equation}
\label{mttf-sys}
\text{MTTF} = E_{\v\theta}\{T_i\},
\end{equation}
which if certain assumptions are satisfied[^1] is equivalent to the integration
of the reliability function over its support.

[^1]: $T_i$ is non-negative and continuous, $R_{T_i}(t;\v\theta)$ is a well-defined,
continuous, and differential function for $t > 0$, and $\int_0^\infty R_{T_i}(t;\v\theta) dt$
converges.

While the MTTF provides a summary measure of reliability, it is not a complete description.
Depending on the failure characteristics, MTTF can be misleading. For example,
a system that has a high likelihood of failing early in its life may still have a
large MTTF if it is fat-tailed.[^2]

[^2]: A "fat-tailed" distribution refers to a probability distribution with tails that
decay more slowly than those of the exponential family, such as the case with the Weibull
when its shape parameter is greater than $1$. This means that extreme values are more
likely to occur, and the distribution is more prone to "black swan" events or rare occurrences.
In the context of reliability, a fat-tailed distribution might imply a higher likelihood of
unusually long lifetimes, which can skew measures like the MTTF. [@taleb2007black]

The reliability of the components in the series system determines the reliability
of the system. We denote the MTTF of the $j$\textsuperscript{th} component by
$\text{MTTF}_j$ and, according to Theorem \@ref(thm:prob-k), the probability
that the $j$\textsuperscript{th} component is the cause of failure is given by
$\Pr\{K_i = j\}$. In a well-designed series system, there is no component that is
the "weakest link" that either has a much shorter MTTF or a much higher
probability of being the component cause of failure than any of
the other components, e.g., $\Pr\{K_i = j\} \approx \Pr\{K_i = k\}$ and
and MTTF$_j \approx$ MTTF$_k$ for all $j$ and $k$. This just means that the components
should have similar reliabilities and failure characteristics.

We use these results in the simulation study in Section \@ref(sim-study), where we
assess the sensitivity of the MLE with respect to varying the reliability
of one of the Weibull components. We vary its reliability in two different ways:

1. We vary its shape parameter (keeping its scale parameter constant), which determines
   the failure characteristics of the component and also affects its MTTF.

2. We vary its scale parameter (keeping its shape parameter constant), which scales
   its MTTF while retaining the same failure characteristics.

Likelihood Model for Masked Data {#like-model}
==================================================

We aim to estimate an unknown parameter, $\v\theta$, using *masked data*,
which can have two types of masking. We consider two types of masking:
censoring of system failures and masking component causes of failure.

##### Censoring {-}
We generally encounter two types of censoring: the system failure is observed
to occur within some time interval, or the system failure is not observed
but we know that it was functioning at least until some point in time. The latter
is known as *right-censoring*, which is the type of censoring we consider in
this paper.

##### Component Cause of Failure Masking {-}
In the case of masking the component cause of failure, we may not know the
precise component cause of failure, but we may have some indication. A common
example is when a diagnostician is able to isolate the cause of failure to a
subset of the components. We call this subset the *candidate set*.

##### Masked Data {-}
In this paper, each system is put into operation and observed until either it
fails or its failure is right-censored after some duration $\tau$, so we do not
directly observe the system lifetime but rather we observe the right-censored
lifetime, $S_i$, which is given by
\begin{equation}
    S_i = \min\{\tau, T_i\}.
\end{equation}
We also observe an event indicator, $\delta_i$, which is given by
\begin{equation}
    \delta_i = 1_{T_i < \tau},
\end{equation}
where $1_{\text{condition}}$ is an indicator function that denotes $1$ if
the condition is true and $0$ otherwise.
Here, $\delta_i = 1$ indicates the $i$\textsuperscript{th} system's failure was
observed and $\delta_i = 0$ indicates it was right-censored.[^201]
If a system failure event is observed ($\delta_i = 1$), then we also observe a
candidate set that contains the component cause of failure. We denote the
candidate set for the $i$\textsuperscript{th} system by $\mathcal{C}_i$, which
is a subset of $\{1,\ldots,m\}$.

[^201]: In some likelihood models, there may be more than two possible values for
$\delta_i$, but in this paper, we only consider the case where $\delta_i$ is binary.
Future work could consider the case where $\delta_i$ is categorical by
including more types of censoring events and more types of component cause of
failure masking.

In summary, the observed data is assumed to be i.i.d. and is given by
$D = \{D_1, \ldots, D_n\}$, where each $D_i$ contains the following elements:
\begin{itemize}
\item{$S_i$} is the right-censored system lifetime of the $i$\textsuperscript{th} system.
\item{$\delta_i$} is the event indicator for the $i$\textsuperscript{th} system.
\item{$\mathcal{C}_i$} is the set of candidate component causes of failure for the
$i$\textsuperscript{th} system.
\end{itemize}
The masked data generation process is illustrated in Figure \ref{fig:figureone}.

\begin{figure}[h]
\centering{
\resizebox{0.5\textwidth}{!}{\input{./image/dep_model.tex}}}
\caption{This figure showcases a dependency graph of the generative model for
$D_i = (S_i,\delta_i,\mathcal{C}_i)$. The elements in green are observed in the sample,
while the elements in red are unobserved (latent). We see that $\mathcal{C}_i$ is
related to both the unobserved component lifetimes $T_{i 1},\ldots,T_{i m}$ and
other unknown and unobserved covariates, like ambient temperature or the
particular diagnostician who generated the candidate set. These two complications
for $\mathcal{C}_i$ are why we seek a way to construct a reduced likelihood function
in later sections that is not a function of the distribution of $\mathcal{C}_i$.}
\label{fig:figureone}
\end{figure}

An example of masked data $D$ with a right-censoring time $\tau = 5$ can be seen in Table 1
for a series system with $3$ components.

System | Right-censored lifetime ($S_i$) | Event indicator ($\delta_i$) | Candidate set ($\mathcal{C}_i$) |
------ | ------------------------------- | --------------------------- | --------------------- |
   1   | $1.1$                           | 1                           | $\{1,2\}$             |
   2   | $1.3$                           | 1                           | $\{2\}$               |
   4   | $2.6$                           | 1                           | $\{2,3\}$             |
   5   | $3.7$                           | 1                           | $\{1,2,3\}$           |
   6   | $5$                             | 0                           | $\emptyset$           |
   3   | $5$                             | 0                           | $\emptyset$           |

: Right-censored lifetime data with masked component cause of failure.

In our model, we assume the data is governed by a pdf, which is determined by
a specific parameter, represented as $\v\theta$ within the parameter space $\v\Omega$.
The joint pdf of the data $D$ can be represented as follows:
$$
f(D ; \v\theta) = \prod_{i=1}^n f(D_i;\v\theta) = \prod_{i=1}^n f(s_i,\delta_i,c_i;\v\theta),
$$
where $s_i$ is the observed system lifetime, $\delta_i$ is the observed event
indicator, and $c_i$ is the observed candidate set of the $i$\textsuperscript{th} system.

This joint pdf tells us how likely we are to observe the particular data, $D$, given
the parameter $\v\theta$. When we keep the data constant and allow the parameter
$\v\theta$ to vary, we obtain what is called the likelihood function $L$, defined as
$$
L(\v\theta) = \prod_{i=1}^n L_i(\v\theta)
$$
where
$$
L_i(\v\theta) = f(s_i,\delta_i,c_i;\v\theta)
$$
is the likelihood contribution of the $i$\textsuperscript{th} system.

For each type of data, right-censored data and masked component cause of
failure data, we will derive the *likelihood contribution* $L_i$, which refers to the
part of the likelihood function that this particular piece of data contributes to.
We present the following theorem for the likelihood contribution model.
\begin{theorem}
\label{thm:likelihood_contribution}
The likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:like}
L_i(\v\theta) = \prod_{j=1}^m R_j(s_i;\v{\theta_j}) \biggl(\beta_i \sum_{j \in c_i} h_j(s_i;\v{\theta_j}) \biggr)^{\delta_i}
\end{equation}
where $\delta_i = 0$ indicates the $i$\textsuperscript{th} system is
right-censored at time $s_i$ and $\delta_i = 1$ indicates the $i$\textsuperscript{th} system
is observed to have failed at time $s_i$ with a component cause of failure potentially masked
by the candidate set $c_i$.
\end{theorem}

In the following subsections, we prove this result for each type of masked data,
right-censored system lifetime data $(\delta_i = 0)$ and masking of the
component cause of failure $(\delta_i = 1)$.

## Masked Component Cause of Failure {#candmod}
Suppose a diagnostician is unable to identify the precise component cause of the
failure, e.g., due to cost considerations he or she replaced multiple components
at once, successfully repairing the system but failing to precisely identity
the failed component.
In this case, the cause of failure is said to be *masked*.

The unobserved component lifetimes may have many covariates, like ambient
operating temperature, but the only covariate we observe in our masked data
model are the system's lifetime and additional masked data in the form of
a candidate set that is somehow correlated with the unobserved component
lifetimes.

The key goal of our analysis is to estimate the parameter $\v{\theta}$, which 
maximize the likelihood of the observed data, and to estimate the precision and
accuracy of this estimate using the Bootstrap method.

To achieve this, we first need to assess the joint distribution of the system's
continuous lifetime, $T_i$, and the discrete candidate set, $\mathcal{C}_i$, which
can be written as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = f_{T_i}(t_i;\v{\theta})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\},
$$
where $f_{T_i}(t_i;\v{\theta})$ is the pdf of $T_i$ and
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$ is the conditional
pmf of $\mathcal{C}_i$ given $T_i = t_i$.

We assume the pdf $f_{T_i}(t_i;\v{\theta})$ is known, but we do not have knowledge
of $\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i\}$, i.e., the data generating
process for candidate sets is unknown.

However, it is critical that the masked data, $\mathcal{C}_i$, is correlated with the
$i$\textsuperscript{th} system. This way, the conditional distribution of $\mathcal{C}_i$
given $T_i = t_i$ may provide information about $\v{\theta}$, despite our Statistical
interest being primarily in the series system rather than the candidate sets.

To make this problem tractable, we assume a set of conditions that make it
unnecessary to estimate the generative processes for candidate sets.
The most important way in which $\mathcal{C}_i$ is correlated with the
$i$\textsuperscript{th} system is given by assuming the following condition.
\begin{condition}
\label{cond:c_contains_k}
The candidate set $\mathcal{C}_i$ contains the index of the the failed component, i.e.,
$$
\Pr{}_{\!\v\theta}\{K_i \in \mathcal{C}_i\} = 1
$$
where $K_i$ is the random variable for the failed component index of the
$i$\textsuperscript{th} system.
\end{condition}
Assuming Condition \ref{cond:c_contains_k}, $\mathcal{C}_i$ must contain the
index of the failed component, but we can say little else about what other
component indices may appear in $\mathcal{C}_i$.

In order to derive the joint distribution of $\mathcal{C}_i$ and $T_i$ assuming
Condition \ref{cond:c_contains_k}, we take the following approach.
We notice that $\mathcal{C}_i$ and $K_i$ are statistically dependent.
We denote the conditional pmf of $\mathcal{C}_i$ given $T_i = t_i$ and
$K_i = j$ as
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\}.
$$

Even though $K_i$ is not observable in our masked data model, we can still
consider the joint distribution of $T_i$, $K_i$, and $\mathcal{C}_i$.
By Theorem \@ref(thm:f-k-and-t), the joint pdf of $T_i$ and $K_i$ is given by
$$
f_{T_i,K_i}(t_i,j;\v{\theta}) = h_j(t_i;\v{\theta_j}) \prod_{l=1}^m R_l(t_i;\v{\theta_l}),
$$
where $h_j(t_i;\v{\theta_j})$ and $R_j(s_i;\v{\theta_j})$ are respectively the hazard
and reliability functions of the $j$\textsuperscript{th} component.
Thus, the joint pdf of $T_i$, $K_i$, and $\mathcal{C}_i$ may be written as
\begin{equation}
\label{eq:joint_pdf_t_k_c}
\begin{split}
f_{T_i,K_i,\mathcal{C}_i}(t_i,j,c_i;\v{\theta})
    &= f_{T_i,K_i}(t_i,k;\v{\theta}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}\\
    &= h_j(t_i;\v{\theta_j}) \prod_{l=1}^m R_l(t_i;\v{\theta_l})
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}.
\end{split}
\end{equation}
We are going to need the joint pdf of $T_i$ and $\mathcal{C}_i$, which
may be obtained by summing over the support $\{1,\ldots,m\}$ of $K_i$ in
Equation \eqref{eq:joint_pdf_t_k_c},
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = \prod_{l=1}^m R_l(t_i;\v{\theta_l})
    \sum_{j=1}^m \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
$$
By Condition \ref{cond:c_contains_k},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\} = 0$ when $K_i = j$ and
$j \notin c_i$, and so we may rewrite the joint pdf of $T_i$ and
$\mathcal{C}_i$ as
\begin{equation}
\label{eq:part1}
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) = \prod_{l=1}^m R_l(t_i;\v{\theta_l})
    \sum_{j \in c_i} \biggl\{
        h_j(t_i;\v{\theta_j}) \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
    \biggr\}.
\end{equation}

When we try to find an MLE of $\v\theta$ (see Section \@ref(mle)), we
solve the simultaneous equations of the MLE and choose a solution
$\hat{\v\theta}$ that is a maximum for the likelihood function.
When we do this, we find that $\hat{\v\theta}$ depends on the unknown
conditional pmf $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.
So, we are motivated to seek out more conditions (that approximately hold in
realistic situations) whose MLEs are independent of the pmf
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$.

\begin{condition}
\label{cond:equal_prob_failure_cause}
Any of the components in the candidate set has an equal probability of being the
cause of failure.
That is, for a fixed $j \in c_i$,
$$
\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}
$$
for all $j' \in c_i$.
\end{condition}
In many industrial problems, masking generally
occurred due to time constraints and the expense of failure analysis [@Fran-1991].
In this setting, Condition \ref{cond:equal_prob_failure_cause} generally holds.

Assuming Conditions \ref{cond:c_contains_k} and
\ref{cond:equal_prob_failure_cause},
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j\}$ may be factored out of the
summation in Equation \eqref{eq:part1}, and thus the joint pdf of $T_i$ and
$\mathcal{C}_i$ may be rewritten as
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\} \prod_{l=1}^m R_l(t_i;\v{\theta_l})
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j})
$$
where $j' \in c_i$.

If $\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ is a function of
$\v{\theta}$, the MLEs are still dependent on the unknown
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$.
This is a more tractable problem, but we are primarily interested in the
situation where we do not need to know (nor estimate)
$\Pr{}_{\!\v\theta}\{\mathcal{C}_i=c_i|T_i=t_i,K_i=j'\}$ to find an MLE of
$\v{\theta}$. The last condition we assume achieves this result.
\begin{condition}
\label{cond:masked_indept_theta}
The masking probabilities conditioned on failure time $T_i$ and component cause
of failure $K_i$ are not functions of $\v{\theta}$. In this case, the conditional
probability of $\mathcal{C}_i$ given $T_i=t_i$ and $K_i=j'$ is denoted by
$$
\beta_i = \Pr\{\mathcal{C}_i=c_i | T_i=t_i, K_i=j'\}
$$
where $\beta_i$ is not a function of $\v\theta$.
\end{condition}

When Conditions \ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause},
and \ref{cond:masked_indept_theta} are satisfied, the joint pdf of $T_i$ and
$\mathcal{C}_i$ is given by
$$
f_{T_i,\mathcal{C}_i}(t_i,c_i;\v{\theta}) =
    \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l})
    \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
$$
When we fix the sample and allow $\v{\theta}$ to vary, we obtain the
contribution to the likelihood $L$ from the $i$\textsuperscript{th} observation
when the system lifetime is exactly known (i.e., $\delta_i = 1$) but the
component cause of failure is masked by a candidate set $c_i$:
\begin{equation}
\label{eq:likelihood_contribution_masked}
L_i(\v\theta) = \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l}) \sum_{j \in c_i} h_j(t_i;\v{\theta_j}).
\end{equation}

To summarize this result, assuming Conditions \ref{cond:c_contains_k},
\ref{cond:equal_prob_failure_cause}, and \ref{cond:masked_indept_theta}, 
if we observe an exact system failure time for the $i$-th system ($\delta_i = 1$),
but the component that failed is masked by a candidate set $c_i$, then its likelihood
contribution is given by Equation \eqref{eq:likelihood_contribution_masked}.

## Right-Censored Data

As described in Section \@ref(like-model), we observe realizations of
$(S_i,\delta_i,\mathcal{C}_i)$ where $S_i = \min\{T_i,\tau\}$ is the
right-censored system lifetime, $\delta_i = 1_{T_i < \tau}$ is
the event indicator, and $\mathcal{C}_i$ is the candidate set.

In the previous section, we discussed the likelihood contribution from an
observation of a masked component cause of failure, i.e., $\delta_i = 1$. 
We now derive the likelihood contribution of a *right-censored* observation,
$\delta_i = 0$, in our masked data model.
\begin{theorem}
\label{thm:joint_s_d_c}
The likelihood contribution of a right-censored observation $(\delta_i = 0)$
is given by
\begin{equation}
L_i(\v\theta) = \prod_{l=1}^m R_l(s_i;\v{\theta_l}).
\end{equation}
\end{theorem}
\begin{proof}
When right-censoring occurs, then $S_i = \tau$, and we only know that
$T_i > \tau$, and so we integrate over all possible values that it may have
obtained,
$$
L_i(\v\theta) = \Pr\!{}_{\v\theta}\{T_i > s_i\}.
$$
By definition, this is just the survival or reliability function of the series system
evaluated at $s_i$,
$$
L_i(\v\theta) = R_{T_i}(s_i;\v\theta) = \prod_{l=1}^m R_l(t_i;\v{\theta_l}).
$$
\end{proof}

When we combine the two likelihood contributions, we obtain the likelihood
contribution for the $i$\textsuperscript{th} system shown in Theorem
\ref{thm:likelihood_contribution},
$$
L_i(\v\theta) =
\begin{cases}
    \prod_{l=1}^m R_l(s_i;\v{\theta_l})         &\text{ if } \delta_i = 0\\
    \beta_i \prod_{l=1}^m R_l(t_i;\v{\theta_l})
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
We use this result in Section \@ref(mle) to derive the maximum likelihood
estimator (MLE) of $\v\theta$.

## Identifiability and Convergence Issues

In our likelihood model, masking and right-censoring can lead to issues related
to identifiability and flat likelihood regions.
Identifiability refers to the unique mapping of the model parameters to the
likelihood function, and lack of identifiability can lead
to multiple sets of parameters that explain the data equally well, making inference
about the true parameters challenging [@lehmann1998theory], while 
flat likelihood regions can complicate convergence [@wu1983convergence].

In our simulation study, we address these challenges in a pragmatic way. Specifically,
failure to converge to a solution within a maximum of 125 iterations is interpreted as
evidence of the aforementioned issues, leading to the discarding of the sample, with
the process then repeated with a new synthetic sample. Note, however, that in Section
\@ref(boot) where we discuss the bias-corrected and accelerated (BCa) bootstrap
method for constructing confidence intervals, we do not discard any resamples.

This strategy helps ensure the robustness of the results, while acknowledging the
inherent complexities of likelihood-based estimation in models characterized by
masking and right-censoring.

Maximum Likelihood Estimation {#mle}
========================================
In our analysis, we use maximum likelihood estimation (MLE) to estimate the series
system parameter $\v\theta$ from the masked data [@bain1992; @casella2002statistical].
The MLE finds parameter values that maximize the likelihood of the observed data
under the assumed model. A maximum likelihood estimate, $\hat{\v\theta}$, is a
solution of
\begin{equation}
\label{eq:mle}
L(\hat{\v\theta}) = \max_{\v\theta \in \v\Omega} L(\v\theta),
\end{equation}
where $L(\v\theta)$ is the likelihood function of the observed data. For computational
efficiency and analytical simplicity, we work with the log-likelihood function,
denoted as $\ell(\v\theta)$, instead of the likelihood function [@casella2002statistical].
\begin{theorem}
\label{thm:loglike_total}
The log-likelihood function, $\ell(\v\theta)$, for our masked data model is the sum of the log-likelihoods for each observation,
\begin{equation}
\label{eq:loglike}
\ell(\v\theta) = \sum_{i=1}^n \ell_i(\v\theta),
\end{equation}
where $\ell_i(\v\theta)$ is the log-likelihood contribution for the $i$\textsuperscript{th} observation:
\begin{equation}
\ell_i(\v\theta) = \sum_{j=1}^m \log R_j(s_i;\v{\theta_j}) +
    \delta_i \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \bigr).
\end{equation}
\end{theorem}
\begin{proof}
The log-likelihood function is the logarithm of the likelihood function,
$$
\ell(\v\theta) = \log L(\v\theta) = \log \prod_{i=1}^n L_i(\v\theta) = \sum_{i=1}^n \log L_i(\v\theta).
$$

Substituting $L_i(\v\theta)$ from Equation \eqref{eq:like}, we consider these two cases of $\delta_i$
seperately to obtain the result in Theorem \ref{thm:loglike_total}.

\textbf{Case 1}: If the $i$-th system is right-censored ($\delta_i = 0$),
$$
\ell_i(\v\theta) = \log \prod_{l=1}^m R_l(s_i;\v{\theta_l}) = \sum_{l=1}^m \log R_l(s_i;\v{\theta_l}).
$$

\textbf{Case 2}: If the $i$-th system's component cause of failure is masked but the failure time is known ($\delta_i = 1$),
$$
\ell_i(\v\theta) = \sum_{l=1}^m \log R_l(t_i;\v{\theta_l}) + \log \beta_i + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j})\bigr).
$$
By Condition \ref{cond:masked_indept_theta},
we may discard the $\log \beta_i$ term since it does not depend on $\v\theta$, giving us the result
$$
\ell_i(\v\theta) = \sum_{l=1}^m \log R_l(s_i;\v{\theta_l}) + \log \bigl(\sum_{j\in c_i} h_j(s_i;\v{\theta_j}) \biggr).
$$
Combining these two cases gives us the result in Theorem \ref{thm:loglike_total}.
\end{proof}

The MLE, $\hat{\v\theta}$, is often found by solving a system of equations derived from setting the derivative of the log-likelihood function to zero, i.e.,
\begin{equation}
\label{eq:mle_eq}
\frac{\partial}{\partial \theta_j} \ell(\v\theta) = 0,
\end{equation}
for each component $\theta_j$ of the parameter $\v\theta$ [@bain1992]. When there's no closed-form solution,
we resort to numerical methods like the Newton-Raphson method.

Assuming some regularity conditions, such as the likelihood function being identifiable, the MLE has many desirable
asymptotic properties that underpin statistical inference, namely that it is an asymptotically unbiased estimator
of the parameter $\v\theta$ and it is normally distributed with a variance given by the inverse of the Fisher
Information Matrix (FIM) [@casella2002statistical].
However, for smaller samples, these asymptotic properties may not yield accurate approximations. We propose to use
the bootstrap method to offer an empirical approach for estimating the sampling distribution of the MLE, in particular for
computing confidence intervals.

Bias-Corrected and Accelerated Bootstrap Confidence Intervals {#boot}
===============================================================
We utilize the non-parametric bootstrap to approximate the sampling distribution of
the MLE. In the non-parametric bootstrap, we resample from the observed data
with replacement to generate a bootstrap sample. The MLE is then computed for
the bootstrap sample. This process is repeated $B$ times, giving us $B$ bootstrap
replicates of the MLE. The sampling distribution of the MLE is then approximated
by the empirical distribution of the bootstrap replicates of the MLE.

The method we use to generate confidence intervals is known
as Bias-Corrected and Accelerated Bootstrap Confidence Intervals (BCa), which
applies two corrections to the standard bootstrap method:

- Bias correction: This adjusts for bias in the bootstrap distribution itself.
  This bias is measured as the difference between the mean of the bootstrap distribution and the observed statistic.
  It works by transforming the percentiles of the bootstrap distribution to correct for these issues.
  
  This may be a useful transformation in our case since we are dealing with small samples and we have two potential
  sources of bias: right-censoring and masking component cause of failure. They seem to have opposing effects
  on the MLE, but the relationship is difficult to quantify.

- Acceleration: This adjusts for the rate of change of the statistic as a function of the true, unknown parameter.
  This correction is important when the shape of the statistic's distribution changes with the true parameter.

  Since we have a number of different shape parameters, $k_1,\ldots,k_m$, we may expect the shape of the
  distribution of the MLE to change as a function of the true parameter, making this correction potentially useful.

Since we are primarly interested in generating confidence intervals for small samples for a
potentially biased MLE, the BCa method may be a good choice for our analysis. For more details
on BCa, see [@efron1987better].

In our simulation study, we will assess the performance of the bootstrapped BCa
confidence intervals by computing the coverage probability of the confidence
intervals. A well-calibrated 95% confidence interval contains the true
value around 95% of the time. If the confidence interval is too narrow, it will have
a coverage probability less than 95%, which conveys a sort of false confidence
in the precision of the MLE. If the confidence interval is too wide, it will
have a coverage probability greater than 95%, which conveys a lack of confidence
in the precision of the MLE. We want confidence intervals to be as
narrow as possible while still having a coverage probability close to the
nominal level, 95%.

While the bootstrap method provides a robust and flexible tool for statistical
estimation, its effectiveness can be influenced by several factors
[@efron1994introduction]. Firstly, instances of non-convergence in our bootstrap
samples were observed.
Such cases can occur when the estimation method, like the MLE used in our
analysis, fails to converge due to the specifics of the resampled data
[@casella2002statistical]. This issue can potentially introduce bias or
reduce the effective sample size of our bootstrap distribution.

Secondly, the bootstrap's accuracy can be compromised with small sample sizes,
as the method relies on the law of large numbers to approximate the true sampling
distribution. For small datasets, the bootstrap samples might not adequately
represent the true variability in the data, leading to inaccurate results
[@efron1994introduction].

Thirdly, our data involves right censoring and a masking of the component cause
of failure when a system failure is observed. These aspects can cause certain data points or
trends to be underrepresented or not represented at all in our data, introducing
bias in the bootstrap distribution [@klein2005survival].

Despite these challenges, we found the bootstrap method useful in approximating
the sampling distribution of the MLE, taking care in interpreting the results,
particularly as it relates to coverage probabilities.

Series System with Weibull Components {#weibull}
================================================
The Weibull distribution, introduced by Waloddi Weibull in 1937, has been
instrumental in reliability analysis due to its ability to model a wide range
of failure behaviors. Reflecting on its utility, Weibull
modestly noted that it "[...] may sometimes render good service." [@Abernethy2006].
In the context of our study, we utilize the Weibull
to model a system as originating from Weibull components in a series configuration,
producing a specific form of the likelihood model described in Section \@ref(like-model),
which deals with challenges such as right censoring and masked component cause of failure.

The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{Weibull}(k_j,\lambda_j) \qquad \text{for } i = 1,\ldots,n \text{ and } j = 1,\ldots,m,
$$
where $\lambda_j > 0$ is the scale parameter and $k_j > 0$ is the shape parameter.
The $j$\textsuperscript{th} component has a reliability function, pdf, and hazard function
given respectively by
\begin{align}
    R_j(t;\lambda_j,k_j)
        &= \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
    f_j(t;\lambda_j,k_j)
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
        \exp\biggl\{-\left(\frac{t}{\lambda_j}\right)^{k_j} \biggr\},\\
    h_j(t;\lambda_j,k_j) \label{eq:weibull_haz}
        &= \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}.
\end{align}

The shape parameter of the Weibull distribtion is of particular importance:

- $k_j < 1$ indicates infant mortality. An example of how this might arise is
a result of defective components being weeded out early, and the remaining
components surviving for a much longer time.
- $k_j = 1$ indicates random failures (independent of age). An example of how
this might arise is a result of random shocks to the system, but otherwise
the system is age-independent.[^4]
- $k_j > 1$ indicates wear-out failures. An example of how this might arise is a
result of components wearing as they age

[^4]: The exponential distribution is a special case of the Weibull distribution when $k_j = 1$.

We show that the lifetime of the series system composed of $m$ Weibull components
has a reliability, hazard, and probability density functions given by the following theorem.
::: {.theorem #sys_weibull}
The lifetime of a series system composed of $m$ Weibull components
has a reliability function, hazard function, and pdf respectively given by
\begin{align}
\label{eq:sys_weibull_reliability_function}
R_{T_i}(t;\v\theta) &= \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\},\\
\label{eq:sys_weibull_failure_rate_function}
h_{T_i}(t;\v\theta) &= \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1},\\
\label{eq:sys_weibull_pdf}
f_{T_i}(t;\v\theta) &= \biggl\{
    \sum_{j=1}^m \frac{k_j}{\lambda_j}\left(\frac{t}{\lambda_j}\right)^{k_j-1}
\biggr\}
\exp
\biggl\{
    -\sum_{j=1}^m \bigl(\frac{t}{\lambda_j}\bigr)^{k_j}
\biggr\}.
\end{align}
:::

::: {.proof}
The proof for the reliability function follows from Theorem \@ref(thm:sys-reliability-function),
$$
R_{T_i}(t;\v\theta) = \prod_{j=1}^{m} R_j(t;\lambda_j,k_j).
$$
Plugging in the Weibull component reliability functions obtains the result
\begin{align*}
R_{T_i}(t;\v\theta)
    = \prod_{j=1}^{m} \exp\biggl\{-\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}
    = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j}\biggr\}.
\end{align*}
The proof for the hazard function follows from Theorem \@ref(thm:sys-failure-rate),
\begin{align*}
h_{T_i}(t;\v\theta)
    = \sum_{j=1}^{m} h_j(t;\v{\theta_j})
    = \sum_{j=1}^{m} \frac{k_j}{\lambda_j}\biggl(\frac{t}{\lambda_j}\biggr)^{k_j-1}
\end{align*}
The proof for the pdf follows from Theorem \@ref(thm:sys-pdf). By definition,
$$
f_{T_i}(t;\v\theta) = h_{T_i}(t;\v\theta) R_{T_i}(t;\v\theta).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:sys_weibull_reliability_function} and
\eqref{eq:sys_weibull_failure_rate_function} completes the proof.
:::

## Reliability
In Section \@ref(reliability), we discussed the concept of reliability.
In the case of Weibull components, the MTTF of the $j$\textsuperscript{th}
component is given by
\begin{equation}
\label{eq:mttf-weibull}
\text{MTTF}_j = \lambda_j \Gamma\biggl(1 + \frac{1}{k_j}\biggr),
\end{equation}
where $\Gamma$ is the gamma function.

We mentioned that the MTTF can sometimes be a poor measure of reliability, e.g.,
the MTTF and the probability of failing early can be large. The Weibull is a good
example of this phenomenon. If $k > 1$, the Weibull is a fat-tailed distribution,
and it can exhibit both a large MTTF and a high probability of failing early.

Components may have similar MTTFs, but some components may be more likely to fail
early and others may be more likely to fail late, depending upon their failure
characterstics (shape parameters), and so the probability of component failure given by
Equation \eqref{eq:prob_k} is a useful measure of component reliability compared to
the other components in the system.

In a well-designed series system, the component failure characteristics are similar:
they have a similar MTTF and a similar probability of being the component cause of
failure, i.e., they have similar shapes and scales, so that system failures are not
dominated by some subset of components.

## Likelihood Model {#sys-weibull-like}
In Section \@ref(like-model), we discussed two separate kinds of likelihood
contributions, masked component cause of failure data (with exact system failure
times) and right-censored data. The likelihood contribution of the
$i$\textsuperscript{th} system is given by the following theorem.
\begin{theorem}
Let $\delta_i$ be an indicator variable that is 1 if the
$i$\textsuperscript{th} system fails and 0 (right-censored) otherwise.
Then the likelihood contribution of the $i$\textsuperscript{th} system is given by
\begin{equation}
\label{eq:weibull_likelihood_contribution}
L_i(\v\theta) =
\begin{cases}
    \exp\biggl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\}
        \beta_i \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    & \text{if } \delta_i = 1,\\
    \exp\bigl\{-\sum_{j=1}^{m}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j}\biggr\} & \text{if } \delta_i = 0.
\end{cases}
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{thm:likelihood_contribution}, the likelihood contribution of the
$i$-th system is given by
$$
L_i(\v\theta) =
\begin{cases}
    R_{T_i}(s_i;\v\theta)                      &\text{ if } \delta_i = 0\\
    \beta_i R_{T_i}(s_i;\v\theta)
        \sum_{j\in c_i} h_j(s_i;\v{\theta_j})   &\text{ if } \delta_i = 1.
\end{cases}
$$
By Equation \eqref{eq:sys_weibull_reliability_function}, the system reliability
function $R_{T_i}$ is given by
$$
R_{T_i}(t_i;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j}\biggr\}.
$$
and by Equation \eqref{eq:weibull_haz}, the Weibull component hazard function $h_j$ is
given by
$$
h_j(t_i;\v{\theta_j}) = \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}.
$$
Plugging these into the likelihood contribution function obtains the result.
\end{proof}

Taking the log of the likelihood contribution function obtains the following result.
\begin{corollary}
The log-likelihood contribution of the $i$-th system is given by
\begin{equation}
\label{eq:weibull_log_likelihood_contribution}
\ell_i(\v\theta) =
-\sum_{j=1}^{m}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j} +
    \delta_i \log \!\Biggl(    
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\biggl(\frac{t_i}{\lambda_j}\biggr)^{k_j-1}
    \Biggr)
\end{equation}
where we drop any terms that do not depend on $\v\theta$ since they do not
affect the MLE.
\end{corollary}

See Appendix \@ref(app-weibull-loglik-r) for the R code that implements the log-likelihood function
for the series system with Weibull components.

We find an MLE by solving \eqref{eq:mle_eq},
i.e., a point $\v{\hat\theta} = (\hat{k}_1,\hat{\lambda}_1,\ldots,\hat{k}_m,\hat{\lambda}_m)$ satisfying
$\nabla_{\theta} \ell(\v{\hat\theta}) = \v{0}$, where $\nabla_{\v\theta}$
is the gradient of the log-likelihood function (score) with respect to $\v\theta$.

To solve this system of equations, we use the Newton-Raphson method, which requires
the score and the Hessian of the log-likelihood function.
We analytically derive the score since it is useful to have for the Newton-Raphson
method, but we do not do the same for the Hessian of the log-likelihood for the following reasons:

1. The gradient is easy to derive, and it is useful to have for
computing gradients efficiently and accurately, which will be useful for
numerically approximating the Hessian.

2. The Hessian is tedious and error prone to derive, and Newton-like methods
often do not require the Hessian to be explicitly computed.

The following theorem derives the score function.
\begin{theorem}
\label{thm:weibull_score}
The score function of the log-likelihood contribution of the $i$-th Weibull series
system is given by
\begin{equation}
\label{eq:weibull_score}
\nabla \ell_i(\v\theta) = \biggl(
    \frac{\partial \ell_i(\v\theta)}{\partial k_1},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_1},
    \cdots, 
    \frac{\partial \ell_i(\v\theta)}{\partial k_m},
    \frac{\partial \ell_i(\v\theta)}{\partial \lambda_m} \biggr)',
\end{equation}
where
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial k_r} = 
    -\biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r}    
        \!\!\log\biggl(\frac{t_i}{\lambda_r}\biggr) +
        \frac{\frac{1}{t_i} \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r}
            \bigl(1+ k_r \log\bigl(\frac{t_i}{\lambda_r}\bigr)\bigr)}
            {\sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}}
        1_{\delta_i = 1 \land r \in c_i}
\end{equation}
and 
\begin{equation}
\frac{\partial \ell_i(\v\theta)}{\partial \lambda_r} = 
    \frac{k_r}{\lambda_r} \biggl(\frac{t_i}{\lambda_r}\biggr)^{k_r} -
    \frac{
        \bigl(\frac{k_r}{\lambda_r}\bigr)^2 \bigl(\frac{t_i}{\lambda_r}\bigr)^{k_r - 1}
    }
    {
        \sum_{j \in c_i} \frac{k_j}{\lambda_j}\bigl(\frac{t_i}{\lambda_j}\bigr)^{k_j-1}
    }
    1_{\delta_i = 1 \land r \in c_i}
\end{equation}
\end{theorem}

The result follows from taking the partial derivatives of the log-likelihood
contribution of the $i$-th system given by Equation
\eqref{eq:weibull_likelihood_contribution}. It is a tedious calculation so the proof
has been omitted, but the result has been verified by using a very precise numerical
approximation of the gradient.

By the linearity of differentiation, the gradient of a sum of functions is
the sum of their gradients, and so the score function conditioned on the entire
sample is given by
\begin{equation}
\label{eq:weibull_series_score}
\nabla \ell(\v\theta) = \sum_{i=1}^n \nabla \ell_i(\v\theta).
\end{equation}

## Weibull Series System: Homogeneous Shape Parameters {#reduced-weibull}
A series system composed of Weibull components is not generally Weibull unless the
shape parameters of the components are homogeneous.
\begin{theorem}
If the shape parameters of the components are homogenous, then the lifetime
series system follows a Weibull distribution with a shape parameter $k$ given by
the identical shape parameters of the components and a scale parameter $\lambda$
given by
\begin{equation}
\label{eq:sys_weibull_scale}
\lambda = \biggl(\sum_{j=1}^{m} \lambda_j^{-k}\biggr)^{-1/k},
\end{equation}
where $\lambda_j$ is the scale parameter of the $j$\textsuperscript{th} component.
\end{theorem}
\begin{proof}
Given $m$ Weibull lifetimes $T_{i 1}, \ldots, T_{i m}$ with the same shape parameter $k$
and scale parameters $\lambda_1, \ldots, \lambda_m$, the reliability function of the series
system is
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\sum_{j=1}^{m}\biggl(\frac{t}{\lambda_j}\biggr)^{k}\biggr\}.
$$
To show that the series system lifetime is Weibull, we need to find a single scale parameter $\lambda$ such that
$$
R_{T_i}(t;\v\theta) = \exp\biggl\{-\biggl(\frac{t}{\lambda}\biggr)^{k}\biggr\},
$$
which has the solution
$$
\lambda = \frac{1}{\left(\frac{1}{\lambda_1^k} + \ldots + \frac{1}{\lambda_m^k}\right)^{\frac{1}{k}}}.
$$
\end{proof}

::: {.theorem}
If a series system has Weibull components with homogeneous shape parameters, the component
cause of failure is conditionally independent of the system failure time:
$$
    \Pr\{K_i = j | T_i = t_i \} = \Pr\{K_i = j\} = \frac{\lambda_j^{-k}}{\sum_{l=1}^{m} \lambda_l^{-k}}.
$$
:::

::: {.proof}
By Theorem \@ref(thm:prob-k-given-t), the conditional probability of the $j$\textsuperscript{th} component being the
cause of failure given the system failure time is given by
\begin{align*}
\Pr\{K_i = j | T_i = t\}
    &= \frac{f_{K_i, T_i}(j, t;\v\theta)}{f_{T_i}(t;\v\theta)}
    = \frac{h_j(t;k,\lambda_j) R_{T_i}(t;\v\theta)}
        {h_{T_i}(t;\v{\theta_j}) R_{T_i}(t;\v\theta)}\\
    &= \frac{h_j(t;k,\lambda_j)}{\sum_{l=1}^m h_l(t;k,\lambda_l)}
    = \frac{\frac{k}{\lambda_j}\bigl(\frac{t}{\lambda_j}\bigr)^{k-1}}
        {\sum_{l=1}^m \frac{k}{\lambda_l}\bigl(\frac{t}{\lambda_l}\bigr)^{k-1}}
    = \frac{\bigl(\frac{1}{\lambda_j}\bigr)^k}
        {\sum_{l=1}^m \bigl(\frac{1}{\lambda_l}\bigr)^k}.
\end{align*}
:::

If we have prior knowledge that the shape parameters are sufficiently homogenous, it may
be useful to simplify the likelihood model by assuming the shape parameters are identical,
simplifying the series system to Weibull, facilitating analysis and interpretation.
According to the bias-variance trade-off, we expect the MLE to be more biased but
have lower sampling variance.

We denote the full model log-likelihood function by $\ell_F$ and the reduced model log-likelihood
by $\ell_R$. The reduced model is obtained by setting the shape parameter of each component to
be the same, i.e., $k_1 = \cdots = k_m = k$. Thus, the reduced model log-likelihood function is given by
$$
\ell_R(k, \lambda_1, \lambda_2, \cdots, \lambda_m) =
        \ell_F(k, \lambda_1, k, \lambda_2, \ldots, k, \lambda_m),
$$
The same may be done for the score and hessian of the log-likelihood functions.

Simulation Study: Series System with Weibull Components {#sim-study}
====================================================================

```{r sim-study-design, echo = F}
theta <- c(
  shape1 = 1.2576, scale1 = 994.3661,
  shape2 = 1.1635, scale2 = 908.9458,
  shape3 = 1.1308, scale3 = 840.1141,
  shape4 = 1.1802, scale4 = 940.1342,
  shape5 = 1.2034, scale5 = 923.1631
)

shapes <- theta[grepl("shape", names(theta))]
scales <- theta[grepl("scale", names(theta))]
```

In this section, we conduct a simulation study to assess the performance
of the MLE for the full likelihood model defined in Section \@ref(weibull).
In this simulation study, we assess the sensitivity of the MLE to
various simulation scenarios. In particular, we assess two important
properties of the MLE with respect to a scenario:

1. Accuracy (Bias): How close is the expected value of the MLE to the true
   parameter values? If the expected value of the MLE is close to the true
   parameter values, the accuracy is high.

2. Precision: How much does the MLE vary from sample to sample? We measure
   this by assessing the 95% confidence intervals (BCa, Bias-Corrected and
   accelerated). If the confidence intervals are both small and have good 
   coverage probability (the proportion of confidence intervals that contain 
   the true parameter values), then the MLE is precise.

We begin by specifying the parameters of the series system that will be
the central object of our simulation study. We consider the data in
[@Huairu-2013], in which they study the reliability of a series system with
three components. They fit Weibull components in a series configuration to
the data, resulting in an MLE with shape and scale estimates given by the
first three components in Table \ref{tab:series-sys}. To make the model
slightly more complex, we add two more components to this series system,
with shape and scale parameters given by the last two components in Table
\ref{tab:series-sys}. We will refer to this system as the **base** system.

In Section \@ref(reliability), we defined a well-designed series
system as one that consists of components with similar reliabilities, where we define
reliability in two ways, the mean time to failure (MTTF) and the probability that a
specific component will be the cause of failure. All things else being equal,
components with long MTTFs and with near uniform probability of being the component
cause of failure is preferrable, otherwise we have a weak link in the system.

The base system defined in Table \ref{tab:series-sys} satisfies this definition
of being a well-designed system. We see that there are no components that are
significantly less reliable than any of the others, component 1 being the most reliable
and component 3 being the least reliable. This is a result of the scales and shapes
being similar for each component. In addition, the shapes are larger than $1$, which
means components are unlikely to fail early.

```{r series-sys, table.attr = "style='width:50%;'", echo = F, fig.align = "center"}
mttf.sys <- integrate(function(t) {
  t * dwei_series(t,
    scales = scales,
    shapes = shapes
  )
}, lower = 0, upper = Inf)$value

mttf.sys <- round(mttf.sys, digits = 3)
mttf <- round(gamma(1 + 1 / shapes) * scales, digits = 3)
probs <- round(wei_series_cause(1L:5L, scales = scales, shapes = shapes), digits = 3)
tau <- qwei_series(p = 0.825, scales = scales, shapes = shapes)
surv.sys <- round(surv_wei_series(tau, scales = scales, shapes = shapes), digits = 3)
surv <- round(pweibull(tau, shape = shapes, scale = scales, lower.tail = FALSE), digits = 3)
components <- data.frame(
  Shape = shapes,
  Scale = scales,
  MTTF = mttf,
  Prob = probs,
  Survival = surv,
  row.names = paste("Component", 1:5)
)

components <- rbind(components, "Series System" = c(NA, NA, mttf.sys, NA, surv.sys))
names(components) <- gsub("\\.", " ", names(components))
names(components)[1] <- "Shape ($k_j$)"
names(components)[2] <- "Scale ($\\lambda_j$)"
names(components)[3] <- "MTTF$_j$"
names(components)[4] <- "$\\Pr\\{K_i = j\\}$"
names(components)[5] <- "$R_j(\\tau;k_j,\\lambda_j)$"
knitr::kable(
  components,
  caption = "Weibull Components in Series Configuration",
  escape = FALSE # Add this line to allow raw LaTeX
)
```



## Data Generating Process {#data-gen-proc}

In this section, we describe the data generating process for our simulation studies.
It consists of three parts: the series system, the candidate set model, and the
right-censoring model.

### Series System Lifetime {-}

We generate data from a Weibull series system with $m$ components.
As described in Section \@ref(weibull), the $j$\textsuperscript{th} component
of the $i$\textsuperscript{th} system has a lifetime distribution given by
$$
    T_{i j} \sim \operatorname{Weibull}(k_j, \lambda_j)
$$
and the lifetime of the series system composed of $m$ Weibull components
is defined as
$$
    T_i = \min\{T_{i 1}, \ldots, T_{i m}\}.
$$
To generate a data set, we first generate the $m$ component failure times,
by efficiently sampling from their respective distributions, and we then set
the failure time $t_i$ of the system to the minimum of the component failure times.

### Right-Censoring Model {-}
We employ a simple right-censoring model, where the right-censoring time
$\tau$ is fixed at some known value, e.g., an experiment is run for a fixed
amount of time $\tau$, and all systems that have not failed by the end of the
experiment are right-censored. The censoring time $S_i$ of the
$i$\textsuperscript{th} system is thus given by
$$
    S_i = \min\{T_i, \tau\}.
$$
So, after we generate the system failure time $T_i$, we generate the censoring
time $S_i$ by taking the minimum of $T_i$ and $\tau$.
In our simulation study, we paramaterize the right-censoring time $\tau$ by the
quantile $q = 0.825$ of the series system,
$$
    \tau = F_{T_i}^{-1}(q).
$$
This means that $82.5\%$ of the series systems are expected to fail before time $\tau$
and $17.5\%$ of the series are expected to be right-censored. To solve for the $82.5\%$
quantile of the series system, we define the function $g$ as
$$
g(\tau) = F_{T_i}(\tau;\v\theta) - q
$$
and find its root using the Newton-Raphson method. See Appendix \@ref(app-series-quantile) for the R code that
implements this procedure.

### Masking Model for Component Cause of Failure {-}

We must generate data that satisfies the masking conditions described in
Section \@ref(candmod).
There are many ways to satisfying the masking conditions. We choose the simplest
method, which we call the *Bernoulli candidate set model*. In this model, each
non-failed component is included in the candidate set with
a fixed probability $p$, independently of all other components and independently
of $\v\theta$, and the failed component is always included in the candidate set.
See \@ref(app-cand-model-r)} for the R code that implements this model.

## Simulation Scenarios

We define a simulation scenario to be some combination of $n$ (sample size),
$p$ (masking probability in our Bernoulli candidate set model), $k_3$
(shape parameter of the third component), $\lambda_3$ (scale parameter of the
third component), and $q$ (right-censoring quantile). We are interested in
choosing a small number of scenarios that are representative of real-world
scenarios and that are interesting to analyze.

Here is an outline of the simulation study for a particular scenario:

1. Fix a combination of simulation parameters to some value, and vary the remaining
   parameters. For example, if we want to assess how the sampling distribution of
   the MLE changes with respect to sample size, we might choose some particular
   values for $p$, $k_3$, $\lambda_3$, and $q$, and vary the sample size $n$ over the
   desired range.

2. Simulate $R \geq 300$ datasets from the Data Generating Process (DGP) described in
   Section \@ref(data-gen-proc) and compute an MLE for each dataset. We choose $R$ to be
   large enough so that the sampling distribution of the MLE is well approximated by
   the empirical distribution of the $R$ MLEs.

3. For each of these $R$ MLEs, compute some function of the MLE, like the BCa confidence
   intervals or the likelihood ratio test statistic. This will give us $R$ statistics
   as a Monte-carlo estimate of the sampling distribution of the statistic.

4. Use the $R$ statistics to estimate some property of the sampling distribution of the
   statistic, e.g., the mean of the MLE or the coverage probability of the BCa confidence
   intervals, with respect to the parameter(s) we are varying in the scenario, e.g.,
   assess how the coverage probability of the BCa confidence intervals changes with
   respect to sample size.

5. Visualize the results and assess the behavior of estimator under the chosen scenario.
   
For how we run a simulation scenario, see Appendix \@ref(app-sim-study-r).

## Scenario: Assessing the Impact of Right-Censoring {#effect-censoring}

In this scenario, we use the well-designed series system described in Table \ref{tab:series-sys},
and we vary the right-censoring quantile ($q$) from $60\%$ to $100\%$
(no right-censoring), with a component cause of failure masking
probability of $21.5\%$ and sample size $n = 100$.

```{r q-vs-stats, out.width='100%', cache = T, fig.cap=c("Right-Censoring Quantile vs MLE ($p = 0.215, n = 100$)","Right-Qensoring Quantile ($q$) vs MLEs"), fig.align="center", echo = F}
# knitr::include_graphics("image/tau/plot-q-vs-mle.pdf")
knitr::include_graphics("image/5_system_tau_fig.pdf")
```

When a right-censoring event occurs, in order to increase the likelihood of the data, the MLE
is nudged in a direction that increases the probability of a right-censoring event at time $\tau$,
which is given by $R_{T_i}(t;\v\theta)$, representing a source of bias in the estimate.

To increase $R_{T_i}(\tau)$, we move in the direction (gradient) of these partial derivatives.
The partial derivatives of $R_{T_i}(\tau)$
are given by
\begin{align*}
\frac{\partial R_{T_i}(\tau)}{\partial \lambda_j} &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \frac{k_j}{\lambda_j},\\
\frac{\partial R_{T_i}(\tau)}{\partial k_j}       &= R_{T_i}(\tau;\v\theta) \left(\frac{\tau}{\lambda_j}\right)^{k_j} \left(\log \lambda_j - \log \tau\right),
\end{align*}
for $j = 1, \ldots, m$. We see that these partial derivatives are related to the score of a right-censored likelihood contribution in
Theorem \ref{thm:weibull_score}. Let us analyze these partial derivatives:

- As the right-censoring quantile $q$ increases ($\tau$ increases), $R_{T_i}(\tau;\v\theta)$
  decreases, and so the effect right-censoring has on the MLE decreases. This is what we see
  in Figure \ref{fig:q-vs-stats}.

- The partial derivatives with respect to the scale parameters are always positive, so right-censoring positively bias the scale parameter
  estimates to make right-censoring events more likely. The more right-censoring, the more the positive bias. We see this in Figure
  \ref{fig:q-vs-stats}, where the bias of the MLE for the scale parameter decreases as we decrease the probability ($1-q$) of a right-censoring event.

- The partial derivative with respect to the shape parameter of the $j$\textsuperscript{th} component, $k_j$, is
  non-negative if $\lambda_j \geq \tau$ and otherwise negative. In our well-designed series system, the scale parameters
  are large compared to most of the right-censoring times for $\tau(q)$, so the MLE nudges the shape parameter estimates
  in a positive direction to increase the probability of a right-censoring event $R_{T_i}(\tau)$ at time $\tau$. We see this
  in Figure \ref{fig:q-vs-stats}, where the shape parameter estimates are positively biased for most of the quantiles
  $q$.
  
### Key Observations

##### Coverage Probability (CP) {-}
The CP is well-calibrated, obtaining a value near the
nominal 95% level across different right-censoring quantiles. This suggests that the
bootstrapped CIs will contain the true value of the parameters with the specified confidence
level. The CIs are neither too wide nor too narrow.

##### Dispersion of MLEs {-}
The shaded regions representing the 95% probability range of
the MLEs get narrower as the right-censoring quantile increases. This is an indicator of the
increased precision in the estimates as more data is available due to decreased
censoring.

##### IQR of Bootstrapped CIs {-}
The IQR (vertical blue bars) reduces with an increase in
sample size. This suggests that the bootstrapped CIs are getting more consistent and
focused around a narrower range with larger samples while maintaining a good coverage
probability. As we get more data, the bootstrapped CIs are more likely to be closer
to each other and the true value of the parameters.

For small right-censoring quantiles (small right-censoring times), they are quite
large, but to maintain well-calibrated CIs, this  was necessary. The estimator is quite
sensitive to the data, and so the bootstrapped CIs are quite wide to account for this
sensitivity when the sample contains insufficient information due to censoring.

##### Bias of MLEs {-}
The red dashed line indicating the mean of MLEs initially is quite biased,
but quickly diminshes to neglible levels for scale parameters. The bias for the shape
parameters never reach zero, but this is potentially due to masking. At a larger sample
size, we anticipate the bias in the shape estimates would also decrease to zero.

## Scenario: Assessing the Impact of Sample Size {#effect-samp-size}

In this scenario, we use the well-designed series system described in Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
we fix the right-censoring quantile to $q = 0.825$ (moderate censoring), and we vary the sample
size $n$ from $50$ (small sample size) to $1000$ (very large sample size).

```{r samp-size-n-vs-stats, out.width='100%', fig.cap=c("Sample Size vs MLEs ($p = 0.215, q = 0.825$)","Sample Size ($n$) vs MLEs"), fig.align="center", echo = F}
# knitr::include_graphics("image/n-vs-mles.pdf")
knitr::include_graphics("image/5_system_samp_size_fig.pdf")
```

In Figure \ref{fig:samp-size-n-vs-stats}, we show the effect of the sample size $n$ on the MLEs
for the shape and scale parameters. The top four plots only show the effect on the MLEs for the
shape and scale parameters of components $1$ and $4$, since the rest were essentially identical,
and the bottom two plots show the coverage probabilities for all parameters.

### Key Observations

##### Coverage Probability (CP) {-}
The CP is well-calibrated, obtaining a value near the
nominal $95\%$ level across different sample sizes. This suggests that the bootstrapped
CIs will contain the true value of the shape parameter with the specified confidence
level. The CIs are neither too wide nor too narrow.

##### Dispersion of MLEs {-}
The shaded regions representing the $95\%$ probability range of
the MLEs get narrower as the sample size increases. This is an indicator of the
increased precision in the estimates when provided with more data. This is consistent
with the asymptotic properties of the MLE when the regularity conditions are satisfied,
e.g., converges in probability to the true value of the parameter as $n$ goes to infinity.

##### IQR of Bootstrapped CIs {-}
The IQR (vertical blue bars) reduces with an increase in
sample size. This suggests that the bootstrapped CIs are getting more consistent and
focused around a narrower range with larger samples while maintaining a good coverage
probability. As we get more data, the bootstrapped CIs are more likely to be closer
to each other and the true value of the scale parameter.

For small sample sizes, they are quite large, but to maintain well-calibrated CIs, this
was necessary. The estimator is quite sensitive to the data, and so the bootstrapped
CIs are quite wide to account for this sensitivity when the sample size is small and
not necessarily representative of the true distribution.

##### Bias of MLEs for Scales {-}
The red dashed line indicating the mean of MLEs remains stable across
different sample sizes and close to the true value, suggesting that the scale MLEs are 
reasonably unbiased.

##### Bias of MLEs for Shapes {-}
The red dashed line is the mean of shape MLEs. Unlike the scale MLEs,
we see that for small samples, particularly less than $200$, we observe a significant
amount of positive bias for shape MLEs. The MLE for the shape parameters in this
scenario appear to be more sensitive to the data than the scale parameters.

This scenario successfully illustrates the importance of sample size in estimating parameters.
The findings align with statistical theory and provide insights into the behavior of these estimators
for different sample sizes. In particular, it highlights the sensitivity of the shape parameter
estimates to right-censoring and masking for small sample sizes and the importance of having
sufficient data to overcome these effects.

## Scenario: Assessing the Impact of Masking Probability for Component Cause of Failure {#p-vs-mttf}

In this scenario, we use the well-designed series system described in
Table \ref{tab:series-sys}. We fix the sample size to $n = 90$ (reasonable sample size) and 
we fix the right-censoring quantile to $q = 0.825$, and we vary the masking probability 
from $p$ from $0.1$ (very slight masking the component cause of failure) to $0.85$
(extreme masking of the component cause of failure).

In Figure \ref{fig:masking-prob-vs-stats}, we show the effect of the masking probability
$p$ on the MLE for the shape and scale parameters. The top four plots only show the effect
on the MLEs for the the shape and scale parameters of components $1$ and $4$, since the
rest were essentially identical, and the bottom two plots show the coverage probabilities for
all parameters.

```{r masking-prob-vs-stats, fig.cap=c("Component Cause of Failure Masking ($p$) vs MLE", "Component Cause of Failure Masking Probability (p) vs. MLE"), fig.align="center", echo = F}
# knitr::include_graphics("image/p-vs-mle.pdf")
knitr::include_graphics("image/5_system_prob_fig.pdf")
```

### Key Observations

##### Coverage Probability (CP) {-}
For the scale parameters, the $95\%$ CI is well-calibrated
for Bernulli masking probabilities up to $p = 0.725$, which is really quite significant,
obtaining coverages over $90\%$, but drops precipitously after that point.  
For the shape parameters, the $95\%$ CI is well-calibrated for masking probabilities only
up to $p = 0.4$, which is still large, obtaining coverages generally over $90%$, but
begins to drop slowly after that point.
  
The BCa confidence intervals are well-calibrated for most realistic
masking probabilities, constructing CIs that are neither too wide nor too narrow,
but when the masking is severe and the sample size is small, one should
take the CIs with a grain of salt.

##### Dispersion of MLEs {-}
The shaded regions representing the $95\%$ quantile of
the MLEs become wider as the masking probability increases. This is an indicator of the
decreased precision in the estimates when provided with more ambiguous data about the
component cause of failure. However, even for fairly significant Bernoulli masking,
$p \leq 0.55$, the $95\%$ quantiles are narrow and the CP is
well-calibrated, indicating that the MLEs are still precise and accurate.

##### IQR of Bootstrapped CIs {-}
The IQR (vertical blue bars) show that the bootstrapped BCa
CIs are becoming more spread out as the masking probability increases. They are also
asymmetric, with the lower bound being more spread out than the upper bound, but this
is consistent with the actual behavior of the dispersion of the MLEs, which exhibits
the same pattern. The width of the CIs consistently increase as the masking probability
increases, which we intuitively expected given the increased uncertanity about the
component cause of failure.
After a Bernoulli masking probability of $p \approx 0.5$, the width of the CIs rapidly increase,
which is apparently necessary for the CPs to remain well-calibrated.

##### Bias of MLEs {-}
The red dashed line indicating the mean of the MLEs remains
stable across different masking probabilities, only showing a significant positive bias
when the masking probability $p$ becomes quite significant.

## Scenario: Assessing the Impact of Changing the Scale Parameter of Component 3 {#scale-vs-mttf}

By Equation \eqref{eq:mttf-weibull}, we see that MTTF$_j$ is proportional to the scale parameter $\lambda_j$, which means
when we decrease the scale parameter of a component, we proportionally decrease the MTTF.
In this scenario, we start with the well-designed series system described in Table \ref{tab:series-sys},
and we will manipulate the MTTF of component 3, MTTF$_3$, by changing its
scale parameter, $\lambda_3$, and observing the effect this has on the MLE. Since the other components
had a similiar MTTF, we will arbitrarily choose component 1 to represent the other components.
The bottom plot shows the coverage probabilities for all parameters.

In Figure \ref{fig:mttf-vs-ci}, we show the effect of changing the scale parameter of component $3$, $lambda_3$,
but map $\lambda_3$ to MTTF$_3$ to make it more intuitive to reason about. We vary the MTTF of component 3
from $300$ to $1500$ and the other components have their MTTFs fixed at around $900$, as shown in
Table \ref{tab:series-sys}. We fix the masking probability to $p = 0.215$ (moderate masking),
the right-censoring quantile to $q = 0.825$ (moderate censoring), and the sample size to $n = 100$ (moderate sample size).

```{r mttf-vs-ci, fig.cap=c("MTTF$_3$ vs MLE By Varying Scale", "Mean-Time-To-Failure (MTTF) of Component 3 by Varying Its Scale Parameter"), fig.align="center", echo = F}
# knitr::include_graphics("image/plot-scale3-vs-stats.pdf")
# knitr::include_graphics("image/5_system_scale3_vary.pdf")
knitr::include_graphics("image/5_system_mttf3_by_scale3.pdf")
```

### Key Observations

##### Coverage Probability (CP) {-}
When MTTF of component 3 is much smaller than other components,
the CP for $k_3$ is very well calibrated (approximately obtaining the nominal level $95\%$)
while the CP for other componentns are around $90\%$, which is still reasonable.
(This is the case even though the width of the CI for $k_3$ is extremely narrow compared to the others).
As MTTF$_3$ increases, the CP for $k_3$ decreases, while the CP for the other components increase
slightly. The scale parameters are generally well-calibrated for all of the components, except
for component 3 when its MTTF is large and it dips down to $90\%$. Despite the individual differences,
the mean of the CPs for shape and scale parameters hardly change.

##### Dispersion of MLEs {-}
For component 3, as its MTTF decreases, the dispersion of MLEs narrows,
indicating more precise estimates. Conversely, dispersion for other components widens. As MTTF
of component 3 increases, its dispersion widens while others narrow. This is consistent with
the fact that the smaller MTTF of component 3 means that, in this well-designed system at least,
it is more likely to be the component cause of failure, and so we have more information about
its parameters and are able to estimate them more accurately.

##### IQR of Bootstrapped CIs {-}
The dark blue vertical lines representing IQR are consistent with the dispersion of MLEs,
which is the ideal behavior, and suggests that the BCa confidence intervals are performing well.

##### Bias of MLEs {-}
For component 3, the bias of MLE for the scale parameter becomes slightly more negatively biased
as MTTF$_3$ increases, and the bias of the MLE for the shape parameter becomes slightly more positively
biased. The MLE for the shape and scale parameters for component 1 have a very small bias, if any,
and are not affected by the MTTF$_3$. The scale parameters are easier to estimate than the shape
parameters, and so they are less sensitive to changes in scale than the shape parameters, as
we will show in the next scenario.

## Scenario: Assessing the Impact of Changing the Shape Parameter of Component 3 {#shape3-vary}

The shape parameter determines the failure characteristics.
We vary the shape paramenter of component 3 from $0.1$ to $3.5$ and observe the effect
it has on the MLE.
When $k_3 < 1$, this indicates infant mortality, and when $k_3 > 1$, this indicates
wear-out failures.

We analyze the effect of component 3's shape parameter on the MLE and the bootstrapped confidence intervals for the
shape and scale parameters of components 1 and 3 (the component we are varying). First, we look at the effect
on the scale parameter.


```{r prob3-vs-mle, out.width="100%", fig.cap=c("Probability of Component 3 Failure vs MLE"), fig.align="center", echo = F}
knitr::include_graphics("image/5_system_shape3_fig.pdf")
```

### Key Observations

##### Coverage Probability (CP) {-}
The CP for the scale parameters
are well-calibrated and close to the nominal level of $0.95$ for all values of $\Pr\{K_i = 3\}$.
For the the shape parameter of component 3 ($k_3$) in
bold orange colors, we see that it is well-calibrated for all values of
$\Pr\{K_i = 3\}$, but actually may become too large for extreme values of $\Pr\{K_i = 3\}$.
The CP for the shape parameters of the other components decreases with $$\Pr\{K_i = 3\}$, dipping below $90\%$ for $\Pr\{K_i = 3\} > 0.4$. At a sample size of $n = 100$, the CP for the shape parameters of the other components is generally not well-calibrated for $\Pr\{K_i = 3\} > 0.4$.

##### Dispersion of MLEs {-}
The dispersion of the MLE for the shape and scale parameters of component 1, $k_1$ and $\lambda_1$,
is fairly steady but begins to increase rapdily at the extreme values of $\Pr\{K_i = 3\}$. This is indicative of
having less information about the failure characteristcs of component $1$ as component $3$ begins to dominate the
component cause of failure.
The dispersion of the shape parameter $k_3$ is initially quite large, indicative of having very little
information about the failure characteristcs of component 3 since it is unlikely to be the component cause of
failure, but its dispersion rapidly decreases as $\Pr\{K_i = 3\}$ increases and more information is
available about component 3's failure characteristics. In fact, it nearly becomes a point at $\Pr\{K_i = 3\} = 0.6$.
The dispersion of the the scale parameter of component $3$, $\lambda_1$, is quite steady and is less spread out
than the MLE for $\lambda_1$, but at extreme values of $\Pr\{K_i = 3\}$, it also begins to rapidly increase,
suggesting some complex interactions between the shape and scale parameters of component 3.

##### IQR of Bootstrapped CIs {-}
The CIs precisely track the dispersion of the MLEs, which is the ideal behavior,
and suggests that the BCa confidence intervals are performing well.

##### Bias of MLEs {-}
The MLE for the scale parameters are nearly unbiased and generally seem unaffected by changes
in $\Pr\{K_i = 3\}$. As $\Pr\{K_i = 3\}$ increases the MLE is adjusting $k_1$ to be more
positively biased, decreasing its infant morality rate to make it less likely to be the
component cause of failure, and adjusting $k_3$ to be less positively biased, increasing
its infant mortality rate, to make it more likely to be the component cause of failure.

Future Work
===========

#### Relaxation of Masking Conditions {-}

Investigate relaxations of Conditions 1, 2, and 3.
Condition 1 stipulates that the failed component is always in
the candidate set,
$$
    \Pr\{K_i \in \mathcal{C}_i\} = 1.
$$
Instead, we could model this as a probability, where the probability
of the failed component being in the candidate set is a function
of the failure time $T_i$ and the component cause of failure $K_i$,
$$
    \Pr\{K_i \in \mathcal{C}_i | K_i = j, T_i = t_i\} = g(j, t_i).
$$
Condition 2 stipulates that
$$
    \Pr\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j\} = 
    \Pr\{\mathcal{C}_i = c_i | T_i = t_i, K_i = j'\}
$$
for all $j, j' \in c_i$. We call this an *uninformed* candidate set,
since the conditional probability of the candidate set given the
failure time and component cause of failure is independent of the
component cause of failure. We could relax this condition to allow
for *informed* candidate sets, where the conditional probability
of the candidate set given the failure time and component cause of
failure is dependent on the component cause of failure.

In each of these violations or relaxations, we can either construct
a new likelihood model that takes this relaxation into account, or
we can use the existing likelihood model and assess the sensitivity
of the estimator to this violation. A potentially interesting way to
do the lattr is by using KL-divergence to measure the distance
between, for instance, the uninformed and informed candidate set models,
and then assess the sensitivity of the estimators to this distance.

#### Expanded Sensitivity Analyses {-}

Explore the use of a reduced model that assumes homogeneity in shape parameters.
This approach would simplify the system to $m+1$ parameters, as opposed to the
$2m$ parameters in the current model, potentially increasing interpretability
and reducing estimator variability. This direction warrants detailed
investigation to ensure the reduced model retains sufficient accuracy and
adequately describes the data.

#### Semi-Parametric Bootstrap {-}

We used the non-parameteric bootstrap to construct confidence intervals,
but we could also use the semi-parametric bootstrap.
In the semi-parametric bootstrap, instead of resampling from the original
data, we sample component lifetimes from the parametric distribution fitted
to the original data and sample candidate sets from the empirical distribution
of the conditional candidate sets in the original data.
This is a compromise between the non-parametric bootstrap and the fully
parametric bootstrap.[^30]
[^30]: The fully parametric bootstrap is not appropriate for our likelihood
model because we do not assume a parametric form for the distribution of the
candidate sets.

#### Data Augmentation {-}

Assess the robustness of Data Augmentation (DA) as an implicit
prior. For example, we may adopt the prior that the system is
well-designed and augment particularly small samples with synthetic
data from a reduced model (with homogenous shape parameters) fitted to the
original data.

Unlike a full Bayesian approach, where we would need to specify
a prior for the parameters, DA is an implicit prior that need not
be explicitly specified. It is a form of regularization that
reduces the variance of the estimator by leveraging the structure
of the model and the data.

#### Penalized Likelihood For Homogenous Shape Parameters {-}

Assess the use of penalized likelihood methods instead of
DA as a form of regularization. For instance, we can add
a penalty term to the log-likelihood function that penalizes
the likelihood when the shape parameters are not close to
each other. Instead of using a reduced model, we can 
use a penalized likelihood approach to encourage the shape
parameters to be close to each other, but not necessarily
equal.

#### General Likelihood Model with Predictors {-}

In this paper, we focused on a likelihood model that assumed Weibull components
in a series configuration.
We can extend this model by generalizing the hazard functions in two ways:

1. Let the hazard model be a function of predictors $\v{w_1}, \ldots, \v{w_n}$,
where $\v{w_i}$ is a vector of predictors for the $i$th observation. Then, the
hazard function for the $j$th component is
$$
    h_j(t_i|\v{w_i};\v{\beta_j}),
$$
for instance we might make the shape and scale parameters of the Weibull
component model be a function of the predictors $\v{w_i}$.

2. Replace the Weibull hazard function with a more general hazard function.
For instance, in the Cox proportional hazards model [@cox1972regression], the
hazard function for the $j$th component is given by
$$
    h_j(t_i|\v{w_i};\v{\beta_j}) = h_0(t_i) \exp(\v{\beta_j}^T \v{w_i}),
$$
where $h_{0}(t_i)$ is a baseline hazard function shared by all components and
$\v{\beta_j}$ is the parameter vector for the $j$th component. A more general
model would allow the component hazard functions to take any valid form, namely
non-negative and integrable.

In either case, by the relation
$$
  R_j(t_i|\v{w_i};\v{\beta_j}) = e^{-H_j(t)},
$$
where
$$
  H_j(t_i) = \int_0^{t_i} h_j(u|\v{w_i};\v{\beta_j}) du
$$
is the cumulative hazard function for the $j$\textsuperscript{th} component, we
can plug these component hazard and reliability functions into the likelihood
contribution model in Theorem \ref{thm:likelihood_contribution} to obtain a
general likelihood model with predictors for the series system.

#### Assess the Calibration of Other Related Bootstrapped Statistics {-}

The calibration of the bootstrapped confidence intervals were evaluated
and shown to be quite robust. We could do a similar analysis for other
bootstrapped statistics. For instance, we could assess the bootstrapped $95\%$
prediction interval for the probability that component $j$ is the component
cause of the next system failure given the data $\mathcal{D}_n$,
$$
    \Pr\{K_{n+1} = j | \mathcal{D}_n\}.
$$

Conclusion {#conclusion}
========================

This paper presented maximum likelihood methods for estimating
component reliability from masked failure data in series systems.
We accounted for right-censoring and masking issues in our likelihood model,
thereby achieving a rigorous framework for reliability analysis from limited
observational data.

Our simulation studies reveal that shape parameters are particularly sensitive
to data and harder to estimate precisely compared to scale parameters.
Right-censoring and masking were found to bias shape parameters positively,
although scale parameters exhibited more robust behavior. Despite these
challenges, bootstrapping techniques yielded well-calibrated confidence
intervals even for small sample sizes.

The sensitivities in shape parameters lead to a need for caution, especially
when dealing with small sample sizes. Coverage probabilities for shape
parameters, for instance, drop below $90\%$ in the presence of significant
masking. On the other hand, scale parameters display more robust properties,
maintaining well-calibrated confidence intervals even under challenging
conditions.

Overall, this work provides a rigorous framework for quantifying
component reliability from limited observational data. The methods
demonstrated accurate and robust performance despite the significant
challenges introduced by masking and right-censoring.

In light of our findings, to continue to refine our understanding and broaden
the applicability of these methods, future work can focus on relaxing modeling
assumptions, expanding the sensitivity analysis, and evaluating the viability
of methods designed to reduce the variability of shape parameter estimates. 

(APPENDIX) Appendix {-}
=======================


# Log-likelihood Function {#app-weibull-loglik-r}
The following R code implements the log-likelihood function for the Weibull
series system with right censoring and masking of component failure data. It is
implemented in the R library `wei.series.md.c1.c2.c3` and
is available on [GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).
For clarity and brevity, we removed some of the functionality that is not
relevant to the analysis in this paper and produce a simplified version of the
code below.

```{r, eval = FALSE}
#' Generates a log-likelihood function for a Weibull series system with respect
#' to parameter `theta` (shape, scale) for masked data with candidate sets
#' that satisfy conditions C1, C2, and C3 and right-censored data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Log-likelihood with respect to `theta` given `df`
loglik_wei_series_md_c1_c2_c3 <- function(df, theta) {
  n <- nrow(df)
  C <- md_decode_matrix(df, candset)
  m <- ncol(C)
  delta <- df[[right_censoring_indicator]]
  t <- df[[lifetime]]
  k <- length(theta)
  shapes <- theta[seq(1, k, 2)]
  scales <- theta[seq(2, k, 2)]

  s <- 0
  for (i in 1:n) {
    s <- s - sum((t[i] / scales)^shapes)
    if (delta[i]) {
      s <- s + log(sum(shapes[C[i, ]] / scales[C[i, ]] *
        (t[i] / scales[C[i, ]])^(shapes[C[i, ]] - 1)))
    }
  }
  return(s)
}
```

# Score Function {#app-score-fn-r}
The following code is the score function (gradient of the log-likelihood
function with respect to $\v\theta$) for the Weibull series system with a
likelihood model that includes masked component cause of failure and
right-censoring. It is implemented in the R library `wei.series.md.c1.c2.c3` and
is available on [GitHub](https://github.com/queelius/wei.series.md.c1.c2.c3).

For clarity and brevity, we removed some of the functionality that is not
relevant to the
analysis in this paper.

```{r score-code, eval = FALSE}
#' Computes the score function (gradient of the log-likelihood function) for a
#' Weibull series system with respect to parameter `theta` (shape, scale) for masked
#' data with candidate sets that satisfy conditions C1, C2, and C3 and right-censored
#' data.
#'
#' @param df (masked) data frame
#' @param theta parameter vector (shape1, scale1, ..., shapem, scalem)
#' @returns Score with respect to `theta` given `df`
score_wei_series_md_c1_c2_c3 <- function(df, theta) {
  n <- nrow(df)
  C <- md_decode_matrix(df, candset)
  m <- ncol(C)
  delta <- df[[right_censoring_indicator]]
  t <- df[[lifetime]]
  shapes <- theta[seq(1, length(theta), 2)]
  scales <- theta[seq(2, length(theta), 2)]
  shape_scores <- rep(0, m)
  scale_scores <- rep(0, m)

  for (i in 1:n) {
    rt.term.shapes <- -(t[i] / scales)^shapes * log(t[i] / scales)
    rt.term.scales <- (shapes / scales) * (t[i] / scales)^shapes

    # Initialize mask terms to 0
    mask.term.shapes <- rep(0, m)
    mask.term.scales <- rep(0, m)

    if (delta[i]) {
      cindex <- C[i, ]
      denom <- sum(shapes[cindex] / scales[cindex] *
        (t[i] / scales[cindex])^(shapes[cindex] - 1))

      numer.shapes <- 1 / t[i] * (t[i] / scales[cindex])^shapes[cindex] *
        (1 + shapes[cindex] * log(t[i] / scales[cindex]))
      mask.term.shapes[cindex] <- numer.shapes / denom

      numer.scales <- (shapes[cindex] / scales[cindex])^2 *
        (t[i] / scales[cindex])^(shapes[cindex] - 1)
      mask.term.scales[cindex] <- numer.scales / denom
    }

    shape_scores <- shape_scores + rt.term.shapes + mask.term.shapes
    scale_scores <- scale_scores + rt.term.scales - mask.term.scales
  }

  scr <- rep(0, length(theta))
  scr[seq(1, length(theta), 2)] <- shape_scores
  scr[seq(2, length(theta), 2)] <- scale_scores
  return(scr)
}
```

# Scenario Simulation {#app-sim-study-r}

The following R code is the Monte-carlo simulation code for running the various
scenarios described in Section \@ref(sim-study).

```{r bootstrap-sim-code, eval=FALSE}
#### Setup simulation parameters here ####
theta <- c(
  shape1 = 1.2576, scale1 = 994.3661,
  shape2 = 1.1635, scale2 = 908.9458,
  shape3 = NA, scale3 = 840.1141,
  shape4 = 1.1802, scale4 = 940.1342,
  shape5 = 1.2034, scale5 = 923.1631
)

shapes3 <- c(1.1308) # shape 3 true parameter values to simulate
scales3 <- c(840.1141) # scale 3 true parameter values to simulate
N <- c(30, 60, 100) # sample sizes to simulate
P <- c(.215) # masking probabilities to simulate
Q <- c(.825) # right censoring probabilities to simulate
R <- 1000L # number of simulations per scenario
B <- 1000L # number of bootstrap samples
max_iter <- 125L # max iterations for MLE
max_boot_iter <- 125L # max iterations for bootstrap MLE
n_cores <- detectCores() - 1 # number of cores to use for parallel processing
filename <- "data" # filename prefix for output files

#### Simulation code below here ####
library(tidyverse)
library(parallel)
library(boot)
library(algebraic.mle) # for `mle_boot`
library(wei.series.md.c1.c2.c3) # for `mle_lbfgsb_wei_series_md_c1_c2_c3` etc

file.meta <- paste0(filename, ".txt")
file.csv <- paste0(filename, ".csv")
if (file.exists(file.meta)) {
  stop("File already exists: ", file.meta)
}
if (file.exists(file.csv)) {
  stop("File already exists: ", file.csv)
}

shapes <- theta[seq(1, length(theta), 2)]
scales <- theta[seq(2, length(theta), 2)]
m <- length(shapes)

sink(file.meta)
cat("boostrap of confidence intervals:\n")
cat("   simulated on: ", Sys.time(), "\n")
cat("   type: ", ci_method, "\n")
cat("weibull series system:\n")
cat("   number of components: ", m, "\n")
cat("   scale parameters: ", scales, "\n")
cat("   shape parameters: ", shapes, "\n")
cat("simulation parameters:\n")
cat("   shapes3: ", shapes3, "\n")
cat("   scales3: ", scales3, "\n")
cat("   N: ", N, "\n")
cat("   P: ", P, "\n")
cat("   Q: ", Q, "\n")
cat("   R: ", R, "\n")
cat("   B: ", B, "\n")
cat("   max_iter: ", max_iter, "\n")
cat("   max_boot_iter: ", max_boot_iter, "\n")
cat("   n_cores: ", n_cores, "\n")
sink()

for (scale3 in scales3) {
  for (shape3 in shapes3) {
    for (n in N) {
      for (p in P) {
        for (q in Q) {
          shapes[3] <- shape3
          theta["shape3"] <- shape3

          cat("[starting scenario: scale3 = ", scale3, ", shape3 = ", shape3,
            ", n = ", n, ", p = ", p, ", q = ", q, "]\n")
          tau <- qwei_series(p = q, scales = scales, shapes = shapes)

          # we compute R MLEs for each scenario
          shapes.mle <- matrix(NA, nrow = R, ncol = m)
          scales.mle <- matrix(NA, nrow = R, ncol = m)
          shapes.lower <- matrix(NA, nrow = R, ncol = m)
          shapes.upper <- matrix(NA, nrow = R, ncol = m)
          scales.lower <- matrix(NA, nrow = R, ncol = m)
          scales.upper <- matrix(NA, nrow = R, ncol = m)

          iter <- 0L
          repeat {
            retry <- FALSE
            tryCatch(
              {
                repeat {
                  df <- generate_guo_weibull_table_2_data(
                    shapes = shapes, scales = scales, n = n, p = p, tau = tau
                  )

                  sol <- mle_lbfgsb_wei_series_md_c1_c2_c3(
                    theta0 = theta, df = df, hessian = FALSE,
                    control = list(maxit = max_iter, parscale = theta)
                  )
                  if (sol$convergence == 0) {
                    break
                  }
                  cat("[", iter, "] MLE did not converge, retrying.\n")
                }

                mle_solver <- function(df, i) {
                  mle_lbfgsb_wei_series_md_c1_c2_c3(
                    theta0 = sol$par, df = df[i, ], hessian = FALSE,
                    control = list(maxit = max_boot_iter, parscale = sol$par)
                  )$par
                }

                # do the non-parametric bootstrap
                sol.boot <- boot(df, mle_solver, R = B, parallel = "multicore",
                  ncpus = n_cores)
              },
              error = function(e) {
                cat("[error] ", conditionMessage(e), "\n")
                cat("[retrying scenario: n = ", n, ", p = ", p, ", q = ", q, "\n")
                retry <<- TRUE
              }
            )
            if (retry) {
              next
            }
            iter <- iter + 1L
            shapes.mle[iter, ] <- sol$par[seq(1, length(theta), 2)]
            scales.mle[iter, ] <- sol$par[seq(2, length(theta), 2)]

            tryCatch(
              {
                ci <- confint(mle_boot(sol.boot), type = ci_method,
                    level = ci_level)
                shapes.ci <- ci[seq(1, length(theta), 2), ]
                scales.ci <- ci[seq(2, length(theta), 2), ]
                shapes.lower[iter, ] <- shapes.ci[, 1]
                shapes.upper[iter, ] <- shapes.ci[, 2]
                scales.lower[iter, ] <- scales.ci[, 1]
                scales.upper[iter, ] <- scales.ci[, 2]
              },
              error = function(e) {
                cat("[error] ", conditionMessage(e), "\n")
              }
            )
            if (iter %% 5 == 0) {
              cat("[iteration ", iter, "] shapes = ", shapes.mle[iter, ],
                  "scales = ", scales.mle[iter, ], "\n")
            }

            if (iter == R) {
              break
            }
          }

          df <- data.frame(
            n = rep(n, R), rep(scale3, R), rep(shape3, R),
            p = rep(p, R), q = rep(q, R), tau = rep(tau, R), B = rep(B, R),
            shapes = shapes.mle, scales = scales.mle,
            shapes.lower = shapes.lower, shapes.upper = shapes.upper,
            scales.lower = scales.lower, scales.upper = scales.upper
          )

          write.table(df,
            file = file.csv, sep = ",", row.names = FALSE,
            col.names = !file.exists(file.csv), append = TRUE
          )
        }
      }
    }
  }
}
```

# Bernoulli Candidate Set Model {#app-cand-model-r}

```{r, eval = FALSE}
#' Bernoulli candidate set model is a particular type of *uninformed* model.
#' This model satisfies conditions C1, C2, and C3.
#' The failed component will be in the corresponding candidate set with
#' probability 1, and the remaining components will be in the candidate set
#' with probability `p` (the same probability for each component). `p`
#' may be different for each system, but it is assumed to be the same for
#' each component within a system, so `p` can be a vector such that the
#' length of `p` is the number of systems in the data set (with recycling
#' if necessary).
#'
#' @param df masked data.
#' @param p a vector of probabilities (p[j] is the probability that the jth
#'          system will include a non-failed component in its candidate set,
#'          assuming the jth system is not right-censored).
md_bernoulli_cand_c1_c2_c3 <- function(df, p) {
  n <- nrow(df)
  p <- rep(p, length.out = n)
  Tm <- md_decode_matrix(df, comp)
  m <- ncol(Tm)
  Q <- matrix(p, nrow = n, ncol = m)
  Q[cbind(1:n, apply(Tm, 1, which.min))] <- 1
  Q[!df[[right_censoring_indicator]], ] <- 0
  df %>% bind_cols(md_encode_matrix(Q, prob))
}
```

# Series System Quantile Function {#app-series-quantile}

```{r, eval=FALSE}
#' Quantile function (inverse of the cdf).
#' By definition, the quantile `p` * 100% for a strictly monotonically increasing
#' cdf `F` is the value `t` that satisfies `F(t) - p = 0`.
#' We solve for `t` using newton's method.
#'
#' @param p vector of probabilities.
#' @param shapes vector of weibull shape parameters for weibull lifetime
#'               components
#' @param scales vector of weibull scale parameters for weibull lifetime
#'               components
qwei_series <- function(p, shapes, scales) {
  t0 <- 1
  repeat {
    t1 <- t0 - sum((t0 / scales)^shapes) + log(1 - p) /
      sum(shapes * t0^(shapes - 1) / scales^shapes)
    if (abs(t1 - t0) < tol) {
      break
    }
    t0 <- t1
  }
  return(t1)
}
```


